{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c2e215-b652-4dbb-a951-8a62aff35046",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7b97e18-ce46-4d28-a4ed-22394adba752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODEL_TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "HYPERPARAMS_PATH = './hyperparams/'\n",
    "WEIGHTS_PATH = './feature_weights/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yabWKtrCSTTp",
   "metadata": {
    "id": "yabWKtrCSTTp",
    "tags": []
   },
   "source": [
    "## Carga Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9kRlrtLcSWSU",
   "metadata": {
    "id": "9kRlrtLcSWSU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509f4f4-6594-4a0c-9bed-927d3922d6a0",
   "metadata": {},
   "source": [
    "## Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b404dc-0e9c-4945-b4e5-d1a77d863a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb628e-a7f4-40e8-a62d-576f7a78c2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb40e62-06ae-46b9-a711-a5d5667c4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32053d4d-f4d8-4b07-9cd8-bb5bf4ac8d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 18:10:50.389211: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, BatchNormalization, Add, concatenate, Conv2DTranspose, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5732f07-8398-47a1-9d62-53cf7dd45556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "Sat Apr  9 18:10:52 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P0    N/A /  N/A |    172MiB /  4096MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1151      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      1886      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A     10266      C   ...onda3/envs/TFM/bin/python      161MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 18:10:51.797140: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-09 18:10:51.798370: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-09 18:10:51.864049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:51.864280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 computeCapability: 6.1\n",
      "coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-04-09 18:10:51.864317: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-09 18:10:51.867952: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-04-09 18:10:51.868018: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-04-09 18:10:51.869230: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-04-09 18:10:51.869530: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-04-09 18:10:51.873143: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-04-09 18:10:51.874088: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-04-09 18:10:51.874294: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-04-09 18:10:51.874414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:51.874639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:51.874787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-04-09 18:10:51.874830: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-04-09 18:10:52.499288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-09 18:10:52.499327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2022-04-09 18:10:52.499337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2022-04-09 18:10:52.499514: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:52.499725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:52.499904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:52.500053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/device:GPU:0 with 3370 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b591c-8916-415a-a23c-8309f52f56e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importador/Exportador Feature Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75269bf0-27f7-4b71-b311-256035370133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_weights(feature_vector, root_path, file_name):\n",
    "    with open(root_path + file_name, 'w') as outfile:\n",
    "        json.dump(feature_vector, outfile)\n",
    "\n",
    "def load_weights(root_path, file_name):\n",
    "    with open(root_path + file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4388716-87a9-4e2d-ab4a-e3959fa1958f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b5661a-a9ac-4686-9611-6d43711d1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_feature_matrix_indexes(sorted_feature_vector,matrix):  \n",
    "\n",
    "    half_row = round((matrix.shape[0] - 1) / 2)\n",
    "    half_column = round((matrix.shape[1] - 1) / 2)\n",
    "\n",
    "    matrix_indexes = {}\n",
    "    \n",
    "    index = 0\n",
    "\n",
    "    for parent_key in sorted_feature_vector:\n",
    "        normalized_index = math.ceil(index/2)\n",
    "\n",
    "        if (index % 2 != 0): # Impar\n",
    "            current_row = half_row - normalized_index\n",
    "        else: # Par\n",
    "            current_row = half_row + normalized_index\n",
    "\n",
    "        sorted_child_indexes = np.argsort(feature_vector[parent_key]['feature_weights'])[::-1]\n",
    "\n",
    "        child_names   = np.array(feature_vector[parent_key]['feature_childs'])\n",
    "        child_weights = np.array(feature_vector[parent_key]['feature_weights'])\n",
    "\n",
    "        sorted_child_names   = child_names[sorted_child_indexes]\n",
    "        sorted_child_weights = child_weights[sorted_child_indexes]\n",
    "\n",
    "        position = 0\n",
    "        for sorted_child_index in sorted_child_indexes:\n",
    "            normalized_position = math.ceil(position/2)\n",
    "\n",
    "            if (position % 2 != 0): # Impar\n",
    "                current_column = half_column - normalized_position\n",
    "            else: # Par\n",
    "                current_column = half_column + normalized_position\n",
    "\n",
    "            matrix_indexes[child_names[sorted_child_index]] = [current_row, current_column]\n",
    "            position = position + 1 \n",
    "\n",
    "        index = index + 1\n",
    "\n",
    "    return matrix_indexes\n",
    "    \n",
    "def fv2gi(feature_vector):\n",
    "\n",
    "    max_dimension = 0\n",
    "    for key in feature_vector:\n",
    "        childs_number = len(feature_vector[key]['feature_childs'])\n",
    "        max_dimension = max(childs_number, max_dimension)\n",
    "                \n",
    "    matrix = np.zeros((max_dimension, max_dimension))\n",
    "\n",
    "    weights_vector = []\n",
    "    for parent_key in feature_vector:\n",
    "        wpi = sum([float(child_weight) for child_weight in feature_vector[parent_key]['feature_weights']])\n",
    "        feature_vector[parent_key]['wpi'] = wpi\n",
    "        weights_vector.append(wpi)\n",
    "\n",
    "   \n",
    "    sorted_feature_vector = sorted(feature_vector.items(),\n",
    "                                   key = lambda item: item[1]['wpi'],\n",
    "                                   reverse = True)\n",
    "     \n",
    "    sorted_feature_vector = dict(sorted_feature_vector)\n",
    "\n",
    "    \n",
    "    matrix_indexes = get_feature_matrix_indexes(sorted_feature_vector, matrix)\n",
    "\n",
    "    return matrix_indexes\n",
    "\n",
    "# matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb0004-1f9b-493a-9c83-91d943b5309d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077b732a-ca4a-440e-8ae2-dbb3d15ba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feature_vector(X_dataset,child_weights):\n",
    "  # Obtenemos el set de columnas del dataset\n",
    "  train_columns_set  = set(X_dataset.columns)\n",
    "\n",
    "  for parent_feature in feature_vector.keys():\n",
    "    # Obtiene el set de características hijas del padre actual\n",
    "    # dict.fromleys para mantener el orden, un set desordena los valores\n",
    "    feature_childs_set = dict.fromkeys(feature_vector[parent_feature]['feature_childs'])\n",
    "\n",
    "    # Obtener el índice de las columnas del actual padre para acceder a los pesos del XGBoost\n",
    "    index_feature_childs = X_dataset.columns.get_indexer(feature_childs_set)\n",
    "\n",
    "    feature_vector[parent_feature]['feature_weights'] = list([str(child_weight) for child_weight in child_weights[index_feature_childs]])\n",
    "\n",
    "  return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c95ea-ae18-4406-b116-5ce6674b7f6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017e1821-71be-48d9-85cb-07808165a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_data(X_data):\n",
    "\n",
    "    # Create a sample df\n",
    "    normalized_df = X_data\n",
    "\n",
    "    # Calculate the zscores and drop zscores into new column\n",
    "    for column in normalized_df.columns:\n",
    "        normalized_df[column] = zscore(normalized_df[column])\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358c9a0-6423-4aa8-92a3-e01a8ac72bb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Oversampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efac95c3-8dc1-4acf-b4e0-78d24fa64131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "def oversample_data(X_data, Y_labels):\n",
    "\n",
    "    oversample = BorderlineSMOTE(kind='borderline-2')\n",
    "    X_oversampled, Y_oversampled = oversample.fit_resample(X_data, Y_labels)\n",
    "\n",
    "    print('********** After OverSampling **********')\n",
    "    print('Slight: ', (Y_oversampled == 'Slight').sum())\n",
    "    print('Serious:', (Y_oversampled == 'Serious').sum())\n",
    "    print('Fatal:  ', (Y_oversampled == 'Fatal').sum())\n",
    "    print('\\n Total X: ', len(X_oversampled), ' Total Y: ', len(Y_oversampled))\n",
    "\n",
    "    return X_oversampled, Y_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e306be-543d-4ba7-b9a4-2cb1cb938f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b61998b-bd46-4dad-a0e8-8363f92f7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gray_images(dataset, max_dimension, matrix_indexes):\n",
    "\n",
    "    matrix_3d = np.zeros((max_dimension, max_dimension, len(dataset.index)))\n",
    "    print(len(dataset.index))\n",
    "    for feature, value in matrix_indexes.items():\n",
    "        matrix_3d[value[0], value[1],] = dataset[feature]\n",
    "        \n",
    "    return matrix_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d6d9f-8ae4-4740-9983-e51d39cdeac6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ded491b-6d05-44f5-9ee9-44064babfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one channel\n",
    "# https://machinelearningmastery.com/a-gentle-introduction-to-channels-first-and-channels-last-image-formats-for-deep-learning/\n",
    "\n",
    "# Add one channel to gray images depending of the number of the data\n",
    "def shape_images(X_data, gray_images):\n",
    "  images = []\n",
    "\n",
    "  for i in range(0,len(X_data)):\n",
    "      original_matrix = gray_images[:,:,i]\n",
    "      # print(original_matrix.shape)\n",
    "      shaped_image = np.expand_dims(original_matrix, axis=2)\n",
    "      # print(shaped_image.shape)\n",
    "      images.append(shaped_image)\n",
    "      # plt.matshow(shaped_image)\n",
    "\n",
    "  return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71fa80-2c77-46a3-819c-bb7a0ff9a5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## One-Hot Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a2db8a-49a5-4a06-bf66-025a0d472001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casualty_to_one_hot(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        'Slight': 0,\n",
    "        'Serious': 1,\n",
    "        'Fatal': 2\n",
    "    }\n",
    "\n",
    "    Y_labels.replace(transf, inplace = True)\n",
    "\n",
    "    return tf.one_hot(Y_labels, 3)\n",
    "\n",
    "def one_hot_to_casualty(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        0: 'Slight',\n",
    "        1: 'Serious',\n",
    "        2: 'Fatal'\n",
    "    }   \n",
    "\n",
    "    return Y_labels.replace(transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021df180-f056-4e21-ac6e-39d41369831b",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a4a49-64d8-4a32-abf7-e626f39d0938",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29dfa69d-7135-476f-9c1d-4ebf5d30bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def correlation_matrix(X_data):\n",
    "    corrMatrix = X_data.corr()\n",
    "    fig,ax  = plt.subplots(1,1,figsize=(15,10))\n",
    "    sns.heatmap(corrMatrix, annot=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaa5de-34f0-4724-ba36-0fdcf19c64e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d011694-26fe-41e3-9a39-85722f128f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(X_train_data, X_test_data):\n",
    "    pca = PCA()\n",
    "    X_train_pca = pca.fit_transform(X_train_data)\n",
    "    X_test_pca  = pca.transform(X_test_data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    figure_name = plt.figure(figsize=(15, 10))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401456b5-b7b9-421c-83db-709796ee1d8e",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74c55dca-09c1-4831-a64c-bc04a9cdf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_TSNE(X_data, Y_data, n_components, output_file_name = None):\n",
    "\n",
    "    X_data_scaled = StandardScaler().fit_transform(X_data)\n",
    "    z_data = TSNE(n_components = n_components).fit_transform(X_data_scaled)\n",
    "\n",
    "    # X_test_scaled = StandardScaler().fit_transform(X_test),\n",
    "    # z_test = TSNE(n_components=2).fit_transform(X_test_scaled),\n",
    "\n",
    "    palette = sns.color_palette('husl', 3)\n",
    "    fig,ax  = plt.subplots(1,1,figsize=(15,10))\n",
    "    sns.scatterplot(x = z_data[:,0],\n",
    "                    y = z_data[:,1],\n",
    "                    hue = Y_data,\n",
    "                    palette = palette,\n",
    "                    legend = 'full')\n",
    "\n",
    "    if (output_file_name): plt.savefig('./Out/' + output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b4c26-5100-4723-9342-b7b9aa187d79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f4072a7-e2e5-4e45-9456-4bea3270a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder ():\n",
    "    input_img = Input(shape=(25,))\n",
    "\n",
    "    # definimos el encoder, que tendra una entrada de Input_img y una segunda capa con entrada de encoder1 y salida 3\n",
    "    encoder1 = layers.Dense(15, activation='sigmoid')(input_img)\n",
    "    encoder2 = layers.Dense(3, activation='sigmoid')(encoder1)\n",
    "\n",
    "    # definimos el  decoder que tendra una entrada inicial de encoder3 y una salida de 128 y finalmete una capa de salida con los mismos que Input_img\n",
    "    decoder1 = layers.Dense(15, activation='sigmoid')(encoder2)\n",
    "    decoder2 = layers.Dense(25, activation='sigmoid')(decoder1)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = tf.keras.Model(inputs=input_img, outputs=decoder2)\n",
    "    autoencoder.summary()\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy') #se usan estos dos en estas arquitecturas\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ea062-c0b7-4486-9ebe-94ae66a58b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TASP-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05497b70-0400-4219-8a07-3a43005066cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.6.0 and strictly below 2.9.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.5.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "2022-04-09 18:10:54.454947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.455137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 computeCapability: 6.1\n",
      "coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-04-09 18:10:54.455295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.455494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.455617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-04-09 18:10:54.455975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.456131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1050 computeCapability: 6.1\n",
      "coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s\n",
      "2022-04-09 18:10:54.456229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.456410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.456538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-04-09 18:10:54.456581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-04-09 18:10:54.456590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2022-04-09 18:10:54.456598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2022-04-09 18:10:54.456716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.456911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-09 18:10:54.457059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3370 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "lr_init = 0.1\n",
    "num_classes = 3\n",
    "\n",
    "tasp_cnn = models.Sequential()\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3),strides=(1, 1), activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "tasp_cnn.add(layers.BatchNormalization())\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "tasp_cnn.add(layers.BatchNormalization())\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "tasp_cnn.add(layers.BatchNormalization())\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "tasp_cnn.add(layers.BatchNormalization())  \n",
    "tasp_cnn.add(layers.Flatten())\n",
    "tasp_cnn.add(layers.Dense(units=128))\n",
    "tasp_cnn.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "tasp_cnn.compile(\n",
    "    optimizer=Adam(learning_rate = lr_init, epsilon=1e-06),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41d457bb-2db3-4b69-b0b7-bcf867dbeb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pcQtLPSUVwgf",
   "metadata": {
    "id": "pcQtLPSUVwgf",
    "tags": []
   },
   "source": [
    "# Leeds Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4tAAMniVQ-O",
   "metadata": {
    "id": "e4tAAMniVQ-O",
    "tags": []
   },
   "source": [
    "## Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d027c46-8e31-4698-86f1-38547595abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pandas --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6951ab4",
   "metadata": {
    "id": "e6951ab4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# https://datamillnorth.org/dataset/road-traffic-accidents\n",
    "# root_path = '/content/drive/Othercomputers/Mi portátil/Drive/Master UA/TFM/TFM_DATA/'\n",
    "\n",
    "root_path = './Data/Leeds/'\n",
    "\n",
    "file_path_2009 = './2009.csv'\n",
    "file_path_2010 = './2010.csv'\n",
    "file_path_2011 = './2011.csv'\n",
    "file_path_2012 = './2012.csv'\n",
    "file_path_2013 = './2013.csv'\n",
    "file_path_2014 = './2014.csv'\n",
    "file_path_2015 = './2015.csv'\n",
    "file_path_2016 = './2016.csv'\n",
    "\n",
    "file_2009 = pd.read_csv(root_path + file_path_2009, encoding = 'cp1252')\n",
    "file_2010 = pd.read_csv(root_path + file_path_2010, encoding = 'cp1252')\n",
    "file_2011 = pd.read_csv(root_path + file_path_2011, encoding = 'cp1252')\n",
    "file_2012 = pd.read_csv(root_path + file_path_2012, encoding = 'cp1252')\n",
    "file_2013 = pd.read_csv(root_path + file_path_2013, encoding = 'cp1252')\n",
    "file_2014 = pd.read_csv(root_path + file_path_2014, encoding = 'cp1252')\n",
    "file_2015 = pd.read_csv(root_path + file_path_2015, encoding = 'cp1252')\n",
    "file_2016 = pd.read_csv(root_path + file_path_2016, encoding = 'cp1252')\n",
    "\n",
    "###################### UNIÓN DE ARCHIVOS ######################\n",
    "\n",
    "a = pd.concat([file_2009,file_2010])\n",
    "\n",
    "file_2013 = clean_df = file_2013.loc[:, ~file_2013.columns.isin(['Casualty Class'])]\n",
    "file_2013.set_axis(a.columns, axis=1, inplace=True)\n",
    "                                             \n",
    "file_2014 = clean_df = file_2014.loc[:, ~file_2014.columns.isin(['Casualty Class'])]\n",
    "file_2014.set_axis(a.columns, axis=1, inplace=True)\n",
    "\n",
    "# file_2015 = clean_df = file_2015.loc[:, ~file_2015.columns.isin(['Casualty Class'])]\n",
    "file_2015.set_axis(a.columns, axis=1, inplace=True)\n",
    "file_2016 = clean_df = file_2016.loc[:, ~file_2016.columns.isin(['Expr1'])]\n",
    "file_2016.set_axis(a.columns, axis=1, inplace=True)\n",
    "\n",
    "a = pd.concat([a, file_2011])\n",
    "a = pd.concat([a, file_2012])\n",
    "a = pd.concat([a, file_2013])\n",
    "a = pd.concat([a, file_2014])\n",
    "a = pd.concat([a, file_2015])\n",
    "a = pd.concat([a, file_2016])\n",
    "\n",
    "# a['1st Road Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Y9SjctrVXCD",
   "metadata": {
    "id": "3Y9SjctrVXCD",
    "tags": []
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b726f75b",
   "metadata": {
    "id": "b726f75b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10266/4205016787.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['Weather Conditions'] = clean_df['Weather Conditions'].astype('int')\n",
      "/tmp/ipykernel_10266/4205016787.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['Casualty Class']     = clean_df['Casualty Class'].astype('int')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Easting</th>\n",
       "      <th>Northing</th>\n",
       "      <th>Number of Vehicles</th>\n",
       "      <th>Accident Time</th>\n",
       "      <th>1st Road Class</th>\n",
       "      <th>Road Surface</th>\n",
       "      <th>Lighting Conditions</th>\n",
       "      <th>Weather Conditions</th>\n",
       "      <th>Casualty Class</th>\n",
       "      <th>Casualty Severity</th>\n",
       "      <th>Sex of Casualty</th>\n",
       "      <th>Age of Casualty</th>\n",
       "      <th>Type of Vehicle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429093</td>\n",
       "      <td>436258</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>434723</td>\n",
       "      <td>435534</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Serious</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>441173</td>\n",
       "      <td>433047</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428487</td>\n",
       "      <td>431364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>425928</td>\n",
       "      <td>435480</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20302</th>\n",
       "      <td>423815</td>\n",
       "      <td>434248</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20303</th>\n",
       "      <td>427102</td>\n",
       "      <td>427700</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20304</th>\n",
       "      <td>419983</td>\n",
       "      <td>440944</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Serious</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20305</th>\n",
       "      <td>419983</td>\n",
       "      <td>440944</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20306</th>\n",
       "      <td>427222</td>\n",
       "      <td>433739</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Serious</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20307 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Easting  Northing  Number of Vehicles  Accident Time  1st Road Class  \\\n",
       "0       429093    436258                   1              2               6   \n",
       "1       434723    435534                   1              2               6   \n",
       "2       441173    433047                   1              1               6   \n",
       "3       428487    431364                   1              1               3   \n",
       "4       425928    435480                   2              1               6   \n",
       "...        ...       ...                 ...            ...             ...   \n",
       "20302   423815    434248                   2              2               6   \n",
       "20303   427102    427700                   2              2               6   \n",
       "20304   419983    440944                   2              1               3   \n",
       "20305   419983    440944                   2              1               3   \n",
       "20306   427222    433739                   1              1               6   \n",
       "\n",
       "       Road Surface  Lighting Conditions  Weather Conditions  Casualty Class  \\\n",
       "0                 1                    3                   1               3   \n",
       "1                 1                    3                   1               1   \n",
       "2                 1                    3                   1               3   \n",
       "3                 1                    3                   1               3   \n",
       "4                 1                    1                   1               1   \n",
       "...             ...                  ...                 ...             ...   \n",
       "20302             2                    3                   1               1   \n",
       "20303             2                    3                   1               1   \n",
       "20304             1                    3                   1               1   \n",
       "20305             1                    3                   1               3   \n",
       "20306             1                    3                   1               3   \n",
       "\n",
       "      Casualty Severity  Sex of Casualty  Age of Casualty  Type of Vehicle  \n",
       "0                Slight                1                3                7  \n",
       "1               Serious                2                2                7  \n",
       "2                Slight                2                1                7  \n",
       "3                Slight                1                1                7  \n",
       "4                Slight                2                3                7  \n",
       "...                 ...              ...              ...              ...  \n",
       "20302            Slight                1                3                3  \n",
       "20303            Slight                2                3                7  \n",
       "20304           Serious                2                3                7  \n",
       "20305            Slight                1                3                7  \n",
       "20306           Serious                1                3               17  \n",
       "\n",
       "[20307 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### DICCIONARIOS DE REEMPLAZO ######################\n",
    "# Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# A, A(M) y Motorway lo mismo?\n",
    "# B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "#               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "#               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "#               ¿Carretera como tal?\n",
    "\n",
    "# C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "#               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "road_class_replace = {\n",
    "    'Motorway': 1,\n",
    "    'A(M)': 2,\n",
    "    'A': 3,\n",
    "    'B': 4,\n",
    "    'C': 5,\n",
    "    'Unclassified': 6\n",
    "}\n",
    "\n",
    "##################################\n",
    "accident_date_replace = {\n",
    "    'Dry': 1,\n",
    "    'Wet / Damp': 2,\n",
    "    'Snow': 3,\n",
    "    'Frost / Ice': 4,\n",
    "    'Flood': 5,\n",
    "}\n",
    "##################################\n",
    "\n",
    "road_surface_replace = {\n",
    "    'Dry': 1,\n",
    "    'Wet / Damp': 2,\n",
    "    'Snow': 3,\n",
    "    'Frost/ Ice': 4,\n",
    "    'Frost / Ice': 4,\n",
    "    'Flood': 5,\n",
    "    'Flood (surface water over 3cm deep)': 5,\n",
    "    '5': 5\n",
    "}\n",
    "\n",
    "# La 5: \"Darkness: street lighting unknown\" no está presente en el paper, le hemos puesto un 5 porque sí #\n",
    "lighting_conditions_replace = {\n",
    "    'Daylight: street lights present': 1,\n",
    "    'Darkness: no street lighting': 2,\n",
    "    'Darkness: street lights present and lit': 3,\n",
    "    'Darkness: street lights present but unlit': 4,\n",
    "    'Darkness: street lighting unknown': 5,\n",
    "    '5': 5\n",
    "}\n",
    "\n",
    "# La 8.2: \"Unknown\" no está presente en el paper, le hemos puesto un 8 porque sí (Other) #\n",
    "weather_conditions_replace = {\n",
    "    'Fine without high winds': 1,\n",
    "    'Raining without high winds': 2,\n",
    "    'Snowing without high winds': 3,\n",
    "    'Fine with high winds': 4,\n",
    "    'Raining with high winds': 5,\n",
    "    'Snowing with high winds': 6,\n",
    "    'Fog or mist – if hazard': 7,\n",
    "    'Other': 8,\n",
    "    'Unknown': 8\n",
    "}\n",
    "\n",
    "type_of_vehicle_replace = {\n",
    "    'Pedal cycle': 1,\n",
    "    'M/cycle 50cc and under': 2,\n",
    "    'Motorcycle over 50cc and up to 125cc': 3,\n",
    "    'Motorcycle over 125cc and up to 500cc': 4,\n",
    "    'Motorcycle over 500cc': 5,\n",
    "    'Taxi/Private hire car': 6,\n",
    "    'Car': 7,\n",
    "    'Minibus (8 – 16 passenger seats)': 8,\n",
    "    'Bus or coach (17 or more passenger seats)': 9,\n",
    "    'Ridden horse': 10,\n",
    "    'Agricultural vehicle (includes diggers etc.)': 11,\n",
    "    'Tram / Light rail': 12,\n",
    "    'Goods vehicle 3.5 tonnes mgw and under': 13,\n",
    "    'Goods vehicle over 3.5 tonnes and under 7.5 tonnes mgw': 14,\n",
    "    'Goods vehicle 7.5 tonnes mgw and over': 15,\n",
    "    'Mobility Scooter': 16,\n",
    "    'Other Vehicle ': 17,\n",
    "    'Motorcycle - Unknown CC': 18\n",
    "}\n",
    "\n",
    "casualty_class_replace = {\n",
    "    'Driver': 1,\n",
    "    'Driver/Rider': 1,\n",
    "    'Driver or rider': 1,\n",
    "    'Passenger': 2,\n",
    "    'Vehicle or pillion passenger': 2,\n",
    "    'Pedestrian': 3\n",
    "}\n",
    "\n",
    "\n",
    "sex_of_casualty_replace = {\n",
    "    'Male': 1,\n",
    "    'Female': 2\n",
    "}\n",
    "\n",
    "###################### REEMPLAZOS ######################\n",
    "clean_df = clean_df.dropna()\n",
    "\n",
    "a['1st Road Class'].replace(road_class_replace, inplace = True)\n",
    "# print('1st Road Class:', a['1st Road Class'].unique())\n",
    "\n",
    "##################################\n",
    "# a['Accident Date'].replace(accident_date_replace, inplace = True)\n",
    "# print('Accident Date:', a['Accident Date'].unique())\n",
    "##################################\n",
    "a['Road Surface'].replace(road_surface_replace, inplace = True)\n",
    "a.dropna(inplace = True)\n",
    "\n",
    "a['Road Surface'] = a['Road Surface'].astype('int')\n",
    "# print('Road Surface:', a['Road Surface'].unique())\n",
    "\n",
    "a['Lighting Conditions'].replace(lighting_conditions_replace, inplace = True)\n",
    "# print('Lighting Conditions:', a['Lighting Conditions'].unique())\n",
    "\n",
    "a['Weather Conditions'].replace(weather_conditions_replace, inplace = True)\n",
    "a = a[a['Weather Conditions'] != 'Darkness: street lighting unknown']\n",
    "# print('Weather Conditions:', a['Weather Conditions'].unique())\n",
    "\n",
    "a['Type of Vehicle'].replace(type_of_vehicle_replace, inplace = True)\n",
    "# print('Type of Vehicle:', a['Type of Vehicle'].unique())\n",
    "\n",
    "a['Casualty Class'].replace(casualty_class_replace, inplace = True)\n",
    "# print('Casualty Class:', a['Casualty Class'].unique())\n",
    "\n",
    "a['Sex of Casualty'].replace(sex_of_casualty_replace, inplace = True)\n",
    "# print('Sex of Casualty:', a['Sex of Casualty'].unique())\n",
    "\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'] < 18, 1)\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'].between(18, 25), 2)\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'].between(25, 65), 3)\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'] > 65, 4)\n",
    "# print('Age of Casualty:', a['Age of Casualty'].unique())\n",
    "\n",
    "a['Time (24hr)'] = a['Time (24hr)'].mask(a['Time (24hr)'] < 600, 2)\n",
    "a['Time (24hr)'] = a['Time (24hr)'].mask(a['Time (24hr)'] > 1800, 2)\n",
    "a['Time (24hr)'] = a['Time (24hr)'].mask(a['Time (24hr)'].between(600, 1800), 1)\n",
    "# print('Time (24hr):', a['Time (24hr)'].unique())\n",
    "a.rename(columns={\"Time (24hr)\": \"Accident Time\"}, inplace = True)\n",
    "\n",
    "###################### LIMPIEZA DE VALORES NULOS/DUPLICADOS ######################\n",
    "\n",
    "clean_df = a.loc[:, ~a.columns.isin(['Accident Date', 'Reference Number'])]\n",
    "\n",
    "clean_df['Weather Conditions'] = clean_df['Weather Conditions'].astype('int')\n",
    "clean_df['Casualty Class']     = clean_df['Casualty Class'].astype('int')\n",
    "\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "clean_df = clean_df.dropna()\n",
    "clean_df = clean_df.reset_index(drop=True)\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JOSunlOuVeEm",
   "metadata": {
    "id": "JOSunlOuVeEm",
    "tags": []
   },
   "source": [
    "## Split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a506548e-6b11-4b46-abfb-bc7672c3f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scikit-learn --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c921e711",
   "metadata": {
    "id": "c921e711"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = clean_df['Casualty Severity']\n",
    "\n",
    "train, test = train_test_split(clean_df, test_size=0.2)\n",
    "X_train = train.loc[:, ~train.columns.isin(['Casualty Severity'])]\n",
    "Y_train = train['Casualty Severity']\n",
    "\n",
    "X_test = test.loc[:, ~test.columns.isin(['Casualty Severity'])]\n",
    "Y_test = test['Casualty Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "603c5d0e",
   "metadata": {
    "id": "603c5d0e"
   },
   "outputs": [],
   "source": [
    "# fv2gi(feature_vector)\n",
    "# sorted(feature_vector.items(), key = lambda item: item[0][1])\n",
    "\n",
    "# for item in feature_vector['Accident Features'].items():\n",
    "#     print(item[1])\n",
    "\n",
    "# feature_vector[parent_key].items()\n",
    "\n",
    "# sorted(feature_vector['Accident Features'].items(), key = lambda item: item,\n",
    "#                                reverse = True)\n",
    "\n",
    "# print(feature_vector['Accident Features']['feature_weights'])\n",
    "\n",
    "# fv = np.array(feature_vector['Accident Features']['feature_childs'])\n",
    "# list(fv[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gLtQ4-JMW0Tv",
   "metadata": {
    "id": "gLtQ4-JMW0Tv",
    "tags": []
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f97d173-a0ff-4b31-97e1-fd9d787069ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "FHb1AMbZjm9m",
   "metadata": {
    "id": "FHb1AMbZjm9m"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test  = X_test.astype(int)\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_test  = normalize_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nDVViKK3XCtE",
   "metadata": {
    "id": "nDVViKK3XCtE",
    "tags": []
   },
   "source": [
    "## Oversamplig de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5125433-f3ee-4cd3-bc5d-dd3a6acfca54",
   "metadata": {
    "id": "f5125433-f3ee-4cd3-bc5d-dd3a6acfca54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Before OverSampling **********\n",
      "Slight:  14235\n",
      "Serious: 1892\n",
      "Fatal:   118\n",
      "\n",
      " Total X: 16245  Total Y: 16245 \n",
      "\n",
      "********** After OverSampling **********\n",
      "Slight:  14235\n",
      "Serious: 14235\n",
      "Fatal:   14234\n",
      "\n",
      " Total X:  42704  Total Y:  42704\n"
     ]
    }
   ],
   "source": [
    "print('********** Before OverSampling **********')\n",
    "print('Slight: ', (Y_train == 'Slight').sum())\n",
    "print('Serious:', (Y_train == 'Serious').sum())\n",
    "print('Fatal:  ', (Y_train == 'Fatal').sum())\n",
    "print('\\n Total X:', len(X_train), ' Total Y:', len(Y_train), '\\n')\n",
    "\n",
    "X_train, Y_train = oversample_data(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a4a83-3e38-4848-be3f-8ae4861230b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d2115fc-0d52-4bd4-883d-72bbc28f44aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69081589-f481-4d74-94d4-7b175655424a",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182b535-b21b-40b5-b3cc-ba436ca59f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc61265c-a5ea-4e87-a0d0-61fb2af14304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = 'madrid_hyperparams_v7.json'\n",
    "\n",
    "# best_hyperparams = load_weights(HYPERPARAMS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1064316-3dfa-4bf2-8f31-933ec98df452",
   "metadata": {},
   "source": [
    "#### Cálculo de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21a281da-a027-477b-bda5-0b41257ffaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n",
      "/home/luis/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "\n",
      "job exception: [18:13:25] ../src/data/../common/common.h:157: XGBoost version not compiled with GPU support.\n",
      "Stack trace:\n",
      "  [bt] (0) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x127fa5) [0x7fabc0625fa5]\n",
      "  [bt] (1) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x128a8c) [0x7fabc0626a8c]\n",
      "  [bt] (2) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0xa5) [0x7fabc068ed75]\n",
      "  [bt] (3) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x1f7) [0x7fabc06a27c7]\n",
      "  [bt] (4) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x1cbf14) [0x7fabc06c9f14]\n",
      "  [bt] (5) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(XGBoosterBoostedRounds+0x2b) [0x7fabc059ceab]\n",
      "  [bt] (6) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fad263879dd]\n",
      "  [bt] (7) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/../../libffi.so.7(+0x6067) [0x7fad26387067]\n",
      "  [bt] (8) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x140f6) [0x7fad263a10f6]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                             | 0/5 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[18:13:25] ../src/data/../common/common.h:157: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x127fa5) [0x7fabc0625fa5]\n  [bt] (1) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x128a8c) [0x7fabc0626a8c]\n  [bt] (2) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0xa5) [0x7fabc068ed75]\n  [bt] (3) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x1f7) [0x7fabc06a27c7]\n  [bt] (4) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x1cbf14) [0x7fabc06c9f14]\n  [bt] (5) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(XGBoosterBoostedRounds+0x2b) [0x7fabc059ceab]\n  [bt] (6) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fad263879dd]\n  [bt] (7) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/../../libffi.so.7(+0x6067) [0x7fad26387067]\n  [bt] (8) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x140f6) [0x7fad263a10f6]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39maccuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK }\n\u001b[1;32m     37\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[0;32m---> 39\u001b[0m best_hyperparams \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                        \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/hyperopt/fmin.py:507\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    504\u001b[0m validate_loss_threshold(loss_threshold)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/hyperopt/base.py:682\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/hyperopt/fmin.py:553\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    550\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/hyperopt/fmin.py:356\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/hyperopt/fmin.py:292\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    289\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/hyperopt/fmin.py:170\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    168\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/hyperopt/base.py:907\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    900\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    902\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    903\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    904\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    905\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    906\u001b[0m     )\n\u001b[0;32m--> 907\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    910\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(space)\u001b[0m\n\u001b[1;32m     15\u001b[0m clf \u001b[38;5;241m=\u001b[39m XGBClassifier(n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     16\u001b[0m                     max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     17\u001b[0m                     gamma \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                     tree_method \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree_method\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m                    )\n\u001b[1;32m     24\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m [(X_train, Y_train), (X_test, Y_test)]\n\u001b[0;32m---> 26\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     32\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(Y_test, pred\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/core.py:506\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    505\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/sklearn.py:1250\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1230\u001b[0m model, feval, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, eval_metric, params)\n\u001b[1;32m   1231\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1232\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1233\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     label_transform\u001b[38;5;241m=\u001b[39mlabel_transform,\n\u001b[1;32m   1248\u001b[0m )\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, evals\u001b[38;5;241m=\u001b[39m(), obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m           maximize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, evals_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m           verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xgb_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124;03m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Booster : a trained booster model\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     bst \u001b[38;5;241m=\u001b[39m \u001b[43m_train_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bst\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/training.py:76\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _configure_deprecated_callbacks(\n\u001b[1;32m     72\u001b[0m         verbose_eval, early_stopping_rounds, maximize, start_iteration,\n\u001b[1;32m     73\u001b[0m         num_boost_round, feval, evals_result, callbacks,\n\u001b[1;32m     74\u001b[0m         show_stdv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, cvfolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iteration, num_boost_round):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/callback.py:378\u001b[0m, in \u001b[0;36mCallbackContainer.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m'''Function called before training.'''\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 378\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_training should return the model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cv:\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/callback.py:538\u001b[0m, in \u001b[0;36mEarlyStopping.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarting_round \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_boosted_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/core.py:2186\u001b[0m, in \u001b[0;36mBooster.num_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2184\u001b[0m rounds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2186\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterBoostedRounds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rounds\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/core.py:218\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [18:13:25] ../src/data/../common/common.h:157: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x127fa5) [0x7fabc0625fa5]\n  [bt] (1) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x128a8c) [0x7fabc0626a8c]\n  [bt] (2) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(xgboost::gbm::GBTree::ConfigureUpdaters()+0xa5) [0x7fabc068ed75]\n  [bt] (3) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(xgboost::gbm::GBTree::Configure(std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > const&)+0x1f7) [0x7fabc06a27c7]\n  [bt] (4) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(+0x1cbf14) [0x7fabc06c9f14]\n  [bt] (5) /home/luis/anaconda3/envs/TFM/lib/libxgboost.so(XGBoosterBoostedRounds+0x2b) [0x7fabc059ceab]\n  [bt] (6) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/../../libffi.so.7(+0x69dd) [0x7fad263879dd]\n  [bt] (7) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/../../libffi.so.7(+0x6067) [0x7fad26387067]\n  [bt] (8) /home/luis/anaconda3/envs/TFM/lib/python3.9/lib-dynload/_ctypes.cpython-39-x86_64-linux-gnu.so(+0x140f6) [0x7fad263a10f6]\n\n"
     ]
    }
   ],
   "source": [
    "Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 25, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 15, 1),\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 4000, 100),\n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    clf = XGBClassifier(n_estimators = int(space['n_estimators']),\n",
    "                        max_depth = int(space['max_depth']),\n",
    "                        gamma = space['gamma'],\n",
    "                        reg_alpha = int(space['reg_alpha']),\n",
    "                        min_child_weight = int(space['min_child_weight']),\n",
    "                        colsample_bytree = int(space['colsample_bytree']),\n",
    "                        tree_method = space['tree_method']\n",
    "                       )\n",
    "    \n",
    "    evaluation = [(X_train, Y_train), (X_test, Y_test)]\n",
    "    \n",
    "    clf.fit(X_train, Y_train,\n",
    "            eval_set = evaluation, eval_metric = \"auc\",\n",
    "            early_stopping_rounds = 10, verbose = False)\n",
    "            \n",
    "    \n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, pred>0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 500,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fabde-6f45-4c38-9800-6d568b65d526",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b19a759-7ba4-45a6-a1b0-b74589527b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'leeds_hyperparams' + MODEL_TIMESTAMP + '.json'\n",
    "\n",
    "write_weights(best_hyperparams, HYPERPARAMS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b0507-1bc1-4223-bf64-84d27f5121de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a8ddd-0cea-4817-b248-153387ae9a1f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4634264-34da-4355-af3f-99650d45068d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga definitiva/auxiliar de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc51d38-c18a-46e7-968d-4a8e40b91d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FILE_NAME = 'leeds_calculated_weights.json'\n",
    "FILE_NAME = 'leeds_weights2022-04-09-11:07:27.json'\n",
    "\n",
    "feature_vector = load_weights(WEIGHTS_PATH, FILE_NAME)\n",
    "display(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e7364-7e4b-4ecf-9af2-559deb20df63",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cálculo de pesos de caracetrísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e4a99-5e84-4c6e-af5c-7451e342ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier,XGBRanker\n",
    "from matplotlib import pyplot\n",
    "from xgboost import plot_importance\n",
    "\n",
    "xgboost = XGBClassifier(best_hyperparams)\n",
    "\n",
    "xgboost.fit(X_train, Y_train)\n",
    "\n",
    "child_weights  = np.array(xgboost.feature_importances_)\n",
    "feature_vector = fill_feature_vector(X_train, child_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e34dce-2ff6-474c-b5cc-09362f4d64e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualización pesos calculados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb50b3c-235c-4c50-b8ab-ff4c199e8a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FILE_PATH = './XGBoost_Weights/'\n",
    "# FILE_NAME = 'leeds_figure_weights' + MODEL_TIMESTAMP + '.jpg'\n",
    "\n",
    "# print(xgboost.get_booster().get_score(importance_type= 'weight'))\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.barh(X_train.columns, xgboost.feature_importances_)\n",
    "# plt.savefig(FILE_PATH + FILE_NAME)\n",
    "\n",
    "# for column, weight in zip(X_train.columns,xgboost.feature_importances_):\n",
    "#   print(column, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ffbda-1e05-4edc-84c2-5d8a94796af4",
   "metadata": {},
   "source": [
    "#### Escritura de pesos de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f0895-d1e0-49a4-a88c-a91cfd9f119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_indexes = fv2gi(feature_vector)\n",
    "\n",
    "FILE_NAME = 'leeds_weights' + MODEL_TIMESTAMP + '.json'\n",
    "# FILE_NAME = 'leeds_default_weights.json'\n",
    "\n",
    "write_weights(feature_vector, WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291b03b-03f6-44a8-8030-987312e24667",
   "metadata": {},
   "source": [
    "### Cálculo índices de matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209fd454-5668-4ab9-bc58-c30a546ac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fnVqjYFXk",
   "metadata": {
    "id": "4b3fnVqjYFXk",
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcf79f",
   "metadata": {
    "id": "21dcf79f"
   },
   "outputs": [],
   "source": [
    "train_bgi = build_gray_images(X_train, 5, matrix_indexes)\n",
    "test_bgi  = build_gray_images(X_test, 5, matrix_indexes)\n",
    "\n",
    "pd.DataFrame(train_bgi[:,:,1057])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_5saNqHWX4C_",
   "metadata": {
    "id": "_5saNqHWX4C_"
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdca6dd",
   "metadata": {
    "id": "bbdca6dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images = shape_images(X_data = X_train,\n",
    "                            gray_images = train_bgi)\n",
    "test_images  = shape_images(X_data = X_test,\n",
    "                            gray_images = test_bgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5e9c9",
   "metadata": {
    "id": "15a5e9c9"
   },
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "for i in range(0,3):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(train_bgi[:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad520774-b249-4575-855a-4a4fee307bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iu2JxxiUGkS3",
   "metadata": {
    "id": "iu2JxxiUGkS3"
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "resized_img = train_bgi[:,:,1]\n",
    "\n",
    "for i in range(0,3):\n",
    "  fd, hog_image = hog(train_bgi[:,:,i], orientations=9, pixels_per_cell=(2, 2),\n",
    "                    cells_per_block=(2, 2), visualize=True)\n",
    "  plt.figure(figsize=(3, 3))\n",
    "  plt.imshow(hog_image)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a9bfb",
   "metadata": {
    "id": "609a9bfb"
   },
   "outputs": [],
   "source": [
    "# input_shape = (5, 5)\n",
    "\n",
    "array_train_images = np.asarray(train_images)\n",
    "array_test_images  = np.asarray(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450b32b-4167-487f-b295-3d841ebfda70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab443960-cace-48c5-ad61-2209386324f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb121c88",
   "metadata": {
    "id": "cb121c88"
   },
   "outputs": [],
   "source": [
    "######### EN TERMINAL #########\n",
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fm72aQrpYW7V",
   "metadata": {
    "id": "fm72aQrpYW7V"
   },
   "source": [
    "## One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afbb97",
   "metadata": {
    "id": "01afbb97"
   },
   "outputs": [],
   "source": [
    "Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "Y_test_onehot  = casualty_to_one_hot(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V7Azjtl8gRth",
   "metadata": {
    "id": "V7Azjtl8gRth",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SzhUTtEti5oE",
   "metadata": {
    "id": "SzhUTtEti5oE"
   },
   "outputs": [],
   "source": [
    "# !conda install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qKYh5EeThQ_7",
   "metadata": {
    "id": "qKYh5EeThQ_7",
    "tags": []
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PLcI-nQShVR1",
   "metadata": {
    "id": "PLcI-nQShVR1"
   },
   "outputs": [],
   "source": [
    "# correlation_matrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kISRP5AQhWTD",
   "metadata": {
    "id": "kISRP5AQhWTD",
    "tags": []
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285158d-69c6-4dfc-b728-6d4bd4b7cf1c",
   "metadata": {
    "id": "f285158d-69c6-4dfc-b728-6d4bd4b7cf1c"
   },
   "outputs": [],
   "source": [
    "# pca(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70370ce9-acc4-4fe7-bea0-c592909a305d",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec350b-8032-4ecc-a38e-9db86efb2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_name = './2d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=2)\n",
    "\n",
    "# output_file_name = './2d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=2)\n",
    "\n",
    "# output_file_name = './3d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=3)\n",
    "\n",
    "# output_file_name = './3d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EkC9ryU4lFwg",
   "metadata": {
    "id": "EkC9ryU4lFwg",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cCo2emMclT8h",
   "metadata": {
    "id": "cCo2emMclT8h",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4xBSSjInlHj1",
   "metadata": {
    "id": "4xBSSjInlHj1"
   },
   "outputs": [],
   "source": [
    "X_train = array_train_images\n",
    "X_test = array_test_images\n",
    "\n",
    "X_train = X_train.reshape(len(array_train_images), 25)\n",
    "X_test  = X_test.reshape(len(X_test), 25)\n",
    "\n",
    "# autoencoder().fit(X_train, X_train,\n",
    "#                 epochs=15,\n",
    "#                 batch_size=32,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gJfbDNO5oB1N",
   "metadata": {
    "id": "gJfbDNO5oB1N",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2NjR_MDJlXsJ",
   "metadata": {
    "id": "2NjR_MDJlXsJ"
   },
   "outputs": [],
   "source": [
    "# # create encoder model\n",
    "# encoder = Model(inputs=input_img, outputs=encoder2)\n",
    "# encoder.summary()\n",
    "# # create decoder model\n",
    "# encoded_input = Input(shape=(3,))\n",
    "# #lo que hace aqui es quedarse con las capas que corresponden al decodificador\n",
    "# decoder_layer1 = autoencoder.layers[-2]\n",
    "# decoder_layer2 = autoencoder.layers[-1]\n",
    "# decoder = Model(inputs=encoded_input, outputs=decoder_layer2(decoder_layer1(encoded_input)))\n",
    "# decoder.summary()\n",
    "# # si miramos la salida, son simetricos el uno respecto al otro\n",
    "# # encoder va de input a 3 y decoder de 3 a input\n",
    "\n",
    "# # get latent vector for visualization\n",
    "# latent_vector = encoder.predict(X_test)\n",
    "# # get decoder output to visualize reconstructed image\n",
    "# reconstructed_imgs = decoder.predict(latent_vector)\n",
    "\n",
    "\n",
    "# # visualize in 3D plot\n",
    "# from pylab import rcParams\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "# fig = plt.figure(1)\n",
    "# ax = Axes3D(fig)\n",
    "\n",
    "# xs = latent_vector[:, 0]\n",
    "# ys = latent_vector[:, 1]\n",
    "# zs = latent_vector[:, 2]\n",
    "\n",
    "# color=['red','green','blue']\n",
    "\n",
    "# for x, y, z, label in zip(xs, ys, zs, Y_test):\n",
    "#     c = color[int(label)]\n",
    "#     ax.text(x, y, z, label, backgroundcolor=c)\n",
    "    \n",
    "# ax.set_xlim(xs.min(), xs.max())\n",
    "# ax.set_ylim(ys.min(), ys.max())\n",
    "# ax.set_zlim(zs.min(), zs.max())\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# # X_test_encoded = encoder.predict(X_test, batch_size=32)\n",
    "# # plt.figure(figsize=(6, 6))\n",
    "# # plt.scatter(X_test_encoded[:, 0], X_test_encoded[:, 1], c=Y_test)\n",
    "# # plt.colorbar()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3666f-f387-4ce1-b0f7-f1e12e3a61e7",
   "metadata": {
    "id": "07b3666f-f387-4ce1-b0f7-f1e12e3a61e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_TSNE(X_data, Y_data, n_components, output_file_name=None):\n",
    "    X_data_scaled = StandardScaler().fit_transform(X_data)\n",
    "    z_data = TSNE(n_components=n_components).fit_transform(X_data_scaled)\n",
    "    # X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "    # z_test = TSNE(n_components=2).fit_transform(X_test_scaled)\n",
    "\n",
    "    palette = sns.color_palette(\"hls\", 3)\n",
    "    fig,ax  = plt.subplots(1,1,figsize=(15,10))\n",
    "    sns.scatterplot(x = z_data[:,0],\n",
    "                    y = z_data[:,1],\n",
    "                    hue = Y_data,\n",
    "                    palette = palette,\n",
    "                    legend = 'full')\n",
    "\n",
    "    if (output_file_name): plt.savefig('./Out/' + output_file_name)\n",
    "\n",
    "# output_file_name = './2d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=2)\n",
    "\n",
    "# output_file_name = './2d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=2)\n",
    "\n",
    "# output_file_name = './3d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=3)\n",
    "\n",
    "# output_file_name = './3d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qWl5nEFhfoLX",
   "metadata": {
    "id": "qWl5nEFhfoLX",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## TASP-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ilUM9ijCge",
   "metadata": {
    "id": "81ilUM9ijCge",
    "tags": []
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xQuI-4TFjBEV",
   "metadata": {
    "id": "xQuI-4TFjBEV"
   },
   "outputs": [],
   "source": [
    "# input_train_shape = (len(array_train_images), 5, 5, 1)\n",
    "# input_test_shape = (len(array_test_images), 5, 5, 1)\n",
    "\n",
    "# array_train_images.reshape(input_train_shape)\n",
    "# array_test_images.reshape(input_test_shape)\n",
    "\n",
    "# history = tasp_cnn.fit(array_train_images, Y_train_onehot,\n",
    "#                     batch_size = 128, epochs = 100, shuffle = True,\n",
    "#                     validation_data = (array_test_images, Y_test_onehot))\n",
    "\n",
    "# # history\n",
    "\n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff2b6f-08b6-4a3e-b286-a13c106ce651",
   "metadata": {},
   "source": [
    "### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0147af-5a2a-494f-a286-39476aa5c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasp_cnn.save(root_path + 'leeds_model_bayesian' + MODEL_TIMESTAMP + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aczocEwOkHlA",
   "metadata": {
    "id": "aczocEwOkHlA"
   },
   "source": [
    "### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v6rxkwFlkNz7",
   "metadata": {
    "id": "v6rxkwFlkNz7"
   },
   "outputs": [],
   "source": [
    "# tasp_cnn = tf.keras.models.load_model(root_path + 'leeds_model_bayesian' + MODEL_TIMESTAMP + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O548TVkJjpp5",
   "metadata": {
    "id": "O548TVkJjpp5"
   },
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8NivdSBoMFZE",
   "metadata": {
    "id": "8NivdSBoMFZE"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# Y_test_labels = one_hot_to_casualty(Y_test)\n",
    "\n",
    "# ########################################################################\n",
    "\n",
    "# # F1_SCORE_PATH = 'F1scores/'\n",
    "# # F1_SCORE_NAME = 'leeds_f1_score' + MODEL_TIMESTAMP\n",
    "\n",
    "# # ## Plot history: F1 SCORE\n",
    "# # figure_name = plt.figure(figsize=(20, 10))\n",
    "# # plt.plot(history.history['f1_score'], label='F1 score (training data)')\n",
    "# # plt.plot(history.history['val_f1_score'], label='F1 score (validation data)')\n",
    "# # plt.title('F1 score')\n",
    "# # plt.ylabel('F1 score value')\n",
    "# # plt.xlabel('No. epoch')\n",
    "# # plt.legend(loc=\"upper left\")\n",
    "# # plt.savefig(F1_SCORE_PATH + F1_SCORE_NAME + '.jpg')\n",
    "# # plt.show()\n",
    "\n",
    "# # print(history)\n",
    "\n",
    "# ########################################################################\n",
    "\n",
    "# # evaluate the network\n",
    "# print(\"[INFO] evaluating network...\")\n",
    "# predictions = tasp_cnn.predict(x=array_test_images, batch_size=128)\n",
    "\n",
    "# report = classification_report(tf.argmax(Y_test_onehot, axis=1),\n",
    "#                                predictions.argmax(axis=1),\n",
    "#                                target_names = Y_test_labels.unique(),\n",
    "#                                output_dict = True)\n",
    "\n",
    "# REPORTS_PATH = 'Reports/'\n",
    "# REPORT_NAME  = 'leeds_report' + MODEL_TIMESTAMP + '.csv'\n",
    "\n",
    "# report_df = pd.DataFrame(report).transpose()\n",
    "# report_df.to_csv(REPORTS_PATH + REPORT_NAME, index= True)\n",
    "\n",
    "# report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MN7Z-wpijxcE",
   "metadata": {
    "id": "MN7Z-wpijxcE"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# predictions = xgboost.predict(X_test)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9e577-d044-4f75-9682-619dcd7dabb7",
   "metadata": {
    "id": "2aa9e577-d044-4f75-9682-619dcd7dabb7"
   },
   "outputs": [],
   "source": [
    "# prediction_string = [str(prediction) for prediction in predictions]\n",
    "# pd.DataFrame(prediction_string).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oMQbX1j_zVOO",
   "metadata": {
    "id": "oMQbX1j_zVOO",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Madrid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pG-PDMY6zdqE",
   "metadata": {
    "id": "pG-PDMY6zdqE",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Importación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlkJKFfzBd8g",
   "metadata": {
    "id": "wlkJKFfzBd8g"
   },
   "source": [
    "- [Web Dataset](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=7c2843010d9c3610VgnVCM2000001f4a900aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default)\n",
    "\n",
    "- [Web documentación](https://datos.madrid.es/FWProjects/egob/Catalogo/Seguridad/Ficheros/Estructura_DS_Accidentes_trafico_desde_2019.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ic0tgQy47zEr",
   "metadata": {
    "id": "Ic0tgQy47zEr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# root_path = '/content/drive/Othercomputers/Mi portátil/Drive/Master UA/TFM/Incidentes de Trafico/Datasets/Madrid/'\n",
    "\n",
    "root_path = './Data/Madrid/'\n",
    "\n",
    "file_name_2019 = '2019_Accidentalidad.csv'\n",
    "file_name_2020 = '2020_Accidentalidad.csv'\n",
    "file_name_2021 = '2021_Accidentalidad.csv'\n",
    "file_name_2022 = '2022_Accidentalidad.csv'\n",
    "\n",
    "file_2019 = pd.read_csv(root_path + file_name_2019, sep=';')\n",
    "file_2020 = pd.read_csv(root_path + file_name_2020, sep=';')\n",
    "file_2021 = pd.read_csv(root_path + file_name_2021, sep=';')\n",
    "file_2022 = pd.read_csv(root_path + file_name_2022, sep=';')\n",
    "\n",
    "# print(len(file_2019[file_2019.cod_lesividad == 4]))\n",
    "# print(len(file_2020[file_2020.cod_lesividad == 4]))\n",
    "# print(len(file_2021[file_2021.lesividad == '4']))\n",
    "# print(len(file_2022[file_2022.lesividad == '4']))\n",
    "\n",
    "COLUMNS_TO_REMOVE = ['cod_distrito',\n",
    "                     'tipo_lesividad'\n",
    "                    ]\n",
    "\n",
    "data_frame = file_2019\n",
    "data_frame = pd.concat([data_frame, file_2020])\n",
    "\n",
    "data_frame.rename(columns={\"cod_lesividad\": \"lesividad\"}, inplace = True)\n",
    "data_frame.rename(columns={\"tipo_vehículo\": \"tipo_vehiculo\"}, inplace = True)\n",
    "data_frame = data_frame.drop(COLUMNS_TO_REMOVE, axis=1)\n",
    "\n",
    "data_frame = pd.concat([data_frame, file_2021])\n",
    "\n",
    "data_frame.dropna(subset=['lesividad'], inplace = True)\n",
    "data_frame.lesividad = data_frame.lesividad.replace(' ', 14).astype(int)\n",
    "data_frame = data_frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5PmJpoCCcxMJ",
   "metadata": {
    "id": "5PmJpoCCcxMJ"
   },
   "source": [
    "### Calcular Vehículos implicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utgDSVryALnm",
   "metadata": {
    "id": "utgDSVryALnm"
   },
   "source": [
    "A partir del número de expediente (un mismo expediente en varias filas quiere decir que se trata del mismo accidente) se hace un `groupby` a partir de él. Como el atributo `positiva_alcohol` no tiene valores nulos en ninguna de las filas, hacemos un conteo a partir de él y se asigna a una nueva columna `positiva_alcohol_rename` que posteriormente será renombrada como `vehiculos_implicados`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qy9UBWGFan1j",
   "metadata": {
    "id": "qy9UBWGFan1j"
   },
   "outputs": [],
   "source": [
    "data_frame = data_frame.join(data_frame.groupby('num_expediente')['positiva_alcohol'].count(), on='num_expediente', rsuffix='_rename')\n",
    "data_frame.rename(columns={\"positiva_alcohol_rename\": \"vehiculos_implicados\"}, errors=\"raise\", inplace=True)\n",
    "data_frame = data_frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3lX-EnJ1aby",
   "metadata": {
    "id": "e3lX-EnJ1aby",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ybjvOI7x0PKz",
   "metadata": {
    "id": "ybjvOI7x0PKz",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Clasificación de carreteras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c43193-d332-4c5e-a173-ce0877ad9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### SIGUIENTE CELDA #########################\n",
    "\n",
    "# Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# A, A(M) y Motorway lo mismo?\n",
    "# B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "#               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "#               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "#               ¿Carretera como tal?\n",
    "\n",
    "# C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "#               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "\n",
    "# Unclassified\n",
    "\n",
    "\n",
    "regex = {}\n",
    "regex['paseo_regex'] = 'PASEO|paseo'\n",
    "regex['parque_regex'] = 'PARQUE|PQUE'\n",
    "regex['ronda_regex'] = 'RONDA'\n",
    "regex['puerta_regex'] = 'PUERTA|PTA|Puerta'\n",
    "regex['puente_regex'] = 'PNTE|PUENTE'\n",
    "regex['plaza_regex'] = 'PLAZA|PZA'\n",
    "regex['camino_regex']= 'CMNO|CAMINO'\n",
    "regex['bulevard_regex'] = 'BULE'\n",
    "regex['travesia_regex'] = 'TRVA'\n",
    "regex['cuesta_regex'] = 'CUSTA|CUESTA'\n",
    "regex['rotonda_regex'] = 'GTA|gta|GLORIETA|glorieta|ROTONDA'\n",
    "regex['aeropuerto_regex'] ='AEROPUERTO|AEROP'\n",
    "regex['calzada_regex'] = 'CALZADA'\n",
    "regex['poligono_regex'] ='POLIGONO'\n",
    "regex['highway_regex'] = 'AUTOV.|autovia|A-|M-|M 30|m 30|A\\\\d|M 23|M23' # A,A(M),Motorway\n",
    "regex['road_regex'] = 'CTRA.|CARRETERA|carretera|CRA.' # B\n",
    "regex['avenida_regex'] = 'AVDA|AV|AVENIDA|AVDA|Avda.|avenida'\n",
    "regex['calle_regex']  = 'CALL.|Calle|CALLE|c/|C/|C.|calle'\n",
    "\n",
    "data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "for index,regex_values in enumerate(regex.values()):\n",
    "    print(regex_values)\n",
    "    regex_indexes = data_frame[data_frame.localizacion.str.contains(regex_values,  case = True, regex=True)].index\n",
    "    print(len(regex_indexes))\n",
    "    data_frame.iloc[regex_indexes, data_frame.columns.get_loc('tipo_via')] = str(index)\n",
    "    data_frame.iloc[regex_indexes, data_frame.columns.get_loc('localizacion')] = str(index)\n",
    "    \n",
    "    \n",
    "    \n",
    "# street_indexes  = data_frame[data_frame.localizacion.str.contains('CALL.|Calle|CALLE|c/|C/|C.|calle', case = True, regex=True)].index\n",
    "# highway_indexes = data_frame[data_frame.localizacion.str.contains(highway_regex, case = True, regex=True)].index\n",
    "# road_indexes    = data_frame[data_frame.localizacion.str.contains(road_regex, case = True, regex=True)].index\n",
    "# # avenue_indexes  = data_frame[data_frame.localizacion.str.contains(avenue_regex,  case = True, regex=True)].index\n",
    "# # ride_indexes    = data_frame[data_frame.localizacion.str.contains(ride_regex, case = True, regex=True)].index\n",
    "\n",
    "# data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('tipo_via')] = 'Unclassified'\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('tipo_via')] = 'A'\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('tipo_via')] = 'B'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "\n",
    "\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('localizacion')] = 1\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('localizacion')] = 2\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('localizacion')] = 3\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('localizacion')] = '3'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('localizacion')] = '5'\n",
    "\n",
    "positive_drug_indexes = data_frame[data_frame.positiva_droga == 1].index\n",
    "data_frame.iloc[positive_drug_indexes, data_frame.columns.get_loc('positiva_alcohol')] = 'S'\n",
    "\n",
    "data_frame = data_frame[~(data_frame.tipo_via == 'N/A')]\n",
    "# # print(data_frame.localizacion.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73aa705-11d7-422b-8734-8c3944fe5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.tipo_via.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da252aZ0N3n",
   "metadata": {
    "id": "7da252aZ0N3n"
   },
   "outputs": [],
   "source": [
    "# ######################### SIGUIENTE CELDA #########################\n",
    "\n",
    "# # Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# # A, A(M) y Motorway lo mismo?\n",
    "# # B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "# #               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "# #               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "# #               ¿Carretera como tal?\n",
    "\n",
    "# # C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "# #               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "\n",
    "# # Unclassified\n",
    "# street_regex  = ('CALL.|Calle|CALLE|c/|C/|C.|calle|'\n",
    "#                  'AVDA|AV|AVENIDA|AVDA|avenida|Avda.|'\n",
    "#                  'PASEO|paseo|'\n",
    "#                  'PARQUE|PQUE|'\n",
    "#                  'RONDA|'\n",
    "#                  'PUERTA|PTA|Puerta|'\n",
    "#                  'PNTE|PUENTE|'\n",
    "#                  'PLAZA|PZA|'\n",
    "#                  'CMNO|CAMINO|'\n",
    "#                  'BULE|'\n",
    "#                  'TRVA|'\n",
    "#                  'CUSTA|CUESTA|'\n",
    "#                  'GTA|gta|GLORIETA|glorieta|ROTONDA|'\n",
    "#                  'AEROPUERTO|AEROP'\n",
    "# )\n",
    "\n",
    "# highway_regex = 'AUTOV.|autovia|A-|M-|M 30|m 30|A\\\\d|M 23|M23' # A,A(M),Motorway\n",
    "# road_regex = 'CTRA.|CARRETERA|carretera|CRA.|CALZADA|POLIGONO' # B\n",
    "\n",
    "# street_indexes  = data_frame[data_frame.localizacion.str.contains(street_regex,  case = True, regex=True)].index\n",
    "# highway_indexes = data_frame[data_frame.localizacion.str.contains(highway_regex, case = True, regex=True)].index\n",
    "# road_indexes    = data_frame[data_frame.localizacion.str.contains(road_regex, case = True, regex=True)].index\n",
    "# # avenue_indexes  = data_frame[data_frame.localizacion.str.contains(avenue_regex,  case = True, regex=True)].index\n",
    "# # ride_indexes    = data_frame[data_frame.localizacion.str.contains(ride_regex, case = True, regex=True)].index\n",
    "\n",
    "# data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('tipo_via')] = 'Unclassified'\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('tipo_via')] = 'A'\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('tipo_via')] = 'B'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "\n",
    "\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('localizacion')] = 1\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('localizacion')] = 2\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('localizacion')] = 3\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('localizacion')] = '3'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('localizacion')] = '5'\n",
    "\n",
    "# positive_drug_indexes = data_frame[data_frame.positiva_droga == 1].index\n",
    "# data_frame.iloc[positive_drug_indexes, data_frame.columns.get_loc('positiva_alcohol')] = 'S'\n",
    "\n",
    "# data_frame = data_frame[~(data_frame.tipo_via == 'N/A')]\n",
    "# # print(data_frame.localizacion.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc350245-b09e-4de1-9ecb-3d2f8d60ba7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Consideraciones:\n",
    "\n",
    "- Los patinetes se han considerado como ciclomotres de menos de 50cc.\n",
    "- Las furgonetas se consideran como vehículos de menos de 3.5 toneladas.\n",
    "- Maquinaria de obras se considera la misma tipología que maquinaria agrícola.\n",
    "- Cuadriciclos ligeros y no ligeros se consideran como `Motorcycle-Unknown CC`.\n",
    "- Patinetes y Vehículos de Mobilidad Urbana se consideran como `Mobility Scooters`.\n",
    "- `Vehículo articulado` se considera como un vehículo de más de 7.5 toneladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RwdUhUHc1Up4",
   "metadata": {
    "id": "RwdUhUHc1Up4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_conditions_replace = {\n",
    "    'Despejado': 1,\n",
    "    'Nublado': 2,\n",
    "    'Lluvia débil': 3,\n",
    "    'LLuvia intensa': 4,\n",
    "    'Granizando':  5,\n",
    "    'Nevando': 6,\n",
    "    'Se desconoce': 7 \n",
    "}\n",
    "\n",
    "## CUIDADO CON Motocicleta hasta 125cc!!! HEMOS SUPUESTO QUE LOS CICLOMOTORES SON HASTA 50CC!!\n",
    "type_of_vehicle_replace = {\n",
    "    'Bicicleta': 1,\n",
    "    'Ciclo': 1,\n",
    "    'Bicicleta EPAC (pedaleo asistido)': 1,\n",
    "    'Ciclomotor': 2,\n",
    "    'Ciclomotor de dos ruedas L1e-B': 2,\n",
    "    'Ciclomotor de tres ruedas': 2,\n",
    "    'Motocicleta hasta 125cc': 3,\n",
    "    'Moto de tres ruedas hasta 125cc': 3,\n",
    "    'Motocicleta > 125cc': 4,\n",
    "    'Moto de tres ruedas > 125cc': 4,\n",
    "    'Turismo': 5,\n",
    "    'Todo terreno': 5,\n",
    "    'Microbús <= 17 plazas': 5,\n",
    "    'Autobús': 6,\n",
    "    'Autobus EMT': 6,\n",
    "    'Autobús articulado': 6,\n",
    "    'Autobús articulado EMT': 6,\n",
    "    'Maquinaria agrícola': 7,\n",
    "    'Maquinaria de obras': 8,\n",
    "    'Furgoneta': 9,        # Menos de 3.5 toneladas.\n",
    "    'Ambulancia SAMUR': 10,\n",
    "    'Autocaravana': 11,     # Entre 3.5 y 7.5 toneladas.\n",
    "    'Camión rígido': 12,    # Mayor que 7.5 toneladas.\n",
    "    'Tractocamión': 12,\n",
    "    'Vehículo articulado': 12,\n",
    "    'Camión de bomberos': 12,\n",
    "    'VMU eléctrico': 13,\n",
    "    'Patinete': 13,\n",
    "    'Sin especificar': 14,\n",
    "    'Otros vehículos sin motor': 14,\n",
    "    'Remolque': 14,\n",
    "    'Semiremolque': 14,\n",
    "    'Otros vehículos con motor': 15,\n",
    "    'Cuadriciclo ligero': 15,\n",
    "    'Cuadriciclo no ligero': 15,\n",
    "    'Motorcycle - Unknown CC': 15\n",
    "}\n",
    "\n",
    "# type_of_vehicle_replace = {}\n",
    "# for index,tipo_vehiculo in enumerate(data_frame.tipo_vehiculo.unique()):\n",
    "#     if not pd.isna(tipo_vehiculo): type_of_vehicle_replace[tipo_vehiculo] = index\n",
    "\n",
    "casualty_class_replace = {\n",
    "    'Conductor': 1,\n",
    "    'Pasajero': 2,\n",
    "    'Peatón': 3\n",
    "}\n",
    "\n",
    "### CUIDADO CON DESCONOCIDO!!! MEJOR HACER IMPUTACIÓN PARA RELLENENAR LOS DESCONOCIDOS?\n",
    "sex_of_casualty_replace = {\n",
    "    'Hombre': 1,\n",
    "    'Mujer': 2,\n",
    "    'Desconocido': 3\n",
    "}\n",
    "\n",
    "accident_type_replace = {\n",
    "    'Colisión fronto-lateral': 1,\n",
    "    'Alcance': 2,\n",
    "    'Colisión lateral': 3,\n",
    "    'Choque contra obstáculo fijo': 4,\n",
    "    'Colisión múltiple': 5,\n",
    "    'Caída': 5,\n",
    "    'Atropello a persona': 6,\n",
    "    'Colisión frontal': 7,\n",
    "    'Otro': 8,\n",
    "    'Solo salida de la vía': 9,\n",
    "    'Vuelco': 10,\n",
    "    'Atropello a animal': 11,\n",
    "    'Despeñamiento': 12\n",
    "}\n",
    "\n",
    "alcohol_replace = {\n",
    "    'S': 1,\n",
    "    'N': 2,\n",
    "}\n",
    "\n",
    "accident_class_replace = {\n",
    "    1:  'Slight',  # Atención en urgencias sin posterior ingreso. - LEVE\n",
    "    2:  'Slight',  # Ingreso inferior o igual a 24 horas - LEVE\n",
    "    5:  'Slight',  # Asistencia sanitaria ambulatoria con posterioridad - LEVE\n",
    "    6:  'Slight',  # Asistencia sanitaria inmediata en centro de salud o mutua - LEVE\n",
    "    7:  'Slight',  # Asistencia sanitaria sólo en el lugar del accidente - LEVE\n",
    "    14: 'Slight',  # Sin asistencia sanitaria - LEVE O NADA\n",
    "    3:  'Serious', # Ingreso superior a 24 horas. - GRAVE\n",
    "    4:  'Fatal'    # Fallecido 24 horas - FALLECIDO \n",
    "}\n",
    "###################### REEMPLAZOS ######################\n",
    "\n",
    "# ### OJO QUE ESTAMOS REPLICANDO LA ESTRUCTURA DEL DATASET DE LEEDS\n",
    "age_replace = {\n",
    "    'Menor de 5 años': 1,\n",
    "    'De 6 a 9 años': 1,\n",
    "    'De 6  a  9 años': 1,\n",
    "    'De 10 a 14 años': 1,\n",
    "    'De 15 a 17 años': 1,\n",
    "    'De 18 a 20 años': 2,\n",
    "    'De 21 a 24 años': 2,\n",
    "    'De 25 a 29 años': 3,\n",
    "    'De 30 a 34 años': 3,\n",
    "    'De 35 a 39 años': 3,\n",
    "    'De 40 a 44 años': 3,\n",
    "    'De 45 a 49 años': 3,\n",
    "    'De 50 a 54 años': 3,\n",
    "    'De 55 a 59 años': 3,\n",
    "    'De 60 a 64 años': 3,\n",
    "    'De 65 a 69 años': 4,\n",
    "    'De 70 a 74 años': 4,\n",
    "    'Más de 74 años': 4,\n",
    "    'Desconocido': 5,\n",
    "}\n",
    "\n",
    "### OJO QUE ESTAMOS REPLICANDO LA ESTRUCTURA DEL DATASET DE LEEDS\n",
    "# age_replace = {\n",
    "#     'Menor de 5 años': 1,\n",
    "#     'De 6 a 9 años': 2,\n",
    "#     'De 6  a  9 años': 3,\n",
    "#     'De 10 a 14 años': 4,\n",
    "#     'De 15 a 17 años': 5,\n",
    "#     'De 18 a 20 años': 6,\n",
    "#     'De 21 a 24 años': 7,\n",
    "#     'De 25 a 29 años': 8,\n",
    "#     'De 30 a 34 años': 9,\n",
    "#     'De 35 a 39 años': 10,\n",
    "#     'De 40 a 44 años': 11,\n",
    "#     'De 45 a 49 años': 12,\n",
    "#     'De 50 a 54 años': 13,\n",
    "#     'De 55 a 59 años': 14,\n",
    "#     'De 60 a 64 años': 15,\n",
    "#     'De 65 a 69 años': 16,\n",
    "#     'De 70 a 74 años': 17,\n",
    "#     'Más de 74 años': 18,\n",
    "#     'Desconocido': 19,\n",
    "# }\n",
    "\n",
    "data_frame['estado_meteorológico'].replace(weather_conditions_replace, inplace = True)\n",
    "print('Estado meteorológico: \\n', data_frame['estado_meteorológico'].value_counts())\n",
    "\n",
    "data_frame['tipo_vehiculo'].replace(type_of_vehicle_replace, inplace = True)\n",
    "print('Tipo vehículo: \\n', data_frame['tipo_vehiculo'].value_counts())\n",
    "\n",
    "data_frame['tipo_persona'].replace(casualty_class_replace, inplace = True)\n",
    "print('Tipo de persona: \\n', data_frame['tipo_persona'].value_counts())\n",
    "\n",
    "data_frame['sexo'].replace(sex_of_casualty_replace, inplace = True)\n",
    "print('Sexo: \\n', data_frame['sexo'].value_counts())\n",
    "\n",
    "data_frame['positiva_alcohol'].replace(alcohol_replace, inplace = True)\n",
    "print('Positivo Alcohol: \\n', data_frame['positiva_alcohol'].value_counts())\n",
    "\n",
    "data_frame['lesividad'].replace(accident_class_replace, inplace = True)\n",
    "print('Gravedad: \\n', data_frame['lesividad'].value_counts())\n",
    "\n",
    "data_frame['rango_edad'].replace(age_replace, inplace = True)\n",
    "print('Edad: \\n', data_frame['rango_edad'].value_counts())\n",
    "\n",
    "data_frame.hora = data_frame.hora.mask(pd.to_datetime(data_frame.hora) < '06:00:00', 2)\n",
    "data_frame.hora = data_frame.hora.mask(pd.to_datetime(data_frame.hora) > '18:00:00', 2)\n",
    "data_frame.hora = data_frame.hora.mask(pd.to_datetime(data_frame.hora).between('06:00:00', '18:00:00'), 1)\n",
    "print('hora:', data_frame['hora'].value_counts())\n",
    "\n",
    "district_replace = {}\n",
    "for index,distrito in enumerate(data_frame.distrito.unique()):\n",
    "  if not pd.isna(distrito): district_replace[distrito] = int(index)\n",
    "\n",
    "accident_type_replace = {}\n",
    "for index,accident_type in enumerate(data_frame.tipo_accidente.unique()):\n",
    "    if not pd.isna(accident_type): accident_type_replace[accident_type] = int(index)\n",
    "\n",
    "data_frame['distrito'].replace(district_replace, inplace = True)\n",
    "print('Distrito: \\n', data_frame['distrito'].value_counts())\n",
    "\n",
    "data_frame['tipo_accidente'].replace(accident_type_replace, inplace = True)\n",
    "print('Tipo Accidente: \\n', data_frame['tipo_accidente'].value_counts())\n",
    "\n",
    "# Eliminamos aquellas lesividades desconocidas i.e. 77.\n",
    "data_frame = data_frame[data_frame.lesividad != 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pVPFGQ0AoNRD",
   "metadata": {
    "id": "pVPFGQ0AoNRD"
   },
   "source": [
    "### Coordenadas UTM a números enteros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nabg28LMAHhW",
   "metadata": {
    "id": "nabg28LMAHhW"
   },
   "source": [
    "Las coordenadas UTM son coordenads que están expresadas en término de X e Y partiendo de la base de que parten desde una determinada localización. Estas coordenadas constan de una parte entera y una decimal.\n",
    "\n",
    "En este dataset el formato que presentan estas coordenadas pueden ser de tres tipos:\n",
    "\n",
    "- **XXX.XXX.XXX**: en este caso los seis primeros dígitos forman la parte entera y los tres útlimos la parte decimal.\n",
    "- **XXXXXX,XX**: los seis primeros dígitos indican la parte entera, mientras que tras la coma aparecen dos dígitos de la parte decimal que habrá que completar añadiendo uno más.\n",
    "- **XXXXXX**: indican la parte entera, sin contar con la parte decimal.\n",
    "\n",
    "Por lo que el objetivo es estandarizar todos los formatos convirtiendo cada una de las coordenadas a un número entero, siendo necesario tratar con cada una de las casuísticas para añadir ceros a la derecha en caso de que falten para que cada una de las coordenadas tenga la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sgVHBwC0Fd1N",
   "metadata": {
    "id": "sgVHBwC0Fd1N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Todos las comas a puntos\n",
    "\n",
    "import re\n",
    "\n",
    "s = data_frame.coordenada_x_utm.str\n",
    "s_y = data_frame.coordenada_y_utm.str\n",
    "\n",
    "# Regex que hace match para dos grupos, la parte entera y la parte decimal.\n",
    "group_integer_and_float_pattern = '(?P<Integer>\\d{3}\\.\\d{3})(?P<Float>\\.\\d{2,3})'\n",
    "all_float_pattern   = '(?P<Number>\\d{6},\\d+)'\n",
    "all_integer_pattern = '(?P<Number>\\d{6}$)'\n",
    "\n",
    "group_integer_and_float_pattern_y = '(?P<Integer>\\d\\.\\d{3}\\.\\d{3})(?P<Float>\\.\\d{2,3})'\n",
    "all_float_pattern_y   = '(?P<Number>\\d{7},\\d+)'\n",
    "all_integer_pattern_y = '(?P<Number>\\d{7}$)'\n",
    "\n",
    "# Se extraen en un dataframe independiente ambas partes, la entera y la decimal\n",
    "index_and_extracted_x1 = s.extract(group_integer_and_float_pattern)\n",
    "index_and_extracted_x2 = s.extract(all_float_pattern)\n",
    "index_and_extracted_x3 = s.extract(all_integer_pattern)\n",
    "\n",
    "index_and_extracted_y1 = s_y.extract(group_integer_and_float_pattern_y)\n",
    "index_and_extracted_y2 = s_y.extract(all_float_pattern_y)\n",
    "index_and_extracted_y3 = s_y.extract(all_integer_pattern_y)\n",
    "\n",
    "# Se seleccionan aquellas que no continenen valores nulos el Float.\n",
    "# Es decir, aquellos con los que el match ha tenido éxito (los que llevan punto)\n",
    "# en lugar de comas.\n",
    "selected_rows_x1 = index_and_extracted_x1[~index_and_extracted_x1['Float'].isnull()]\n",
    "selected_rows_x2 = index_and_extracted_x2[~index_and_extracted_x2['Number'].isnull()]\n",
    "selected_rows_x3 = index_and_extracted_x3[~index_and_extracted_x3['Number'].isnull()]\n",
    "\n",
    "selected_rows_y1 = index_and_extracted_y1[~index_and_extracted_y1['Float'].isnull()]\n",
    "selected_rows_y2 = index_and_extracted_y2[~index_and_extracted_y2['Number'].isnull()]\n",
    "selected_rows_y3 = index_and_extracted_y3[~index_and_extracted_y3['Number'].isnull()]\n",
    "\n",
    "# Se cambia el string de la parte entera a un string sin puntos.\n",
    "selected_rows_x1.Integer = selected_rows_x1.Integer.str.replace('.','')\n",
    "selected_rows_x2.Number  = selected_rows_x2.Number.str.replace(',','.')\n",
    "\n",
    "selected_rows_y1.Integer = selected_rows_y1.Integer.str.replace('.','')\n",
    "selected_rows_y2.Number  = selected_rows_y2.Number.str.replace(',','.')\n",
    "\n",
    "# Se crea una nueva columna en el nuevo dataframe con la unión de la parte\n",
    "# entera y la parte decimal.\n",
    "selected_rows_x1['processed_x_utm'] = selected_rows_x1.Integer + selected_rows_x1.Float\n",
    "selected_rows_x2['processed_x_utm'] = selected_rows_x2.Number\n",
    "selected_rows_x3['processed_x_utm'] = selected_rows_x3.Number\n",
    "\n",
    "selected_rows_y1['processed_y_utm'] = selected_rows_y1.Integer + selected_rows_y1.Float\n",
    "selected_rows_y2['processed_y_utm'] = selected_rows_y2.Number\n",
    "selected_rows_y3['processed_y_utm'] = selected_rows_y3.Number\n",
    "\n",
    "data_frame['processed_x_utm'] = 'N/A'\n",
    "data_frame['processed_y_utm'] = 'N/A'\n",
    "\n",
    "# Si la longitud de alguno de los números es menor a diez, hay que añadirle x 0s\n",
    "# de diferencia\n",
    "selected_rows_x2.processed_x_utm = selected_rows_x2.processed_x_utm.transform(lambda x: x + '0'*(10-len(x)))\n",
    "selected_rows_x3.processed_x_utm = selected_rows_x3.processed_x_utm.transform(lambda x: x + '.000')\n",
    "\n",
    "selected_rows_y2.processed_y_utm = selected_rows_y2.processed_y_utm.transform(lambda x: x + '0'*(11-len(x)))\n",
    "selected_rows_y3.processed_y_utm = selected_rows_y3.processed_y_utm.transform(lambda x: x + '.000')\n",
    "\n",
    "data_frame['processed_x_utm'][selected_rows_x1.index] = selected_rows_x1['processed_x_utm']\n",
    "data_frame['processed_x_utm'][selected_rows_x2.index] = selected_rows_x2['processed_x_utm']\n",
    "data_frame['processed_x_utm'][selected_rows_x3.index] = selected_rows_x3['processed_x_utm']\n",
    "\n",
    "data_frame['processed_y_utm'][selected_rows_y1.index] = selected_rows_y1['processed_y_utm']\n",
    "data_frame['processed_y_utm'][selected_rows_y2.index] = selected_rows_y2['processed_y_utm']\n",
    "data_frame['processed_y_utm'][selected_rows_y3.index] = selected_rows_y3['processed_y_utm']\n",
    "\n",
    "# Eliminamos aquellas filas que no tienen coordenadas\n",
    "data_frame = data_frame[data_frame['coordenada_y_utm'] != '0.000']\n",
    "\n",
    "# Eliminamos el punto de la parte decimal para convertirlo a entero\n",
    "data_frame.processed_x_utm = data_frame.processed_x_utm.str.replace('.','')\n",
    "data_frame.processed_y_utm = data_frame.processed_y_utm.str.replace('.','')\n",
    "\n",
    "# Lo convertimos en entero\n",
    "data_frame.processed_x_utm = data_frame.processed_x_utm.astype(int)\n",
    "data_frame.processed_y_utm = data_frame.processed_y_utm.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Z4nz3ioxtXb",
   "metadata": {
    "id": "_Z4nz3ioxtXb"
   },
   "source": [
    "### Renombrado y eliminación de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqnlSOcN71Ah",
   "metadata": {
    "id": "tqnlSOcN71Ah"
   },
   "outputs": [],
   "source": [
    "COLUMNS_TO_REMOVE = ['num_expediente', 'fecha', 'tipo_via', 'numero', 'positiva_droga', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_droga']\n",
    "data_frame = data_frame.loc[:, ~data_frame.columns.isin(COLUMNS_TO_REMOVE)]\n",
    "\n",
    "data_frame.rename(columns={\"localizacion\": \"tipo_carretera\"}, errors=\"raise\", inplace=True)\n",
    "data_frame.rename(columns={\"processed_x_utm\": \"coordenada_x_utm\"}, errors=\"raise\", inplace=True)\n",
    "data_frame.rename(columns={\"processed_y_utm\": \"coordenada_y_utm\"}, errors=\"raise\", inplace=True)\n",
    "data_frame.rename(columns={\"positiva_alcohol\": \"drogas_alcohol_positivo\"}, errors=\"raise\", inplace=True)\n",
    "\n",
    "data_frame = data_frame.drop_duplicates()\n",
    "data_frame = data_frame.dropna()\n",
    "data_frame = data_frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qKuGQ1I8078E",
   "metadata": {
    "id": "qKuGQ1I8078E"
   },
   "source": [
    "## Split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCcJF3i8s3dD",
   "metadata": {
    "id": "NCcJF3i8s3dD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data_frame, test_size=0.2)\n",
    "X_train = train.loc[:, ~train.columns.isin(['lesividad'])]\n",
    "Y_train = train['lesividad']\n",
    "\n",
    "X_test = test.loc[:, ~test.columns.isin(['lesividad'])]\n",
    "Y_test = test['lesividad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x2PcFjlBmTlC",
   "metadata": {
    "id": "x2PcFjlBmTlC"
   },
   "outputs": [],
   "source": [
    "# feature_vector = {}\n",
    "\n",
    "# feature_vector['Accident Features'] = {\n",
    "#     'feature_childs': ['coordenada_x_utm', 'coordenada_y_utm', 'tipo_carretera', 'hora', 'vehiculos_implicados'],\n",
    "#     'feature_weights': [0.043706235, 0.04066877, 0.04932402, 0.037829854,  0.037388127]\n",
    "# } \n",
    "\n",
    "# feature_vector['Roadway Features'] = {\n",
    "#     'feature_childs': ['tipo_accidente', 'distrito'], # Road Surface \n",
    "#     'feature_weights': [0.07036541, 0]\n",
    "# } \n",
    "\n",
    "# feature_vector['Environmental Features'] = {\n",
    "#     'feature_childs': ['estado_meteorológico'],\n",
    "#     'feature_weights': [0.04621587]\n",
    "# } \n",
    "# feature_vector['Vehicle Features'] = {\n",
    "#     'feature_childs': ['tipo_vehiculo'],\n",
    "#     'feature_weights': [0.1835905]\n",
    "# }\n",
    "\n",
    "# feature_vector['Casualty Features'] = {\n",
    "#     'feature_childs': ['tipo_persona', 'sexo', 'rango_edad', 'drogas_alcohol_positivo'],\n",
    "#     'feature_weights': [0.29104254, 0.03937937, 0.054408602, 0.059951354]\n",
    "# } \n",
    "# matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5W4MXdIis6vn",
   "metadata": {
    "id": "5W4MXdIis6vn"
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tHUfNlw8sdxS",
   "metadata": {
    "id": "tHUfNlw8sdxS"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test  = X_test.astype(int)\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_test  = normalize_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kr_UChBJ21Cu",
   "metadata": {
    "id": "kr_UChBJ21Cu"
   },
   "source": [
    "## Oversampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rXwHLi842zLs",
   "metadata": {
    "id": "rXwHLi842zLs"
   },
   "outputs": [],
   "source": [
    "print('********** Before OverSampling **********')\n",
    "print('Slight: ', (Y_train == 'Slight').sum())\n",
    "print('Serious:', (Y_train == 'Serious').sum())\n",
    "print('Fatal:  ', (Y_train == 'Fatal').sum())\n",
    "print('\\n Total X:', len(X_train), ' Total Y:', len(Y_train), '\\n')\n",
    "\n",
    "X_train, Y_train = oversample_data(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eb99c-7ac9-425b-af2e-735ae2155e03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525349f-2832-457b-833f-e4f2d549ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48115b98-c658-4031-bdb1-a0024703952a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ea677-b0e1-43eb-9964-37dcb5c0241f",
   "metadata": {},
   "source": [
    "- [Bayesian Optimization with HYPEROPT](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40166b8f-b982-4cb1-bc9f-989b297970ad",
   "metadata": {},
   "source": [
    "#### Carga hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55470307-b090-4ba0-a615-b200b386b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'madrid_hyperparams' + MODEL_TIMESTAMP + '.json'\n",
    "\n",
    "best_hyperparams = load_weights(HYPERPARAMS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1453d7-ca0f-481e-a7a3-7a157dfa6ae3",
   "metadata": {},
   "source": [
    "#### Cálculo de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74624b8-92fb-4dd8-b163-6f2489bee52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 25, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 15, 1),\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 4000, 100),\n",
    "        # 'tree_method': 'gpu_hist'\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    clf = XGBClassifier(n_estimators = int(space['n_estimators']),\n",
    "                        max_depth = int(space['max_depth']),\n",
    "                        gamma = space['gamma'],\n",
    "                        reg_alpha = int(space['reg_alpha']),\n",
    "                        min_child_weight = int(space['min_child_weight']),\n",
    "                        colsample_bytree = int(space['colsample_bytree']),\n",
    "                        # tree_method = space['tree_method']\n",
    "                       )\n",
    "    \n",
    "    evaluation = [(X_train, Y_train), (X_test, Y_test)]\n",
    "    \n",
    "    clf.fit(X_train, Y_train,\n",
    "            eval_set = evaluation, eval_metric = \"auc\",\n",
    "            early_stopping_rounds = 10, verbose = False)\n",
    "            \n",
    "    \n",
    "    pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(Y_test, pred>0.5)\n",
    "    print (\"SCORE:\", accuracy)\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn = objective,\n",
    "                        space = space,\n",
    "                        algo = tpe.suggest,\n",
    "                        max_evals = 5,\n",
    "                        trials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1213fe-2a09-4686-8834-edc9ba60d2ea",
   "metadata": {},
   "source": [
    "#### Escritura hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad8965-a095-4b09-9cc3-915c4d2c5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_path = './hyperparams/'\n",
    "FILE_NAME = 'madrid_hyperparams' + MODEL_TIMESTAMP + '.json'\n",
    "\n",
    "write_weights(best_hyperparams, HYPERPARAMS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78381f1-6319-4f81-9f26-29a2ed8c9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e290436-2027-4bac-99c1-78870fd94419",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45db8d-ff82-41ac-8ca6-6bcce2d37659",
   "metadata": {},
   "source": [
    "#### Carga definitiva/auxiliar de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49493d-494c-45d1-ad26-5b621d5c4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = 'madrid_calculated_weights.json'\n",
    "FILE_NAME = 'madrid_weights2022-04-09-14:19:48.json'\n",
    "\n",
    "feature_vector = load_weights(WEIGHTS_PATH, FILE_NAME)\n",
    "display(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b64bd4-bc0c-4798-95a9-e7647b73aacd",
   "metadata": {},
   "source": [
    "#### Cálculo de pesos de caracetrísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d884f-1d98-4169-a633-74316ef68e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier(best_hyperparams)\n",
    "\n",
    "xgboost.fit(X_train, Y_train)\n",
    "\n",
    "child_weights  = np.array(xgboost.feature_importances_)\n",
    "feature_vector = fill_feature_vector(X_train, child_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f6472-a98c-4801-b5cc-31d0f46c13a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualización pesos calculados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc1536-1b5a-4d6a-922d-5f27f58e7396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FILE_PATH = './XGBoost_Weights/'\n",
    "# FILE_NAME = 'madrid_figure_weights' + MODEL_TIMESTAMP + '.jpg'\n",
    "\n",
    "# print(xgboost.get_booster().get_score(importance_type= 'weight'))\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.barh(X_train.columns, xgboost.feature_importances_)\n",
    "# plt.savefig(FILE_PATH + FILE_NAME)\n",
    "\n",
    "# print(xgboost.feature_importances_)\n",
    "\n",
    "# for column, weight in zip(X_train.columns,xgboost.feature_importances_):\n",
    "#   print(column, weight)\n",
    "\n",
    "# child_weights  = np.array(xgboost.feature_importances_)\n",
    "# feature_vector = fill_feature_vector(X_train, child_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278163f-8450-412d-8147-c235f6825646",
   "metadata": {},
   "source": [
    "#### Escritura de pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e3738-44ef-45be-9b24-989e4610267e",
   "metadata": {},
   "source": [
    "- v5: Pesos calculados con hiperparámetros. En el dataset están tipificados los vehículos como en el artículo, las edades no están en rango.\n",
    "- v6: Pesos calculados con hiperparámetros. En el dataset están tipificados los vehículos como en el artículo, las edades están en rango.\n",
    "- v7: hiperparams, tipos de carretera tipificados por vía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d2414-bf56-4519-aaf2-ae6333e69b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_indexes = fv2gi(feature_vector)\n",
    "\n",
    "FILE_NAME = 'madrid_weights' + MODEL_TIMESTAMP + '.json'\n",
    "# FILE_NAME = 'default_calculated_weights.json'\n",
    "\n",
    "write_weights(feature_vector, WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc80de-39c3-4818-b7eb-8a406aeed4e2",
   "metadata": {},
   "source": [
    "### Cálculo índices de matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd560eed-aa5b-47ad-a8fd-fb142a5d123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dR3Ah2X24fbw",
   "metadata": {
    "id": "dR3Ah2X24fbw"
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cWGYQ82UI4RM",
   "metadata": {
    "id": "cWGYQ82UI4RM"
   },
   "outputs": [],
   "source": [
    "train_bgi = build_gray_images(X_train, 5, matrix_indexes)\n",
    "test_bgi  = build_gray_images(X_test, 5, matrix_indexes)\n",
    "\n",
    "pd.DataFrame(train_bgi[:,:,1057])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yQTCws554zZL",
   "metadata": {
    "id": "yQTCws554zZL"
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mRrOKk3a43ZI",
   "metadata": {
    "id": "mRrOKk3a43ZI"
   },
   "outputs": [],
   "source": [
    "train_images = shape_images(X_data = X_train,\n",
    "                            gray_images = train_bgi)\n",
    "test_images  = shape_images(X_data = X_test,\n",
    "                            gray_images = test_bgi)\n",
    "\n",
    "plt.gray()\n",
    "for i in range(0,3):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(train_bgi[:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2d341-a847-437b-a1fb-0e40e01cced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images = shape_images(X_data = X_train,\n",
    "#                             gray_images = train_bgi)\n",
    "# test_images  = shape_images(X_data = X_test,\n",
    "#                             gray_images = test_bgi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1PdwhQuQ9o_P",
   "metadata": {
    "id": "1PdwhQuQ9o_P"
   },
   "source": [
    "## One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6hTctKZSI3re",
   "metadata": {
    "id": "6hTctKZSI3re"
   },
   "outputs": [],
   "source": [
    "Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "array_train_images = np.asarray(train_images)\n",
    "array_test_images  = np.asarray(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IStgg--F5L3F",
   "metadata": {
    "id": "IStgg--F5L3F",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rvyfvMPy5L3G",
   "metadata": {
    "id": "rvyfvMPy5L3G"
   },
   "outputs": [],
   "source": [
    "# !conda install -c anaconda seaborn --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dg0d7-k15L3H",
   "metadata": {
    "id": "dg0d7-k15L3H"
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SN7gwKNO5L3H",
   "metadata": {
    "id": "SN7gwKNO5L3H"
   },
   "outputs": [],
   "source": [
    "# correlation_matrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fAIUUg5L3J",
   "metadata": {
    "id": "d5fAIUUg5L3J"
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lhQElB3I5L3J",
   "metadata": {
    "id": "lhQElB3I5L3J"
   },
   "outputs": [],
   "source": [
    "# pca(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffbfe4-9fce-4e68-81b9-766d485b87ff",
   "metadata": {},
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669feae-6b17-4f05-b43b-6d3aa87a1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_name = './2d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=2)\n",
    "\n",
    "# output_file_name = './2d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=2)\n",
    "\n",
    "# output_file_name = './3d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=3)\n",
    "\n",
    "# output_file_name = './3d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XBbgOcIz5L3J",
   "metadata": {
    "id": "XBbgOcIz5L3J",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U84H7J695L3J",
   "metadata": {
    "id": "U84H7J695L3J"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A2mJaZVn5L3L",
   "metadata": {
    "id": "A2mJaZVn5L3L"
   },
   "outputs": [],
   "source": [
    "# input_img = Input(shape=(25,))\n",
    "\n",
    "# # definimos el encoder, que tendra una entrada de Input_img y una segunda capa con entrada de encoder1 y salida 3\n",
    "# encoder1 = layers.Dense(15, activation='sigmoid')(input_img)\n",
    "# encoder2 = layers.Dense(3, activation='sigmoid')(encoder1)\n",
    "\n",
    "# # definimos el  decoder que tendra una entrada inicial de encoder3 y una salida de 128 y finalmete una capa de salida con los mismos que Input_img\n",
    "# decoder1 = layers.Dense(15, activation='sigmoid')(encoder2)\n",
    "# decoder2 = layers.Dense(25, activation='sigmoid')(decoder1)\n",
    "\n",
    "# # this model maps an input to its reconstruction\n",
    "# autoencoder = tf.keras.Model(inputs=input_img, outputs=decoder2)\n",
    "# autoencoder.summary()\n",
    "\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy') #se usan estos dos en estas arquitecturas\n",
    "\n",
    "X_train = array_train_images\n",
    "X_test = array_test_images\n",
    "\n",
    "X_train = X_train.reshape(len(array_train_images), 25)\n",
    "X_test  = X_test.reshape(len(X_test), 25)\n",
    "\n",
    "# autoencoder.fit(X_train, X_train,\n",
    "#                 epochs=15,\n",
    "#                 batch_size=32,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gIiKplwP5L3L",
   "metadata": {
    "id": "gIiKplwP5L3L"
   },
   "source": [
    "#### Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opBOyrIx5L3M",
   "metadata": {
    "id": "opBOyrIx5L3M"
   },
   "outputs": [],
   "source": [
    "# # create encoder model\n",
    "# encoder = tf.keras.Model(inputs=input_img, outputs=encoder2)\n",
    "# encoder.summary()\n",
    "# # create decoder model\n",
    "# encoded_input = Input(shape=(3,))\n",
    "# #lo que hace aqui es quedarse con las capas que corresponden al decodificador\n",
    "# decoder_layer1 = autoencoder.layers[-2]\n",
    "# decoder_layer2 = autoencoder.layers[-1]\n",
    "# decoder = tf.keras.Model(inputs=encoded_input, outputs=decoder_layer2(decoder_layer1(encoded_input)))\n",
    "# decoder.summary()\n",
    "# # si miramos la salida, son simetricos el uno respecto al otro\n",
    "# # encoder va de input a 3 y decoder de 3 a input\n",
    "\n",
    "# # get latent vector for visualization\n",
    "# latent_vector = encoder.predict(X_test)\n",
    "# # get decoder output to visualize reconstructed image\n",
    "# reconstructed_imgs = decoder.predict(latent_vector)\n",
    "\n",
    "\n",
    "# # visualize in 3D plot\n",
    "# from pylab import rcParams\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "# fig = plt.figure(1)\n",
    "# ax = Axes3D(fig)\n",
    "\n",
    "# xs = latent_vector[:, 0]\n",
    "# ys = latent_vector[:, 1]\n",
    "# zs = latent_vector[:, 2]\n",
    "\n",
    "# # color=['red','green','blue']\n",
    "\n",
    "# # for x, y, z, label in zip(xs, ys, zs, Y_test):\n",
    "# #     c = color[int(label)]\n",
    "# #     ax.text(x, y, z, label, backgroundcolor=c)\n",
    "    \n",
    "# # ax.set_xlim(xs.min(), xs.max())\n",
    "# # ax.set_ylim(ys.min(), ys.max())\n",
    "# # ax.set_zlim(zs.min(), zs.max())\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# # X_test_encoded = encoder.predict(X_test, batch_size=32)\n",
    "# # plt.figure(figsize=(6, 6))\n",
    "# # plt.scatter(X_test_encoded[:, 0], X_test_encoded[:, 1], c=Y_test)\n",
    "# # plt.colorbar()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PohCQRSm67P0",
   "metadata": {
    "id": "PohCQRSm67P0",
    "tags": []
   },
   "source": [
    "## TASP-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vPh1ixx-67P2",
   "metadata": {
    "id": "vPh1ixx-67P2"
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b-rJTSQC67P2",
   "metadata": {
    "id": "b-rJTSQC67P2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "array_train_images = np.asarray(train_images)\n",
    "array_test_images  = np.asarray(test_images)\n",
    "\n",
    "display(array_train_images.shape)\n",
    "display(Y_train_onehot.shape)\n",
    "\n",
    "display(array_test_images.shape)\n",
    "display(Y_test_onehot.shape)\n",
    "\n",
    "input_train_shape = (len(array_train_images), 5, 5, 1)\n",
    "input_test_shape  = (len(array_test_images), 5, 5, 1)\n",
    "\n",
    "array_train_images = array_train_images.reshape(input_train_shape)\n",
    "array_test_images  = array_test_images.reshape(input_test_shape)\n",
    "\n",
    "\n",
    "history = tasp_cnn.fit(array_train_images, Y_train_onehot, verbose=2,\n",
    "                    batch_size = 128, epochs = 100, shuffle = True,\n",
    "                    validation_data = (array_test_images, Y_test_onehot))\n",
    "\n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dca0f9-336a-4113-b58c-1fb300c89608",
   "metadata": {},
   "source": [
    "### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac281b-8364-4710-b6e1-d75ae4a401d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasp_cnn.save(root_path + 'madrid_model_bayesian' + MODEL_TIMESTAMP + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aT8XDceKGSdi",
   "metadata": {
    "id": "aT8XDceKGSdi"
   },
   "source": [
    "### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dRaqg9SXGRwb",
   "metadata": {
    "id": "dRaqg9SXGRwb"
   },
   "outputs": [],
   "source": [
    "# tasp_cnn = tf.keras.models.load_model(root_path + 'madrid_model_bayesian' + MODEL_TIMESTAMP + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wD_BOwcwGb4W",
   "metadata": {
    "id": "wD_BOwcwGb4W"
   },
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHVVq0khGato",
   "metadata": {
    "id": "nHVVq0khGato"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_test_labels = one_hot_to_casualty(Y_test)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "F1_SCORE_PATH = 'F1scores/'\n",
    "F1_SCORE_NAME = 'madrid_f1_score' + MODEL_TIMESTAMP\n",
    "\n",
    "## Plot history: F1 SCORE\n",
    "figure_name = plt.figure(figsize=(20, 10))\n",
    "plt.plot(history.history['f1_score'], label='F1 score (training data)')\n",
    "plt.plot(history.history['val_f1_score'], label='F1 score (validation data)')\n",
    "plt.title('F1 score')\n",
    "plt.ylabel('F1 score value')\n",
    "plt.xlabel('No. epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(F1_SCORE_PATH + F1_SCORE_NAME + '.jpg')\n",
    "plt.show()\n",
    "\n",
    "print(history)\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = tasp_cnn.predict(x=array_test_images, batch_size=128)\n",
    "\n",
    "report = classification_report(tf.argmax(Y_test_onehot, axis=1),\n",
    "                               predictions.argmax(axis=1),\n",
    "                               target_names = Y_test_labels.unique(),\n",
    "                               output_dict = True)\n",
    "\n",
    "REPORTS_PATH = 'Reports/'\n",
    "REPORT_NAME  = 'madrid_report' + MODEL_TIMESTAMP + '.csv'\n",
    "\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(REPORTS_PATH + REPORT_NAME, index= True)\n",
    "\n",
    "report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DoJbgcgC1d83",
   "metadata": {
    "id": "DoJbgcgC1d83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tasp_cnn.save(root_path + 'madrid_model_XGBOOST_predicted.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "V7Azjtl8gRth",
    "qKYh5EeThQ_7",
    "kISRP5AQhWTD",
    "cCo2emMclT8h",
    "gJfbDNO5oB1N",
    "7a4EsWwQhe_i",
    "ycdOBuHSjhSk",
    "5PmJpoCCcxMJ",
    "ybjvOI7x0PKz",
    "pVPFGQ0AoNRD",
    "_Z4nz3ioxtXb",
    "dg0d7-k15L3H",
    "d5fAIUUg5L3J"
   ],
   "name": "TFM_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "tfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
