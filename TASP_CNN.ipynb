{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835b171f-bf99-42a4-addd-79c633d23f4b",
   "metadata": {},
   "source": [
    "[TFM](https://github.com/jmrplens/TFG-TFM_EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fb9ae-88b7-4246-8fb7-f7e904b6d049",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Diagrama de flujo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60478eb5-96bc-4235-801e-0bef3c9b2433",
   "metadata": {},
   "source": [
    "<center><img src=\"Data/Data_flow.svg\"/></center>\n",
    "\n",
    "Metodología\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=294849"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2e215-b652-4dbb-a951-8a62aff35046",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yabWKtrCSTTp",
   "metadata": {
    "id": "yabWKtrCSTTp",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Carga Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9kRlrtLcSWSU",
   "metadata": {
    "id": "9kRlrtLcSWSU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509f4f4-6594-4a0c-9bed-927d3922d6a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Versión y especificación de directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b404dc-0e9c-4945-b4e5-d1a77d863a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODEL_TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "\n",
    "WEIGHTS_PATH  = './feature_weights/'\n",
    "REPORTS_PATH  = 'Reports/'\n",
    "MODELS_PATH   = 'Models/'\n",
    "F1_SCORES_PATH = 'F1scores/'\n",
    "GA_SCORES_PATH = 'GA_Scores/'\n",
    "HYPERPARAMS_PATH = './hyperparams/'\n",
    "\n",
    "HYPERPARAMS_EVOLUTON_PATH = './hyperparams_evolution/'\n",
    "FINAL_POPULATION_PATH  = './population/'\n",
    "CONFUSIONS_MATRIX_PATH = 'confusion_matrix/'\n",
    "\n",
    "###### MODELS ######\n",
    "MODELS_NAME = ['knn', 'convolution_1d', 'convolution_2d', 'auto_ml']\n",
    "\n",
    "REPORTS_SUMMARY_PATH = f\"{REPORTS_PATH}summary/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb628e-a7f4-40e8-a62d-576f7a78c2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb40e62-06ae-46b9-a711-a5d5667c4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32053d4d-f4d8-4b07-9cd8-bb5bf4ac8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, BatchNormalization, Add, concatenate, Conv2DTranspose, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5732f07-8398-47a1-9d62-53cf7dd45556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 21:50:17.124139: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-14 21:50:17.185881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:17.251401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:17.251770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:18.134526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:18.134791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:18.134998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:18.135163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 3368 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b591c-8916-415a-a23c-8309f52f56e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importador/Exportador JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75269bf0-27f7-4b71-b311-256035370133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json(feature_vector, root_path, file_name):\n",
    "    with open(root_path + file_name, 'w') as outfile:\n",
    "        json.dump(feature_vector, outfile)\n",
    "\n",
    "def load_json(root_path, file_name):\n",
    "    with open(root_path + file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4388716-87a9-4e2d-ab4a-e3959fa1958f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b5661a-a9ac-4686-9611-6d43711d1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_feature_matrix_indexes(sorted_feature_vector,matrix):  \n",
    "\n",
    "    half_row = round((matrix.shape[0] - 1) / 2)\n",
    "    half_column = round((matrix.shape[1] - 1) / 2)\n",
    "\n",
    "    matrix_indexes = {}\n",
    "    \n",
    "    index = 0\n",
    "\n",
    "    for parent_key in sorted_feature_vector:\n",
    "        normalized_index = math.ceil(index/2)\n",
    "\n",
    "        if (index % 2 != 0): # Impar\n",
    "            current_row = half_row - normalized_index\n",
    "        else: # Par\n",
    "            current_row = half_row + normalized_index\n",
    "\n",
    "        sorted_child_indexes = np.argsort(feature_vector[parent_key]['feature_weights'])[::-1]\n",
    "\n",
    "        child_names   = np.array(feature_vector[parent_key]['feature_childs'])\n",
    "        child_weights = np.array(feature_vector[parent_key]['feature_weights'])\n",
    "\n",
    "        sorted_child_names   = child_names[sorted_child_indexes]\n",
    "        sorted_child_weights = child_weights[sorted_child_indexes]\n",
    "\n",
    "        position = 0\n",
    "        for sorted_child_index in sorted_child_indexes:\n",
    "            normalized_position = math.ceil(position/2)\n",
    "\n",
    "            if (position % 2 != 0): # Impar\n",
    "                current_column = half_column - normalized_position\n",
    "            else: # Par\n",
    "                current_column = half_column + normalized_position\n",
    "\n",
    "            matrix_indexes[child_names[sorted_child_index]] = [current_row, current_column]\n",
    "            position = position + 1 \n",
    "\n",
    "        index = index + 1\n",
    "\n",
    "    return matrix_indexes\n",
    "    \n",
    "def fv2gi(feature_vector):\n",
    "\n",
    "    max_dimension = 0\n",
    "    for key in feature_vector:\n",
    "        childs_number = len(feature_vector[key]['feature_childs'])\n",
    "        max_dimension = max(childs_number, max_dimension)\n",
    "                \n",
    "    matrix = np.zeros((max_dimension, max_dimension))\n",
    "\n",
    "    weights_vector = []\n",
    "    for parent_key in feature_vector:\n",
    "        wpi = sum([float(child_weight) for child_weight in feature_vector[parent_key]['feature_weights']])\n",
    "        feature_vector[parent_key]['wpi'] = wpi\n",
    "        weights_vector.append(wpi)\n",
    "\n",
    "   \n",
    "    sorted_feature_vector = sorted(feature_vector.items(),\n",
    "                                   key = lambda item: item[1]['wpi'],\n",
    "                                   reverse = True)\n",
    "     \n",
    "    sorted_feature_vector = dict(sorted_feature_vector)\n",
    "\n",
    "    \n",
    "    matrix_indexes = get_feature_matrix_indexes(sorted_feature_vector, matrix)\n",
    "\n",
    "    return matrix_indexes\n",
    "\n",
    "# matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb0004-1f9b-493a-9c83-91d943b5309d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "077b732a-ca4a-440e-8ae2-dbb3d15ba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feature_vector(X_dataset,child_weights):\n",
    "  # Obtenemos el set de columnas del dataset\n",
    "  train_columns_set  = set(X_dataset.columns)\n",
    "\n",
    "  for parent_feature in feature_vector.keys():\n",
    "    # Obtiene el set de características hijas del padre actual\n",
    "    # dict.fromleys para mantener el orden, un set desordena los valores\n",
    "    feature_childs_set = dict.fromkeys(feature_vector[parent_feature]['feature_childs'])\n",
    "\n",
    "    # Obtener el índice de las columnas del actual padre para acceder a los pesos del XGBoost\n",
    "    index_feature_childs = X_dataset.columns.get_indexer(feature_childs_set)\n",
    "\n",
    "    feature_vector[parent_feature]['feature_weights'] = list([str(child_weight) for child_weight in child_weights[index_feature_childs]])\n",
    "\n",
    "  return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c95ea-ae18-4406-b116-5ce6674b7f6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017e1821-71be-48d9-85cb-07808165a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_data(X_data):\n",
    "\n",
    "    # Create a sample df\n",
    "    normalized_df = X_data\n",
    "\n",
    "    # Calculate the zscores and drop zscores into new column\n",
    "    for column in normalized_df.columns:\n",
    "        normalized_df[column] = zscore(normalized_df[column])\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358c9a0-6423-4aa8-92a3-e01a8ac72bb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Oversampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efac95c3-8dc1-4acf-b4e0-78d24fa64131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "def oversample_data(X_data, Y_labels):\n",
    "\n",
    "    oversampler = BorderlineSMOTE(kind='borderline-2')\n",
    "    # oversampler = RandomOverSampler()\n",
    "    # oversampler = KMeansSMOTE()\n",
    "    X_oversampled, Y_oversampled = oversampler.fit_resample(X_data, Y_labels)\n",
    "\n",
    "    print('********** After OverSampling **********')\n",
    "    print('Slight: ', (Y_oversampled == 'Slight').sum())\n",
    "    print('Serious:', (Y_oversampled == 'Serious').sum())\n",
    "    print('Fatal:  ', (Y_oversampled == 'Fatal').sum())\n",
    "    print('\\n Total X: ', len(X_oversampled), ' Total Y: ', len(Y_oversampled), '\\n')\n",
    "\n",
    "    return X_oversampled, Y_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e306be-543d-4ba7-b9a4-2cb1cb938f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b61998b-bd46-4dad-a0e8-8363f92f7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gray_images(dataset, max_dimension, matrix_indexes):\n",
    "\n",
    "    matrix_3d = np.zeros((max_dimension, max_dimension, len(dataset.index)))\n",
    "    print(len(dataset.index))\n",
    "    for feature, value in matrix_indexes.items():\n",
    "        matrix_3d[value[0], value[1],] = dataset[feature]\n",
    "        \n",
    "    return matrix_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc000ada-10b7-43b8-ab9f-789d9d35be8c",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Algoritmo genético"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcfd52-f850-40a9-a44f-183653456b65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inicializar población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f539cc1-22ad-467b-8e0b-0c892aff1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_individual(hyperparams_to_optimize):\n",
    "\n",
    "    individual = []\n",
    "\n",
    "    for key in hyperparams_to_optimize:\n",
    "        min_value = hyperparams_to_optimize[key]['init'][0]\n",
    "        max_value = hyperparams_to_optimize[key]['init'][1]\n",
    "        data_type = hyperparams_to_optimize[key]['type']\n",
    "        \n",
    "        if data_type == 'int':\n",
    "            step = hyperparams_to_optimize[key]['step']\n",
    "            hyperparam = int(random.randrange(min_value, max_value))\n",
    "\n",
    "        if data_type == 'float':\n",
    "            round_to = hyperparams_to_optimize[key]['round']\n",
    "            hyperparam = round(random.uniform(min_value, max_value), round_to)\n",
    "\n",
    "        individual.append(hyperparam)\n",
    "    \n",
    "    return individual\n",
    "\n",
    "def initilialize_population(number_of_individuals, hyperparams_to_optimize):\n",
    "    population = []\n",
    "\n",
    "    for i in range(number_of_individuals):\n",
    "\n",
    "        population.append(generate_individual(hyperparams_to_optimize))\n",
    "      \n",
    "    return np.array(population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80a92b-17eb-4a29-b12e-3852265dde12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fitness function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5133e38-307e-4138-a664-fd4e28dc89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def fitness_f1score(y_true, y_pred):\n",
    "\n",
    "    fitness = round((f1_score(y_true, y_pred, average='micro')), 4)\n",
    "\n",
    "    return fitness # Train the data annd find fitness score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9222dcc-088c-468e-bbc7-a0dec6541004",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Evaluación de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f69b5d-1aba-4bad-a705-adcc672e68d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/envs/TFM/lib/python3.9/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "def train_population(population, hyperparams_to_optimize, dMatrixTrain, dMatrixTest, Y_test):\n",
    "    fScore = []\n",
    "    \n",
    "    params = {'objective':'multi:softprob',\n",
    "               'tree_method': 'gpu_hist',\n",
    "               'single_precision_histogram': True,\n",
    "               'num_class': 3\n",
    "             }\n",
    "\n",
    "    for individual_index in range(population.shape[0]):\n",
    "        # Se almacenan en hyperparams_to_optimize los valores del individuo con su nombre correspondiente de hyperparams_name_to_optimize.\n",
    "        hyperparams = {}\n",
    "\n",
    "        for index, hyperparam_value in enumerate(population[individual_index]):\n",
    "\n",
    "            hyperparam_name_to_optimize = list(hyperparams_to_optimize.keys())[index]\n",
    "            data_type = hyperparams_to_optimize[hyperparam_name_to_optimize]['type']\n",
    "\n",
    "            hyperparams[hyperparam_name_to_optimize] = hyperparam_value\n",
    "            hyperparams[hyperparam_name_to_optimize] = hyperparams[hyperparam_name_to_optimize].astype(data_type)\n",
    "        \n",
    "        params.update(hyperparams)\n",
    "\n",
    "        num_round = params['n_estimators']\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        xgb.set_config(verbosity=0)\n",
    "        bst = xgb.train(params,\n",
    "                        dMatrixTrain,\n",
    "                        num_round)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        preds = bst.predict(dMatrixTest)\n",
    "        \n",
    "        single_predictions = [np.argmax(pred) for pred in preds]\n",
    "        # preds = preds > 0.5\n",
    "\n",
    "        fitness_score = fitness_f1score(Y_test, single_predictions)\n",
    "\n",
    "        # print(f\"{individual_index}: {hyperparams} --> time(s): {round(end - start, 2)} --> score: {fitness_score}\")\n",
    "\n",
    "        fScore.append(fitness_score)\n",
    "\n",
    "    return fScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b66fd8-ae70-4f20-84f6-0113ae309a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Selección de padres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "771d8007-f28e-424c-8ff6-f174822b02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parents for mating\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) # Create an array to store fittest parents.\n",
    "\n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1 # Set this value to negative, in case of F1-score, so this parent is not selected again\n",
    "\n",
    "    return selectedParents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9b882-ad82-4446-a301-a856dedbd660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cruzamiento de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db837b53-3d02-446c-aa9c-6c772da6be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mate these parents to create children having parameters from these parents (we are using uniform crossover method)\n",
    "'''\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex  = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) # get all the index\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) # select half  of the indexes randomly\n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes\n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    '''\n",
    "    Create child by choosing parameters from two parents selected using new_parent_selection function. The parameter values\n",
    "    will be picked from the indexes, which were randomly selected above. \n",
    "    '''\n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        #find parent 1 index \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        #find parent 2 index\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "\n",
    "    return children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d549ad-a75d-4943-a89a-ea9e783054a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Mutación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77fcbc49-d8aa-4288-ada2-19366dc78f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mutation(crossover, numberOfParameters):\n",
    "#     # Define minimum and maximum values allowed for each parameterminMaxValue = np.zeros((numberOfParameters, 2))\n",
    "#     minMaxValue = np.zeros((numberOfParameters, 2))\n",
    "\n",
    "#     minMaxValue[0:]  = [0.01, 1.0]  # min/max learning rate\n",
    "#     minMaxValue[1,:] = [10, 2000]   # min/max n_estimator\n",
    "#     minMaxValue[2,:] = [1, 20]      # min/max depth\n",
    "#     minMaxValue[3,:] = [0, 10.0]    # min/max child_weight\n",
    "#     minMaxValue[4,:] = [0.01, 10.0] # min/max gamma\n",
    "#     minMaxValue[5,:] = [0.01, 1.0]  # min/max subsample\n",
    "#     minMaxValue[6,:] = [0.01, 1.0]  # min/max colsample_bytree\n",
    "#     minMaxValue[7,:] = [40.0,180.0] # min/max reg_alpha\n",
    "#     minMaxValue[8,:] = [0.0, 1.0]   # min/max reg_lambda\n",
    " \n",
    "#     # Mutation changes a single gene in each offspring randomly.\n",
    "#     mutationValue = 0\n",
    "#     parameterSelect = np.random.randint(0, numberOfParameters, 1)\n",
    "\n",
    "#     print(parameterSelect)\n",
    "# #             if parameterSelect == 0: # learning_rate\n",
    "# #                 # mutationValue = round(np.random.uniform(-0.2, 0.2), 2)\n",
    "# #                 mutationValue = round(random.uniform(-0.3, 0.3), 2)\n",
    "# #             if parameterSelect == 1: # max_depth\n",
    "# #                 # mutationValue = np.random.randint(-3, 3, 1)\n",
    "# #                 mutationValue = int(random.randrange(-4, 4, step= 1))\n",
    "# #             if parameterSelect == 2: # min_child_weight\n",
    "# #                 # mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "# #                 mutationValue = round(random.uniform(-5, 5), 1)\n",
    "#     if parameterSelect == 0: # learning_rate\n",
    "#         # mutationValue = round(np.random.uniform(-0.2, 0.2), 2)\n",
    "#         mutationValue = round(random.uniform(-0.2, 0.2), 2)\n",
    "#     if parameterSelect == 1: # n_estimators\n",
    "#         mutationValue = np.random.randint(-150, 150, 1)\n",
    "#         # mutationValue = random.randrange(100, 2000, step = 150)\n",
    "#     if parameterSelect == 2: # max_depth\n",
    "#         mutationValue = np.random.randint(-4, 4, 1)\n",
    "#         # mutationValue = int(random.randrange(1, 20, step= 1))\n",
    "#     if parameterSelect == 3: # min_child_weight\n",
    "#         # mutationValue = round(np.random.uniform(5, 5), 2)\n",
    "#         mutationValue = round(random.uniform(-5, 5), 1)\n",
    "#     if parameterSelect == 4: #gamma\n",
    "#         # mutationValue = round(np.random.uniform(-2, 2), 2)\n",
    "#         mutationValue = round(random.uniform(0.01, 10.0), 2)\n",
    "#     if parameterSelect == 5: # subsample\n",
    "#         # mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "#         mutationValue = round(random.uniform(0.01, 1.0), 2)\n",
    "#     if parameterSelect == 6: # colsample\n",
    "#         # mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n",
    "#         mutationValue = round(random.uniform(0.5, 1.0), 2)\n",
    "#     if parameterSelect == 7: # reg_alpha\n",
    "#         # mutationValue = round(np.random.uniform(-20,20), 1)\n",
    "#         mutationValue = round(random.uniform(40,180), 1)\n",
    "#     if parameterSelect == 8: # reg_lambda\n",
    "#         # mutationValue = round(np.random.uniform(-0.2,0.2), 3)\n",
    "#         mutationValue = round(random.uniform(0,1), 3)\n",
    "  \n",
    "#     # Introduce mutation by changing one parameter, and set to max or min if it goes out of range\n",
    "#     for idx in range(crossover.shape[0]):\n",
    "#         crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n",
    "\n",
    "#         if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n",
    "#             crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n",
    "\n",
    "#         if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n",
    "#             crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]\n",
    "\n",
    "#     return crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb741b25-56ed-4e66-adbd-42d3b5607974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(crossover, hyperparams_to_optimize):\n",
    "    \n",
    "    # MUTATION_PROBABILITY = 1/numberOfParameters\n",
    "    \n",
    "    MUTATION_PROBABILITY = 0.2\n",
    "    number_of_parameters = len(hyperparams_to_optimize)\n",
    "\n",
    "    for idx in range(crossover.shape[0]):\n",
    "\n",
    "        mutation_probability = np.random.rand(1)\n",
    "\n",
    "        while MUTATION_PROBABILITY > mutation_probability:\n",
    "\n",
    "            mutationValue = 0\n",
    "\n",
    "            hyperparam_selected_index = np.random.randint(0, number_of_parameters)\n",
    "            hyperparam_selected_name  = list(hyperparams_to_optimize.keys())[hyperparam_selected_index]\n",
    "\n",
    "            min_limit_value = hyperparams_to_optimize[hyperparam_selected_name]['init'][0]\n",
    "            max_limit_value = hyperparams_to_optimize[hyperparam_selected_name]['init'][1]\n",
    "\n",
    "            min_mutation_value = hyperparams_to_optimize[hyperparam_selected_name]['mutation'][0]\n",
    "            max_mutation_value = hyperparams_to_optimize[hyperparam_selected_name]['mutation'][1]\n",
    "\n",
    "            data_type = hyperparams_to_optimize[hyperparam_selected_name]['type']\n",
    "            \n",
    "            if data_type == 'int':\n",
    "                step = hyperparams_to_optimize[hyperparam_selected_name]['step']\n",
    "                mutationValue = int(random.randrange(min_mutation_value, max_mutation_value, step = step))\n",
    "\n",
    "            if data_type == 'float':\n",
    "                round_to = hyperparams_to_optimize[hyperparam_selected_name]['round']\n",
    "                mutationValue = round(random.uniform(min_mutation_value, max_mutation_value), round_to)\n",
    "                \n",
    "            print(idx, hyperparam_selected_name, mutationValue)\n",
    "\n",
    "            crossover[idx, hyperparam_selected_index] = crossover[idx,hyperparam_selected_index] + mutationValue\n",
    "\n",
    "            if(crossover[idx, hyperparam_selected_index] > max_limit_value):\n",
    "                crossover[idx, hyperparam_selected_index] = max_limit_value\n",
    "\n",
    "            if(crossover[idx, hyperparam_selected_index] < min_limit_value):\n",
    "                crossover[idx, hyperparam_selected_index] = min_limit_value\n",
    "                \n",
    "            mutation_probability = np.random.rand(1)\n",
    "\n",
    "\n",
    "    return crossover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d6d9f-8ae4-4740-9983-e51d39cdeac6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ded491b-6d05-44f5-9ee9-44064babfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one channel\n",
    "# https://machinelearningmastery.com/a-gentle-introduction-to-channels-first-and-channels-last-image-formats-for-deep-learning/\n",
    "\n",
    "# Add one channel to gray images depending of the number of the data\n",
    "def shape_images(X_data, gray_images):\n",
    "  images = []\n",
    "\n",
    "  for i in range(0,len(X_data)):\n",
    "      original_matrix = gray_images[:,:,i]\n",
    "      # print(original_matrix.shape)\n",
    "      shaped_image = np.expand_dims(original_matrix, axis=2)\n",
    "      # print(shaped_image.shape)\n",
    "      images.append(shaped_image)\n",
    "      # plt.matshow(shaped_image)\n",
    "\n",
    "  return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71fa80-2c77-46a3-819c-bb7a0ff9a5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## One-Hot Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81a2db8a-49a5-4a06-bf66-025a0d472001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casualty_to_one_hot(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        'Slight': 0,\n",
    "        'Serious': 1,\n",
    "        'Fatal': 2\n",
    "    }\n",
    "\n",
    "    Y_labels.replace(transf, inplace = True)\n",
    "\n",
    "    return tf.one_hot(Y_labels, 3)\n",
    "\n",
    "def one_hot_to_casualty(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        0: 'Slight',\n",
    "        1: 'Serious',\n",
    "        2: 'Fatal'\n",
    "    }   \n",
    "\n",
    "    return Y_labels.replace(transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021df180-f056-4e21-ac6e-39d41369831b",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a4a49-64d8-4a32-abf7-e626f39d0938",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29dfa69d-7135-476f-9c1d-4ebf5d30bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def correlation_matrix(X_data):\n",
    "    corrMatrix = X_data.corr()\n",
    "    fig,ax  = plt.subplots(1,1,figsize=(20,15))\n",
    "    sns.heatmap(corrMatrix, annot=True)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "    plt.savefig('saving-a-seaborn-plot-as-eps-file.svg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaa5de-34f0-4724-ba36-0fdcf19c64e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d011694-26fe-41e3-9a39-85722f128f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(X_train_data, X_test_data):\n",
    "    pca = PCA()\n",
    "    X_train_pca = pca.fit_transform(X_train_data)\n",
    "    X_test_pca  = pca.transform(X_test_data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    figure_name = plt.figure(figsize=(20, 15))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    plt.savefig('saving-a-seaborn-plot-as-eps-file.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401456b5-b7b9-421c-83db-709796ee1d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74c55dca-09c1-4831-a64c-bc04a9cdf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_TSNE(X_data, Y_data, n_components, output_file_name = None):\n",
    "\n",
    "    X_data_scaled = StandardScaler().fit_transform(X_data)\n",
    "    z_data = TSNE(n_components = n_components).fit_transform(X_data_scaled)\n",
    "\n",
    "    # X_test_scaled = StandardScaler().fit_transform(X_test),\n",
    "    # z_test = TSNE(n_components=2).fit_transform(X_test_scaled),\n",
    "\n",
    "    palette = sns.color_palette('husl', 3)\n",
    "    fig,ax  = plt.subplots(1, 1, figsize=(15,10))\n",
    "    sns.scatterplot(x = z_data[:,0],\n",
    "                    y = z_data[:,1],\n",
    "                    hue = Y_data,\n",
    "                    palette = palette,\n",
    "                    legend = 'full')\n",
    "\n",
    "    if (output_file_name): plt.savefig('./Out/' + output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b4c26-5100-4723-9342-b7b9aa187d79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f4072a7-e2e5-4e45-9456-4bea3270a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder ():\n",
    "    input_img = Input(shape=(25,))\n",
    "\n",
    "    # definimos el encoder, que tendra una entrada de Input_img y una segunda capa con entrada de encoder1 y salida 3\n",
    "    encoder1 = layers.Dense(15, activation='sigmoid')(input_img)\n",
    "    encoder2 = layers.Dense(3, activation='sigmoid')(encoder1)\n",
    "\n",
    "    # definimos el  decoder que tendra una entrada inicial de encoder3 y una salida de 128 y finalmete una capa de salida con los mismos que Input_img\n",
    "    decoder1 = layers.Dense(15, activation='sigmoid')(encoder2)\n",
    "    decoder2 = layers.Dense(25, activation='sigmoid')(decoder1)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = tf.keras.Model(inputs=input_img, outputs=decoder2)\n",
    "    autoencoder.summary()\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy') #se usan estos dos en estas arquitecturas\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc6df4-982b-49e8-8649-8da2c48a5af1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1D-Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22b2924f-95ff-40c7-9f3f-f25569fc4267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-14 21:50:20.239738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.240029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.240219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.240677: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.240882: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.241068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.241317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.241511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-14 21:50:20.241652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3368 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "lr_init = 0.1\n",
    "num_classes = 3\n",
    "\n",
    "convolution_1d = models.Sequential()\n",
    "convolution_1d.add(layers.Conv1D(256, 3,strides = 1, activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "convolution_1d.add(layers.BatchNormalization())\n",
    "convolution_1d.add(layers.Conv1D(256, 3, strides = 1, activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "convolution_1d.add(layers.BatchNormalization())\n",
    "convolution_1d.add(layers.Conv1D(256, 3, strides = 1, activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "convolution_1d.add(layers.BatchNormalization())\n",
    "convolution_1d.add(layers.Conv1D(256, 3, strides = 1, activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "convolution_1d.add(layers.BatchNormalization())  \n",
    "convolution_1d.add(layers.Flatten())\n",
    "convolution_1d.add(layers.Dense(units=128))\n",
    "convolution_1d.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "convolution_1d.compile(\n",
    "    optimizer=Adam(learning_rate = lr_init, epsilon=1e-06),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ea062-c0b7-4486-9ebe-94ae66a58b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TASP-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05497b70-0400-4219-8a07-3a43005066cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_init = 0.1\n",
    "num_classes = 3\n",
    "\n",
    "tasp_cnn = models.Sequential()\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "tasp_cnn.add(layers.BatchNormalization())\n",
    "tasp_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "tasp_cnn.add(layers.BatchNormalization())\n",
    "tasp_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "tasp_cnn.add(layers.BatchNormalization())\n",
    "tasp_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "tasp_cnn.add(layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 256)))\n",
    "tasp_cnn.add(layers.BatchNormalization())  \n",
    "tasp_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "tasp_cnn.add(layers.Flatten())\n",
    "tasp_cnn.add(layers.Dense(units=128))\n",
    "tasp_cnn.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "tasp_cnn.compile(\n",
    "    optimizer=Adam(learning_rate = lr_init, epsilon=1e-06),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b99ef66-1c52-47f9-851f-2cab1406b114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 5, 5, 256)         2560      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 5, 5, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 5, 5, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 5, 5, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 5, 5, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               819328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,596,611\n",
      "Trainable params: 2,594,563\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tasp_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41d457bb-2db3-4b69-b0b7-bcf867dbeb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea9112-c2a3-474d-87a5-ba64c599ab56",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5be32-4a2d-457d-bd39-5ab2da01a809",
   "metadata": {},
   "source": [
    "### F1-Score History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6daa83e3-3b42-4e84-ae09-087c65298e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_score_history(f1_score_path, f1_score_name, history):\n",
    "    figure_name = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    plt.plot(history.history['f1_score'], label='F1 score (training data)')\n",
    "    plt.plot(history.history['val_f1_score'], label='F1 score (validation data)')\n",
    "    plt.title('F1 score')\n",
    "    plt.ylabel('F1 score value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(f1_score_path + f1_score_name)\n",
    "    plt.show()\n",
    "    \n",
    "    print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529e0fc-6e5c-46aa-8029-d053d4df3d88",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14bcc399-bc1c-490d-b537-3fd46d4e49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def plot_classification_report(path, file_name, y_true, y_predicted):\n",
    "\n",
    "    labels = one_hot_to_casualty(y_true)\n",
    "\n",
    "    report = classification_report(y_true,\n",
    "                                   y_predicted,\n",
    "                                   target_names = labels.unique(),\n",
    "                                   output_dict  = True)\n",
    "\n",
    "\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(path + file_name, index = True)\n",
    "\n",
    "    print(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9703d-abc2-42b0-8dff-31a2a715815e",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d46827f-b218-49c7-9902-4337c1457d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(path, file_name, y_true, y_predicted):\n",
    "\n",
    "    cm = confusion_matrix(y_true,\n",
    "                          y_predicted,\n",
    "                          labels = y_true.unique())\n",
    "\n",
    "    labels = one_hot_to_casualty(y_true)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm,\n",
    "                                  display_labels = labels.unique()).plot()\n",
    "\n",
    "    plt.savefig(path + file_name, dpi = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pcQtLPSUVwgf",
   "metadata": {
    "id": "pcQtLPSUVwgf",
    "tags": []
   },
   "source": [
    "# Leeds Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4tAAMniVQ-O",
   "metadata": {
    "id": "e4tAAMniVQ-O",
    "tags": []
   },
   "source": [
    "## Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d027c46-8e31-4698-86f1-38547595abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pandas --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6951ab4",
   "metadata": {
    "id": "e6951ab4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# https://datamillnorth.org/dataset/road-traffic-accidents\n",
    "# root_path = '/content/drive/Othercomputers/Mi portátil/Drive/Master UA/TFM/TFM_DATA/'\n",
    "\n",
    "city_name = 'leeds'\n",
    "\n",
    "root_path = './Data/Leeds/'\n",
    "\n",
    "file_path_2009 = './2009.csv'\n",
    "file_path_2010 = './2010.csv'\n",
    "file_path_2011 = './2011.csv'\n",
    "file_path_2012 = './2012.csv'\n",
    "file_path_2013 = './2013.csv'\n",
    "file_path_2014 = './2014.csv'\n",
    "file_path_2015 = './2015.csv'\n",
    "file_path_2016 = './2016.csv'\n",
    "\n",
    "file_2009 = pd.read_csv(root_path + file_path_2009, encoding = 'cp1252')\n",
    "file_2010 = pd.read_csv(root_path + file_path_2010, encoding = 'cp1252')\n",
    "file_2011 = pd.read_csv(root_path + file_path_2011, encoding = 'cp1252')\n",
    "file_2012 = pd.read_csv(root_path + file_path_2012, encoding = 'cp1252')\n",
    "file_2013 = pd.read_csv(root_path + file_path_2013, encoding = 'cp1252')\n",
    "file_2014 = pd.read_csv(root_path + file_path_2014, encoding = 'cp1252')\n",
    "file_2015 = pd.read_csv(root_path + file_path_2015, encoding = 'cp1252')\n",
    "file_2016 = pd.read_csv(root_path + file_path_2016, encoding = 'cp1252')\n",
    "\n",
    "###################### UNIÓN DE ARCHIVOS ######################\n",
    "\n",
    "a = pd.concat([file_2009,file_2010])\n",
    "\n",
    "file_2013 = clean_df = file_2013.loc[:, ~file_2013.columns.isin(['Casualty Class'])]\n",
    "file_2013.set_axis(a.columns, axis=1, inplace=True)\n",
    "                                             \n",
    "file_2014 = clean_df = file_2014.loc[:, ~file_2014.columns.isin(['Casualty Class'])]\n",
    "file_2014.set_axis(a.columns, axis=1, inplace=True)\n",
    "\n",
    "# file_2015 = clean_df = file_2015.loc[:, ~file_2015.columns.isin(['Casualty Class'])]\n",
    "file_2015.set_axis(a.columns, axis=1, inplace=True)\n",
    "file_2016 = clean_df = file_2016.loc[:, ~file_2016.columns.isin(['Expr1'])]\n",
    "file_2016.set_axis(a.columns, axis=1, inplace=True)\n",
    "\n",
    "a = pd.concat([a, file_2011])\n",
    "a = pd.concat([a, file_2012])\n",
    "a = pd.concat([a, file_2013])\n",
    "a = pd.concat([a, file_2014])\n",
    "a = pd.concat([a, file_2015])\n",
    "a = pd.concat([a, file_2016])\n",
    "\n",
    "# a['1st Road Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Y9SjctrVXCD",
   "metadata": {
    "id": "3Y9SjctrVXCD",
    "tags": []
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b726f75b",
   "metadata": {
    "id": "b726f75b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41373/4205016787.py:148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['Weather Conditions'] = clean_df['Weather Conditions'].astype('int')\n",
      "/tmp/ipykernel_41373/4205016787.py:149: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['Casualty Class']     = clean_df['Casualty Class'].astype('int')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Easting</th>\n",
       "      <th>Northing</th>\n",
       "      <th>Number of Vehicles</th>\n",
       "      <th>Accident Time</th>\n",
       "      <th>1st Road Class</th>\n",
       "      <th>Road Surface</th>\n",
       "      <th>Lighting Conditions</th>\n",
       "      <th>Weather Conditions</th>\n",
       "      <th>Casualty Class</th>\n",
       "      <th>Casualty Severity</th>\n",
       "      <th>Sex of Casualty</th>\n",
       "      <th>Age of Casualty</th>\n",
       "      <th>Type of Vehicle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>429093</td>\n",
       "      <td>436258</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>434723</td>\n",
       "      <td>435534</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Serious</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>441173</td>\n",
       "      <td>433047</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428487</td>\n",
       "      <td>431364</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>425928</td>\n",
       "      <td>435480</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20302</th>\n",
       "      <td>423815</td>\n",
       "      <td>434248</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20303</th>\n",
       "      <td>427102</td>\n",
       "      <td>427700</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Slight</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20304</th>\n",
       "      <td>419983</td>\n",
       "      <td>440944</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Serious</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20305</th>\n",
       "      <td>419983</td>\n",
       "      <td>440944</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20306</th>\n",
       "      <td>427222</td>\n",
       "      <td>433739</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Serious</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20307 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Easting  Northing  Number of Vehicles  Accident Time  1st Road Class  \\\n",
       "0       429093    436258                   1              2               6   \n",
       "1       434723    435534                   1              2               6   \n",
       "2       441173    433047                   1              1               6   \n",
       "3       428487    431364                   1              1               3   \n",
       "4       425928    435480                   2              1               6   \n",
       "...        ...       ...                 ...            ...             ...   \n",
       "20302   423815    434248                   2              2               6   \n",
       "20303   427102    427700                   2              2               6   \n",
       "20304   419983    440944                   2              1               3   \n",
       "20305   419983    440944                   2              1               3   \n",
       "20306   427222    433739                   1              1               6   \n",
       "\n",
       "       Road Surface  Lighting Conditions  Weather Conditions  Casualty Class  \\\n",
       "0                 1                    3                   1               3   \n",
       "1                 1                    3                   1               1   \n",
       "2                 1                    3                   1               3   \n",
       "3                 1                    3                   1               3   \n",
       "4                 1                    1                   1               1   \n",
       "...             ...                  ...                 ...             ...   \n",
       "20302             2                    3                   1               1   \n",
       "20303             2                    3                   1               1   \n",
       "20304             1                    3                   1               1   \n",
       "20305             1                    3                   1               3   \n",
       "20306             1                    3                   1               3   \n",
       "\n",
       "      Casualty Severity  Sex of Casualty  Age of Casualty  Type of Vehicle  \n",
       "0                Slight                1                3                7  \n",
       "1               Serious                2                2                7  \n",
       "2                Slight                2                1                7  \n",
       "3                Slight                1                1                7  \n",
       "4                Slight                2                3                7  \n",
       "...                 ...              ...              ...              ...  \n",
       "20302            Slight                1                3                3  \n",
       "20303            Slight                2                3                7  \n",
       "20304           Serious                2                3                7  \n",
       "20305            Slight                1                3                7  \n",
       "20306           Serious                1                3               17  \n",
       "\n",
       "[20307 rows x 13 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### DICCIONARIOS DE REEMPLAZO ######################\n",
    "# Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# A, A(M) y Motorway lo mismo?\n",
    "# B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "#               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "#               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "#               ¿Carretera como tal?\n",
    "\n",
    "# C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "#               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "road_class_replace = {\n",
    "    'Motorway': 1,\n",
    "    'A(M)': 2,\n",
    "    'A': 3,\n",
    "    'B': 4,\n",
    "    'C': 5,\n",
    "    'Unclassified': 6\n",
    "}\n",
    "\n",
    "##################################\n",
    "accident_date_replace = {\n",
    "    'Dry': 1,\n",
    "    'Wet / Damp': 2,\n",
    "    'Snow': 3,\n",
    "    'Frost / Ice': 4,\n",
    "    'Flood': 5,\n",
    "}\n",
    "##################################\n",
    "\n",
    "road_surface_replace = {\n",
    "    'Dry': 1,\n",
    "    'Wet / Damp': 2,\n",
    "    'Snow': 3,\n",
    "    'Frost/ Ice': 4,\n",
    "    'Frost / Ice': 4,\n",
    "    'Flood': 5,\n",
    "    'Flood (surface water over 3cm deep)': 5,\n",
    "    '5': 5\n",
    "}\n",
    "\n",
    "# La 5: \"Darkness: street lighting unknown\" no está presente en el paper, le hemos puesto un 5 porque sí #\n",
    "lighting_conditions_replace = {\n",
    "    'Daylight: street lights present': 1,\n",
    "    'Darkness: no street lighting': 2,\n",
    "    'Darkness: street lights present and lit': 3,\n",
    "    'Darkness: street lights present but unlit': 4,\n",
    "    'Darkness: street lighting unknown': 5,\n",
    "    '5': 5\n",
    "}\n",
    "\n",
    "# La 8.2: \"Unknown\" no está presente en el paper, le hemos puesto un 8 porque sí (Other) #\n",
    "weather_conditions_replace = {\n",
    "    'Fine without high winds': 1,\n",
    "    'Raining without high winds': 2,\n",
    "    'Snowing without high winds': 3,\n",
    "    'Fine with high winds': 4,\n",
    "    'Raining with high winds': 5,\n",
    "    'Snowing with high winds': 6,\n",
    "    'Fog or mist – if hazard': 7,\n",
    "    'Other': 8,\n",
    "    'Unknown': 8\n",
    "}\n",
    "\n",
    "type_of_vehicle_replace = {\n",
    "    'Pedal cycle': 1,\n",
    "    'M/cycle 50cc and under': 2,\n",
    "    'Motorcycle over 50cc and up to 125cc': 3,\n",
    "    'Motorcycle over 125cc and up to 500cc': 4,\n",
    "    'Motorcycle over 500cc': 5,\n",
    "    'Taxi/Private hire car': 6,\n",
    "    'Car': 7,\n",
    "    'Minibus (8 – 16 passenger seats)': 8,\n",
    "    'Bus or coach (17 or more passenger seats)': 9,\n",
    "    'Ridden horse': 10,\n",
    "    'Agricultural vehicle (includes diggers etc.)': 11,\n",
    "    'Tram / Light rail': 12,\n",
    "    'Goods vehicle 3.5 tonnes mgw and under': 13,\n",
    "    'Goods vehicle over 3.5 tonnes and under 7.5 tonnes mgw': 14,\n",
    "    'Goods vehicle 7.5 tonnes mgw and over': 15,\n",
    "    'Mobility Scooter': 16,\n",
    "    'Other Vehicle ': 17,\n",
    "    'Motorcycle - Unknown CC': 18\n",
    "}\n",
    "\n",
    "casualty_class_replace = {\n",
    "    'Driver': 1,\n",
    "    'Driver/Rider': 1,\n",
    "    'Driver or rider': 1,\n",
    "    'Passenger': 2,\n",
    "    'Vehicle or pillion passenger': 2,\n",
    "    'Pedestrian': 3\n",
    "}\n",
    "\n",
    "\n",
    "sex_of_casualty_replace = {\n",
    "    'Male': 1,\n",
    "    'Female': 2\n",
    "}\n",
    "\n",
    "###################### REEMPLAZOS ######################\n",
    "clean_df = clean_df.dropna()\n",
    "\n",
    "a['1st Road Class'].replace(road_class_replace, inplace = True)\n",
    "# print('1st Road Class:', a['1st Road Class'].unique())\n",
    "\n",
    "##################################\n",
    "# a['Accident Date'].replace(accident_date_replace, inplace = True)\n",
    "# print('Accident Date:', a['Accident Date'].unique())\n",
    "##################################\n",
    "a['Road Surface'].replace(road_surface_replace, inplace = True)\n",
    "a.dropna(inplace = True)\n",
    "\n",
    "a['Road Surface'] = a['Road Surface'].astype('int')\n",
    "# print('Road Surface:', a['Road Surface'].unique())\n",
    "\n",
    "a['Lighting Conditions'].replace(lighting_conditions_replace, inplace = True)\n",
    "# print('Lighting Conditions:', a['Lighting Conditions'].unique())\n",
    "\n",
    "a['Weather Conditions'].replace(weather_conditions_replace, inplace = True)\n",
    "a = a[a['Weather Conditions'] != 'Darkness: street lighting unknown']\n",
    "# print('Weather Conditions:', a['Weather Conditions'].unique())\n",
    "\n",
    "a['Type of Vehicle'].replace(type_of_vehicle_replace, inplace = True)\n",
    "# print('Type of Vehicle:', a['Type of Vehicle'].unique())\n",
    "\n",
    "a['Casualty Class'].replace(casualty_class_replace, inplace = True)\n",
    "# print('Casualty Class:', a['Casualty Class'].unique())\n",
    "\n",
    "a['Sex of Casualty'].replace(sex_of_casualty_replace, inplace = True)\n",
    "# print('Sex of Casualty:', a['Sex of Casualty'].unique())\n",
    "\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'] < 18, 1)\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'].between(18, 25), 2)\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'].between(25, 65), 3)\n",
    "a['Age of Casualty'] = a['Age of Casualty'].mask(a['Age of Casualty'] > 65, 4)\n",
    "# print('Age of Casualty:', a['Age of Casualty'].unique())\n",
    "\n",
    "a['Time (24hr)'] = a['Time (24hr)'].mask(a['Time (24hr)'] < 600, 2)\n",
    "a['Time (24hr)'] = a['Time (24hr)'].mask(a['Time (24hr)'] > 1800, 2)\n",
    "a['Time (24hr)'] = a['Time (24hr)'].mask(a['Time (24hr)'].between(600, 1800), 1)\n",
    "# print('Time (24hr):', a['Time (24hr)'].unique())\n",
    "a.rename(columns={\"Time (24hr)\": \"Accident Time\"}, inplace = True)\n",
    "\n",
    "###################### LIMPIEZA DE VALORES NULOS/DUPLICADOS ######################\n",
    "\n",
    "clean_df = a.loc[:, ~a.columns.isin(['Accident Date', 'Reference Number'])]\n",
    "\n",
    "clean_df['Weather Conditions'] = clean_df['Weather Conditions'].astype('int')\n",
    "clean_df['Casualty Class']     = clean_df['Casualty Class'].astype('int')\n",
    "\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "clean_df = clean_df.dropna()\n",
    "clean_df = clean_df.reset_index(drop=True)\n",
    "\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JOSunlOuVeEm",
   "metadata": {
    "id": "JOSunlOuVeEm",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a506548e-6b11-4b46-abfb-bc7672c3f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scikit-learn --y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c921e711",
   "metadata": {
    "id": "c921e711"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = clean_df['Casualty Severity']\n",
    "\n",
    "train, test = train_test_split(clean_df, test_size=0.2)\n",
    "X_train = X_train_original = train.loc[:, ~train.columns.isin(['Casualty Severity'])]\n",
    "Y_train = Y_train_original = train['Casualty Severity']\n",
    "\n",
    "X_test = test.loc[:, ~test.columns.isin(['Casualty Severity'])]\n",
    "Y_test = test['Casualty Severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86be3e-c840-418f-b50f-3cde2af96d8c",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d18643d-ece1-4ac8-985e-f046d5f5ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = clean_df['Casualty Severity']\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "slight_data  = test[test['Casualty Severity'] == 'Slight']\n",
    "serious_data = test[test['Casualty Severity'] == 'Serious']\n",
    "fatal_data   = test[test['Casualty Severity'] == 'Fatal']\n",
    "\n",
    "X_slight_downsampled  = resample(slight_data,\n",
    "                                 replace = True,\n",
    "                                 n_samples = len(fatal_data))\n",
    "\n",
    "X_serious_downsampled = resample(serious_data,\n",
    "                                 replace = True,\n",
    "                                 n_samples = len(fatal_data))\n",
    "\n",
    "downsampled_dataset = pd.concat([X_slight_downsampled, X_serious_downsampled, fatal_data])\n",
    "\n",
    "downsampled_train, downsampled_test = train_test_split(downsampled_dataset, test_size=0.3)\n",
    "\n",
    "\n",
    "X_train_downsampled = downsampled_train.loc[:, ~downsampled_train.columns.isin(['Casualty Severity'])]\n",
    "Y_train_downsampled = downsampled_train['Casualty Severity']\n",
    "\n",
    "X_test_downsampled = downsampled_test.loc[:, ~downsampled_test.columns.isin(['Casualty Severity'])]\n",
    "Y_test_downsampled = downsampled_test['Casualty Severity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "603c5d0e",
   "metadata": {
    "id": "603c5d0e"
   },
   "outputs": [],
   "source": [
    "# fv2gi(feature_vector)\n",
    "# sorted(feature_vector.items(), key = lambda item: item[0][1])\n",
    "\n",
    "# for item in feature_vector['Accident Features'].items():\n",
    "#     print(item[1])\n",
    "\n",
    "# feature_vector[parent_key].items()\n",
    "\n",
    "# sorted(feature_vector['Accident Features'].items(), key = lambda item: item,\n",
    "#                                reverse = True)\n",
    "\n",
    "# print(feature_vector['Accident Features']['feature_weights'])\n",
    "\n",
    "# fv = np.array(feature_vector['Accident Features']['feature_childs'])\n",
    "# list(fv[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gLtQ4-JMW0Tv",
   "metadata": {
    "id": "gLtQ4-JMW0Tv",
    "tags": []
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f97d173-a0ff-4b31-97e1-fd9d787069ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "FHb1AMbZjm9m",
   "metadata": {
    "id": "FHb1AMbZjm9m"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test  = X_test.astype(int)\n",
    "X_train_downsampled = X_train_downsampled.astype(int)\n",
    "X_test_downsampled  = X_test_downsampled.astype(int)\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_test  = normalize_data(X_test)\n",
    "X_train_downsampled = normalize_data(X_train_downsampled)\n",
    "X_test_downsampled  = normalize_data(X_test_downsampled)\n",
    "\n",
    "X_train_original = X_train_original.astype(int)\n",
    "X_train_original = normalize_data(X_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nDVViKK3XCtE",
   "metadata": {
    "id": "nDVViKK3XCtE",
    "tags": []
   },
   "source": [
    "## Oversamplig de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f5125433-f3ee-4cd3-bc5d-dd3a6acfca54",
   "metadata": {
    "id": "f5125433-f3ee-4cd3-bc5d-dd3a6acfca54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Train Before OverSampling **********\n",
      "Slight:  0\n",
      "Serious: 0\n",
      "Fatal:   0\n",
      "\n",
      " Total X: 42737  Total Y: 42737 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFatal:  \u001b[39m\u001b[38;5;124m'\u001b[39m, (Y_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFatal\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Total X:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Total Y:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(Y_train), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m X_train, Y_train \u001b[38;5;241m=\u001b[39m \u001b[43moversample_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m********** Test **********\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlight: \u001b[39m\u001b[38;5;124m'\u001b[39m, (Y_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum())\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36moversample_data\u001b[0;34m(X_data, Y_labels)\u001b[0m\n\u001b[1;32m      7\u001b[0m oversampler \u001b[38;5;241m=\u001b[39m BorderlineSMOTE(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mborderline-2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# oversampler = RandomOverSampler()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# oversampler = KMeansSMOTE()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m X_oversampled, Y_oversampled \u001b[38;5;241m=\u001b[39m \u001b[43moversampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m********** After OverSampling **********\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlight: \u001b[39m\u001b[38;5;124m'\u001b[39m, (Y_oversampled \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSlight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msum())\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/imblearn/base.py:83\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     77\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m     81\u001b[0m )\n\u001b[0;32m---> 83\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     86\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     89\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/imblearn/over_sampling/_smote/filter.py:179\u001b[0m, in \u001b[0;36mBorderlineSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    176\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_m_\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m--> 179\u001b[0m danger_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_in_danger_noise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdanger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    181\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(danger_index):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/imblearn/over_sampling/_smote/base.py:190\u001b[0m, in \u001b[0;36mBaseSMOTE._in_danger_noise\u001b[0;34m(self, nn_estimator, samples, target_class, y, kind)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_in_danger_noise\u001b[39m(\u001b[38;5;28mself\u001b[39m, nn_estimator, samples, target_class, y, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdanger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;124;03m\"\"\"Estimate if a set of sample are in danger or noise.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    Used by BorderlineSMOTE and SVMSMOTE.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        A boolean array where True refer to samples in danger or noise.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    191\u001b[0m     nn_label \u001b[38;5;241m=\u001b[39m (y[x] \u001b[38;5;241m!=\u001b[39m target_class)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    192\u001b[0m     n_maj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(nn_label, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/sklearn/neighbors/_base.py:705\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m         kwds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_\n\u001b[0;32m--> 705\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1633\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1632\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1633\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1634\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/sklearn/neighbors/_base.py:582\u001b[0m, in \u001b[0;36mKNeighborsMixin._kneighbors_reduce_func\u001b[0;34m(self, dist, start, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m\"\"\"Reduce a chunk of distances to the nearest neighbors\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \n\u001b[1;32m    557\u001b[0m \u001b[38;5;124;03mCallback to :func:`sklearn.metrics.pairwise.pairwise_distances_chunked`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    The neighbors indices.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    581\u001b[0m sample_range \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(dist\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 582\u001b[0m neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m neigh_ind \u001b[38;5;241m=\u001b[39m neigh_ind[:, :n_neighbors]\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# argpartition doesn't guarantee sorted order, so we sort again\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/numpy/core/fromnumeric.py:839\u001b[0m, in \u001b[0;36margpartition\u001b[0;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    837\u001b[0m \n\u001b[1;32m    838\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('********** Train Before OverSampling **********')\n",
    "print('Slight: ', (Y_train == 'Slight').sum())\n",
    "print('Serious:', (Y_train == 'Serious').sum())\n",
    "print('Fatal:  ', (Y_train == 'Fatal').sum())\n",
    "print('\\n Total X:', len(X_train), ' Total Y:', len(Y_train), '\\n')\n",
    "\n",
    "X_train, Y_train = oversample_data(X_train, Y_train)\n",
    "\n",
    "print('********** Test **********')\n",
    "print('Slight: ', (Y_test == 'Slight').sum())\n",
    "print('Serious:', (Y_test == 'Serious').sum())\n",
    "print('Fatal:  ', (Y_test == 'Fatal').sum())\n",
    "print('\\n Total X:', len(Y_test), ' Total Y:', len(Y_test), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a4a83-3e38-4848-be3f-8ae4861230b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3d2115fc-0d52-4bd4-883d-72bbc28f44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2e5b8-f51f-4b6d-a5c7-c01239512bc7",
   "metadata": {},
   "source": [
    "### Genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95e4e1f6-4e34-462f-afad-1e203bf2c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS_TO_OPTIMIZE = {'eta': {'type': 'float',\n",
    "                                   'init': [0.01, 1],\n",
    "                                   'mutation': [-0.3, 0.3],\n",
    "                                   'round': 2\n",
    "                                   },\n",
    "                           'max_depth': {'type': 'int',\n",
    "                                         'init': [1, 25],\n",
    "                                         'mutation': [-6, 6],\n",
    "                                         'step': 1\n",
    "                                   },\n",
    "                           'min_child_weight': {'type': 'float',\n",
    "                                                'init': [0.01, 20.0],\n",
    "                                                'mutation': [-7, 7],\n",
    "                                                'round': 1\n",
    "                                   },\n",
    "                           'n_estimators': {'type': 'int',\n",
    "                                            'init': [0, 1500],\n",
    "                                            'mutation': [-150, 150],\n",
    "                                            'step': 25\n",
    "                                   },\n",
    "                           # 'gamma': {'type': 'float',\n",
    "                           #                   'init': [0.01, 10.0],\n",
    "                           #                   'mutation': [-4, 4],\n",
    "                           #                   'round': 2\n",
    "                           #         },\n",
    "                           # 'subsample': {'type': 'float', ## ATTENTION! SUBSAMPLE OF TRAINING\n",
    "                           #               'init': [0.01, 1],\n",
    "                           #               'mutation': [-0.4, 0.4],\n",
    "                           #               'round': 2\n",
    "                           #         },\n",
    "                           # 'colsample_bytree': {'type': 'float', ## ATENTION!! SUBSAMPLE OF COLUMNS\n",
    "                           #               'init': [0.01, 1],\n",
    "                           #               'mutation': [-0.4, 0.4],\n",
    "                           #               'round': 2\n",
    "                           #         },\n",
    "                           # 'reg_alpha': {'type': 'float', ## ATENTION!! MODEL MORE CONSERVATIVE!\n",
    "                           #               'init': [0, 1],\n",
    "                           #               'mutation': [-0.4, 0.4],\n",
    "                           #               'round': 2\n",
    "                           #         },\n",
    "                           # 'reg_lambda': {'type': 'float', ## ATENTION!! MODEL MORE CONSERVATIVE!\n",
    "                           #               'init': [0, 1],\n",
    "                           #               'mutation': [-0.4, 0.4],\n",
    "                           #               'round': 2\n",
    "                           #         }\n",
    "                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da533363-181a-41d6-b636-a291510b7d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# import random\n",
    "\n",
    "# Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "# Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "# Y_train_downsampled_onehot = casualty_to_one_hot(Y_train_downsampled)\n",
    "# Y_test_downsampled_onehot  = casualty_to_one_hot(Y_test_downsampled)\n",
    "\n",
    "\n",
    "# number_of_individuals = 100\n",
    "# numberOfParentsMating = 35\n",
    "# number_of_hyperparams = len(HYPERPARAMS_TO_OPTIMIZE)\n",
    "# number_of_generations = 100\n",
    "\n",
    "# populationSize = (number_of_individuals, number_of_hyperparams)\n",
    "# population = initilialize_population(number_of_individuals   = number_of_individuals,\n",
    "#                                      hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "\n",
    "# fitnessHistory = np.empty([number_of_generations+1, number_of_individuals]) # Define an array to store the value of each parameter for each parent and generation\n",
    "# populationHistory = np.empty([(number_of_generations+1)*number_of_individuals, number_of_hyperparams]) # Insert the value of initial parameters in history\n",
    "\n",
    "# best_solution_history = np.empty([(number_of_generations), number_of_hyperparams+1])\n",
    "# populationHistory[0:number_of_individuals,:] = population\n",
    "\n",
    "\n",
    "# xgbDMatrixTrain = xgb.DMatrix(data  = X_train_downsampled,\n",
    "#                               label = Y_train_downsampled)\n",
    "\n",
    "# xgbDMatrixTest  = xgb.DMatrix(data  = X_test_downsampled, \n",
    "#                               label = Y_test_downsampled)\n",
    "\n",
    "# for generation in range(number_of_generations):\n",
    "\n",
    "#     print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "#     new_population = []\n",
    "    \n",
    "#     unique_individuals = np.unique(population, axis=0)\n",
    "    \n",
    "#     new_individuals_to_create = number_of_individuals - len(unique_individuals)\n",
    "    \n",
    "#     for i in range(new_individuals_to_create):\n",
    "#         new_individual = generate_individual(hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "#         new_population.append(new_individual)\n",
    "    \n",
    "#     new_population = np.array(new_population)\n",
    "\n",
    "#     if (new_individuals_to_create):\n",
    "#         population = np.concatenate((unique_individuals, new_population), axis=0)\n",
    "\n",
    "#     # print(f'Current population is {population}')\n",
    "#     print(f'New population is {len(new_population)}')\n",
    "    \n",
    "#     # Train the dataset and obtain fitness\n",
    "#     fitnessValue = train_population(population = population,\n",
    "#                                     hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE,\n",
    "#                                     dMatrixTrain = xgbDMatrixTrain,\n",
    "#                                     dMatrixTest = xgbDMatrixTest,\n",
    "#                                     Y_test = Y_test_downsampled)\n",
    " \n",
    "#     fitnessHistory[generation,:] = fitnessValue\n",
    "\n",
    "#     # Best score in the current iteration\n",
    "#     max_score_index = np.argmax(fitnessHistory[generation,:])\n",
    "#     max_score_value = np.max(fitnessHistory[generation,:])\n",
    "#     max_score_solution = population[max_score_index]\n",
    "\n",
    "#     max_solution_with_score = []\n",
    "#     max_solution_with_score = np.append(max_score_solution, max_score_value)\n",
    "#     best_solution_history[generation] = max_solution_with_score\n",
    "\n",
    "#     print(f\"Best F1 score in the this iteration = {max_score_value}, best solution {max_score_solution}\") # Survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n",
    "    \n",
    "#     parents = new_parents_selection(population = population,\n",
    "#                                     fitness = fitnessValue,\n",
    "#                                     numParents = numberOfParentsMating)\n",
    "    \n",
    "#     # Mate these parents to create children having parameters from these parents (we are using uniform crossover)\n",
    "#     children = crossover_uniform(parents = parents,\n",
    "#                                  childrenSize = (populationSize[0] - parents.shape[0], number_of_hyperparams))\n",
    "    \n",
    "#     # Add mutation to create genetic diversity\n",
    "#     children_mutated = mutation(children,\n",
    "#                                 hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "    \n",
    "#     '''\n",
    "#     We will create new population, which will contain parents that where selected previously based on the\n",
    "#     fitness score and rest of them  will be children\n",
    "#     '''\n",
    "#     population[0:parents.shape[0], :] = parents # Fittest parents\n",
    "#     population[parents.shape[0]:, :]  = children_mutated # Children\n",
    "    \n",
    "#     populationHistory[(generation+1)*number_of_individuals : (generation+1)*number_of_individuals + number_of_individuals , :] = population # Store parent information\n",
    "    \n",
    "# #Best solution from the final iteration\n",
    "\n",
    "# fitness = train_population(population = population,\n",
    "#                            hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE,\n",
    "#                            dMatrixTrain = xgbDMatrixTrain,\n",
    "#                            dMatrixTest = xgbDMatrixTest,\n",
    "#                            Y_test = Y_test_downsampled)\n",
    "\n",
    "# fitnessHistory[generation+1, :] = fitness # index of the best solution\n",
    "# bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "\n",
    "\n",
    "# best_hyperparams = {}\n",
    "# for n_param, hyperparam in enumerate(HYPERPARAMS_TO_OPTIMIZE):\n",
    "#     best_hyperparams[hyperparam] = population[bestFitnessIndex][n_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f518cd5-dc64-4674-8518-3ebdad3d6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### PLOT FITNESS EVOLUTION ####\n",
    "# x_fitness = [np.max(fitnessHistory[i]) for i in range(0,fitnessHistory.shape[0])]\n",
    "\n",
    "# FILE_NAME = 'leeds_ga_' + MODEL_TIMESTAMP  + '.jpg'\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(np.arange(len(x_fitness)), x_fitness)\n",
    "# plt.savefig(GA_SCORES_PATH + FILE_NAME)\n",
    "\n",
    "# #### PLOT HYPERPARAMS EVOLUTION ####\n",
    "# FILE_NAME = f\"leeds_ga_hyperparams_evolution_p{number_of_individuals}_c{numberOfParentsMating}_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# LEGEND_LABELS = HYPERPARAMS_TO_OPTIMIZE.keys()\n",
    "\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# best_solution_history_aux = best_solution_history\n",
    "# best_solution_history_aux[:,1] = best_solution_history[:,1]/2\n",
    "# best_solution_history_aux[:,3] = best_solution_history[:,3]/100\n",
    "# plt.plot(best_solution_history_aux)\n",
    "# plt.legend(LEGEND_LABELS)\n",
    "# plt.savefig(HYPERPARAMS_EVOLUTON_PATH +  'TEST.jpg', dpi=300)\n",
    "\n",
    "# FILE_NAME = f\"leeds_population_p{number_of_individuals}_c{numberOfParentsMating}_{MODEL_TIMESTAMP}.txt\"\n",
    "\n",
    "# np.savetxt(FINAL_POPULATION_PATH + FILE_NAME, population, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69081589-f481-4d74-94d4-7b175655424a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3182b535-b21b-40b5-b3cc-ba436ca59f64",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc61265c-a5ea-4e87-a0d0-61fb2af14304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_NAME = 'leeds_hyperparams_2022-04-28-19:07:55.json'\n",
    "\n",
    "best_hyperparams = load_json(HYPERPARAMS_PATH, FILE_NAME)\n",
    "\n",
    "# # # 0.04 2 6.5 --> 0.778\n",
    "# best_hyperparams = {}\n",
    "# best_hyperparams['eta'] = 0.1\n",
    "# best_hyperparams['max_depth'] = 2\n",
    "# best_hyperparams['min_child_weight'] = 1\n",
    "# best_hyperparams['n_estimators'] = 583\n",
    "\n",
    "# 1.00e-01 2.00e+00 1.00e+00 5.83e+02 --> 0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1064316-3dfa-4bf2-8f31-933ec98df452",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cálculo de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21a281da-a027-477b-bda5-0b41257ffaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "# Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "# space={'max_depth': hp.quniform(\"max_depth\", 3, 25, 1),\n",
    "#         'gamma': hp.uniform ('gamma', 1,9),\n",
    "#         'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "#         'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "#         'min_child_weight' : hp.quniform('min_child_weight', 0, 15, 1),\n",
    "#         'n_estimators': hp.quniform('n_estimators', 100, 4000, 100),\n",
    "#         'tree_method': 'gpu_hist'\n",
    "#     }\n",
    "\n",
    "# def objective(space):\n",
    "#     clf = XGBClassifier(n_estimators = int(space['n_estimators']),\n",
    "#                         max_depth = int(space['max_depth']),\n",
    "#                         gamma = space['gamma'],\n",
    "#                         reg_alpha = int(space['reg_alpha']),\n",
    "#                         min_child_weight = int(space['min_child_weight']),\n",
    "#                         colsample_bytree = int(space['colsample_bytree']),\n",
    "#                         tree_method = space['tree_method']\n",
    "#                        )\n",
    "    \n",
    "#     evaluation = [(X_train, Y_train), (X_test, Y_test)]\n",
    "    \n",
    "#     clf.fit(X_train, Y_train,\n",
    "#             eval_set = evaluation, eval_metric = \"auc\",\n",
    "#             early_stopping_rounds = 10, verbose = False)\n",
    "            \n",
    "    \n",
    "#     pred = clf.predict(X_test)\n",
    "#     accuracy = accuracy_score(Y_test, pred>0.5)\n",
    "#     print (\"SCORE:\", accuracy)\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "# trials = Trials()\n",
    "\n",
    "# best_hyperparams = fmin(fn = objective,\n",
    "#                         space = space,\n",
    "#                         algo = tpe.suggest,\n",
    "#                         max_evals = 100,\n",
    "#                         trials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fabde-6f45-4c38-9800-6d568b65d526",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b19a759-7ba4-45a6-a1b0-b74589527b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = f\"leeds_hyperparams_{MODEL_TIMESTAMP}.json\"\n",
    "\n",
    "# write_json(best_hyperparams, HYPERPARAMS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a8ddd-0cea-4817-b248-153387ae9a1f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4634264-34da-4355-af3f-99650d45068d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga definitiva/auxiliar de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebc51d38-c18a-46e7-968d-4a8e40b91d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FILE_NAME = 'leeds_calculated_weights.json'\n",
    "FILE_NAME = 'leeds_weights2022-04-14-10:55:09.json'\n",
    "\n",
    "feature_vector = load_json(WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e7364-7e4b-4ecf-9af2-559deb20df63",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cálculo de pesos de caracetrísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf8e4a99-5e84-4c6e-af5c-7451e342ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import loadtxt\n",
    "# from xgboost import XGBClassifier,XGBRanker\n",
    "# from matplotlib import pyplot\n",
    "# from xgboost import plot_importance\n",
    "\n",
    "# xgboost = XGBClassifier(best_hyperparams,\n",
    "#                         tree_method = 'gpu_hist',\n",
    "#                         single_precision_histogram =  True)\n",
    "\n",
    "# xgboost.fit(X_train, Y_train)\n",
    "\n",
    "# child_weights  = np.array(xgboost.feature_importances_)\n",
    "# feature_vector = fill_feature_vector(X_train, child_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e34dce-2ff6-474c-b5cc-09362f4d64e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualización pesos calculados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fb50b3c-235c-4c50-b8ab-ff4c199e8a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FILE_NAME = f\"leeds_figure_weights_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# print(xgboost.get_booster().get_score(importance_type= 'weight'))\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.barh(X_train.columns, xgboost.feature_importances_)\n",
    "# plt.savefig(WEIGHTS_PATH + FILE_NAME)\n",
    "\n",
    "# for column, weight in zip(X_train.columns,xgboost.feature_importances_):\n",
    "#   print(column, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ffbda-1e05-4edc-84c2-5d8a94796af4",
   "metadata": {},
   "source": [
    "#### Escritura de pesos de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca0f0895-d1e0-49a4-a88c-a91cfd9f119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_indexes = fv2gi(feature_vector)\n",
    "\n",
    "# FILE_NAME = f\"leeds_weights_{MODEL_TIMESTAMP}.json\"\n",
    "# # FILE_NAME = 'leeds_default_weights.json'\n",
    "\n",
    "# write_json(feature_vector, WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291b03b-03f6-44a8-8030-987312e24667",
   "metadata": {},
   "source": [
    "### Cálculo índices de matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "209fd454-5668-4ab9-bc58-c30a546ac4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fnVqjYFXk",
   "metadata": {
    "id": "4b3fnVqjYFXk",
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21dcf79f",
   "metadata": {
    "id": "21dcf79f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42737\n",
      "16245\n",
      "4062\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.299074</td>\n",
       "      <td>0.952947</td>\n",
       "      <td>-1.153068</td>\n",
       "      <td>-0.634323</td>\n",
       "      <td>0.181166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.958225</td>\n",
       "      <td>0.519257</td>\n",
       "      <td>1.210413</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.082918</td>\n",
       "      <td>2.179661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.264752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  0.000000  0.000000  0.216970  0.000000  0.000000\n",
       "1  0.299074  0.952947 -1.153068 -0.634323  0.181166\n",
       "2  0.000000  1.958225  0.519257  1.210413  0.000000\n",
       "3  0.000000  1.082918  2.179661  0.000000  0.000000\n",
       "4  0.000000  0.000000  1.264752  0.000000  0.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bgi = build_gray_images(X_train, 5, matrix_indexes)\n",
    "train_original_bgi = build_gray_images(X_train_original, 5, matrix_indexes)\n",
    "\n",
    "test_bgi  = build_gray_images(X_test, 5, matrix_indexes)\n",
    "\n",
    "\n",
    "pd.DataFrame(train_bgi[:,:,1057])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_5saNqHWX4C_",
   "metadata": {
    "id": "_5saNqHWX4C_",
    "tags": []
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbdca6dd",
   "metadata": {
    "id": "bbdca6dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_images = shape_images(X_data = X_train,\n",
    "                            gray_images = train_bgi)\n",
    "test_images  = shape_images(X_data = X_test,\n",
    "                            gray_images = test_bgi)\n",
    "\n",
    "\n",
    "train_original_images = shape_images(X_data = X_train_original,\n",
    "                            gray_images = train_original_bgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "15a5e9c9",
   "metadata": {
    "id": "15a5e9c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL4AAADCCAYAAAD3lHgnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAHt0lEQVR4nO3d32vd9R3H8ddrWcQER4SulNCUNRSRFWERQhFyV3IR3Zi3FpYroTcTKkiHu/QfEG+8KbN1dKJY9EJGhhRWkYBTY1fFLgqtbDUYyFaxWlK0dW8vzrmIaTTfknw+33P6fj7gQM5JOJ83zbNfTs6Pz9cRISCbn7Q9ANAGwkdKhI+UCB8pET5SInyk9NMSdzo0NBQjIyMl7rpvDA4Otj2Crl+/3vYIrbty5YquXbvm9bcXCX9kZESzs7Ml7rpvjI6Otj2ClpeX2x6hdSdPntzwdh7qICXCR0qEj5QIHykRPlIifKRE+EiJ8JES4SMlwkdKhI+UGoVve8b2x7Yv2H6y9FBAaZuGb3tA0rOSHpS0X9Ih2/tLDwaU1OSIf0DShYj4JCK+kfSSpIfLjgWU1ST83ZI+XXN9qXvb99g+bHvB9sLq6up2zQcU0ST8m97EL+mmzXgi4lhETEbE5PDw8NYnAwpqEv6SpD1rro9J+qzMOEAdTcJ/V9I9tsdt3yHpEUmvlR0LKGvTjx5GxA3bj0l6XdKApOMRcb74ZEBBjT5zGxFzkuYKzwJUwyu3SInwkRLhIyXCR0qEj5QIHykRPlIifKRE+EiJ8JGSS5zu03br5xC9dOlSq+ufOHGi1fUl6dSpU22PoImJiVbXn5ub0+XLl296az1HfKRE+EiJ8JES4SMlwkdKhI+UCB8pET5SInykRPhIifCREuEjpSbbhB+3vWL7wxoDATU0OeI/L2mm8BxAVZuGHxFvSvq8wixANTzGR0qN9s5swvZhSYe36/6AkrYt/Ig4JumY1BufwAJ+DA91kFKTpzNflPSWpHttL9l+tPxYQFlNTgxxqMYgQE081EFKhI+UCB8pET5SInykRPhIifCREuEjJcJHSoSPlAgfKW3b25LX2rVrl2ZnZ0vcdWNTU1Otrj84ONjq+pJ08eLFtkfQ0aNHW13/h34PHPGREuEjJcJHSoSPlAgfKRE+UiJ8pET4SInwkRLhIyXCR0qEj5Sa7KS2x/YZ24u2z9s+UmMwoKQm7868IemJiDhr+2eS3rN9OiL+VXg2oJgmJ4ZYjoiz3a+/krQoaXfpwYCSbukxvu29ku6X9PYG3ztse8H2wurq6jaNB5TROHzbd0l6RdLjEfHl+u9HxLGImIyIyeHh4e2cEdh2jcK3PahO9C9ExKtlRwLKa/KsjiU9J2kxIp4uPxJQXpMj/pSkWUkHbZ/rXh4qPBdQVJMTQ8xLcoVZgGp45RYpET5SInykRPhIifCREuEjJcJHSoSPlAgfKRE+UiqyP34v2LFjR6vr79u3r9X1pfb3pu9lHPGREuEjJcJHSoSPlAgfKRE+UiJ8pET4SInwkRLhIyXCR0qEj5Sa7KR2p+13bL/f3R//qRqDASU1eXfm15IORsTV7h6a87b/FhH/KDwbUEyTndRC0tXu1cHuJUoOBZTWdLfkAdvnJK1IOh0RN+2PD/STRuFHxLcRMSFpTNIB2/et/xlODIF+ckvP6kTEF5LekDSzwfc4MQT6RpNndXbavrv79ZCkaUkfFZ4LKKrJszqjkv5se0Cd/ygvR8Rfy44FlNXkWZ0P1DnhG3Db4JVbpET4SInwkRLhIyXCR0qEj5QIHykRPlIifKRE+EiJ8JHSbXtiiOnp6VbXn5+fb3V9SRofH297hJ7FER8pET5SInykRPhIifCREuEjJcJHSoSPlAgfKRE+UiJ8pET4SKlx+N0dk/9pm13U0Pdu5Yh/RNJiqUGAmprujz8m6deS/lR2HKCOpkf8ZyT9QdL/f+gH2B8f/aTJNuG/kbQSEe/92M+xPz76SZMj/pSk39r+t6SXJB20/ZeiUwGFbRp+RPwxIsYiYq+kRyT9PSJ+V3wyoCCex0dKt/Rh84h4Q51zYAF9jSM+UiJ8pET4SInwkRLhIyXCR0qEj5QIHykRPlIifKRE+EjJEbH9d2r/V9J/tnAXP5f0v20apx/XZ4btW/8XEbFz/Y1Fwt8q2wsRMZl1fWYovz4PdZAS4SOlXg3/WPL1JWYoun5PPsYHSuvVIz5QVE+Fb3vG9se2L9h+soX1j9tesf1h7bXXzLDH9hnbi7bP2z5Sef07bb9j+/3u+k/VXH/dLMW2reyZ8G0PSHpW0oOS9ks6ZHt/5TGelzRTec31bkh6IiJ+KekBSb+v/O/wtaSDEfErSROSZmw/UHH9tYptW9kz4Us6IOlCRHwSEd+os4fPwzUHiIg3JX1ec80NZliOiLPdr79S5xe/u+L6ERFXu1cHu5fqfwiW3rayl8LfLenTNdeXVPEX3ots75V0v6S3K687YPucpBVJpyOi6vpdz2iTbSu3opfC9wa3pX3KyfZdkl6R9HhEfFlz7Yj4NiImJI1JOmD7vprrN922cit6KfwlSXvWXB+T9FlLs7TK9qA60b8QEa+2NUdEfKHOPkq1/+4pvm1lL4X/rqR7bI/bvkOd7Qpfa3mm6mxb0nOSFiPi6RbW32n77u7XQ5KmJX1Uc4Ya21b2TPgRcUPSY5JeV+cPupcj4nzNGWy/KOktSffaXrL9aM31u6YkzapzlDvXvTxUcf1RSWdsf6DOweh0RNx2Z8HhlVuk1DNHfKAmwkdKhI+UCB8pET5SInykRPhIifCR0nfjQe2d6WiQgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "for i in range(0,3):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(train_bgi[:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ad520774-b249-4575-855a-4a4fee307bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "609a9bfb",
   "metadata": {
    "id": "609a9bfb"
   },
   "outputs": [],
   "source": [
    "# input_shape = (5, 5)\n",
    "\n",
    "array_train_images = np.asarray(train_images)\n",
    "array_test_images  = np.asarray(test_images)\n",
    "\n",
    "array_train_original_images = np.asarray(train_original_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab443960-cace-48c5-ad61-2209386324f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb121c88",
   "metadata": {
    "id": "cb121c88"
   },
   "outputs": [],
   "source": [
    "######### EN TERMINAL #########\n",
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fm72aQrpYW7V",
   "metadata": {
    "id": "fm72aQrpYW7V",
    "tags": []
   },
   "source": [
    "## One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01afbb97",
   "metadata": {
    "id": "01afbb97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41373/3907942643.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Y_labels.replace(transf, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "Y_train_original_onehot = casualty_to_one_hot(Y_train_original)\n",
    "Y_test_onehot  = casualty_to_one_hot(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V7Azjtl8gRth",
   "metadata": {
    "id": "V7Azjtl8gRth",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "SzhUTtEti5oE",
   "metadata": {
    "id": "SzhUTtEti5oE"
   },
   "outputs": [],
   "source": [
    "# !conda install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qKYh5EeThQ_7",
   "metadata": {
    "id": "qKYh5EeThQ_7",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "PLcI-nQShVR1",
   "metadata": {
    "id": "PLcI-nQShVR1"
   },
   "outputs": [],
   "source": [
    "# correlation_matrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kISRP5AQhWTD",
   "metadata": {
    "id": "kISRP5AQhWTD",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f285158d-69c6-4dfc-b728-6d4bd4b7cf1c",
   "metadata": {
    "id": "f285158d-69c6-4dfc-b728-6d4bd4b7cf1c"
   },
   "outputs": [],
   "source": [
    "# pca(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70370ce9-acc4-4fe7-bea0-c592909a305d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58ec350b-8032-4ecc-a38e-9db86efb2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_name = './2d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=2)\n",
    "\n",
    "# output_file_name = './2d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=2)\n",
    "\n",
    "# output_file_name = './3d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=3)\n",
    "\n",
    "# output_file_name = './3d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EkC9ryU4lFwg",
   "metadata": {
    "id": "EkC9ryU4lFwg",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cCo2emMclT8h",
   "metadata": {
    "id": "cCo2emMclT8h",
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4xBSSjInlHj1",
   "metadata": {
    "id": "4xBSSjInlHj1"
   },
   "outputs": [],
   "source": [
    "X_train = array_train_images\n",
    "X_test = array_test_images\n",
    "X_train_original = array_train_original_images\n",
    "\n",
    "X_train = X_train.reshape(len(array_train_images), 25)\n",
    "X_train_original = X_train_original.reshape(len(array_train_original_images), 25)\n",
    "X_test  = X_test.reshape(len(X_test), 25)\n",
    "\n",
    "# autoencoder().fit(X_train, X_train,\n",
    "#                 epochs=15,\n",
    "#                 batch_size=32,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gJfbDNO5oB1N",
   "metadata": {
    "id": "gJfbDNO5oB1N",
    "tags": []
   },
   "source": [
    "#### Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2NjR_MDJlXsJ",
   "metadata": {
    "id": "2NjR_MDJlXsJ"
   },
   "outputs": [],
   "source": [
    "# # create encoder model\n",
    "# encoder = Model(inputs=input_img, outputs=encoder2)\n",
    "# encoder.summary()\n",
    "# # create decoder model\n",
    "# encoded_input = Input(shape=(3,))\n",
    "# #lo que hace aqui es quedarse con las capas que corresponden al decodificador\n",
    "# decoder_layer1 = autoencoder.layers[-2]\n",
    "# decoder_layer2 = autoencoder.layers[-1]\n",
    "# decoder = Model(inputs=encoded_input, outputs=decoder_layer2(decoder_layer1(encoded_input)))\n",
    "# decoder.summary()\n",
    "# # si miramos la salida, son simetricos el uno respecto al otro\n",
    "# # encoder va de input a 3 y decoder de 3 a input\n",
    "\n",
    "# # get latent vector for visualization\n",
    "# latent_vector = encoder.predict(X_test)\n",
    "# # get decoder output to visualize reconstructed image\n",
    "# reconstructed_imgs = decoder.predict(latent_vector)\n",
    "\n",
    "\n",
    "# # visualize in 3D plot\n",
    "# from pylab import rcParams\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "# fig = plt.figure(1)\n",
    "# ax = Axes3D(fig)\n",
    "\n",
    "# xs = latent_vector[:, 0]\n",
    "# ys = latent_vector[:, 1]\n",
    "# zs = latent_vector[:, 2]\n",
    "\n",
    "# color=['red','green','blue']\n",
    "\n",
    "# for x, y, z, label in zip(xs, ys, zs, Y_test):\n",
    "#     c = color[int(label)]\n",
    "#     ax.text(x, y, z, label, backgroundcolor=c)\n",
    "    \n",
    "# ax.set_xlim(xs.min(), xs.max())\n",
    "# ax.set_ylim(ys.min(), ys.max())\n",
    "# ax.set_zlim(zs.min(), zs.max())\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# # X_test_encoded = encoder.predict(X_test, batch_size=32)\n",
    "# # plt.figure(figsize=(6, 6))\n",
    "# # plt.scatter(X_test_encoded[:, 0], X_test_encoded[:, 1], c=Y_test)\n",
    "# # plt.colorbar()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "07b3666f-f387-4ce1-b0f7-f1e12e3a61e7",
   "metadata": {
    "id": "07b3666f-f387-4ce1-b0f7-f1e12e3a61e7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_TSNE(X_data, Y_data, n_components, output_file_name=None):\n",
    "    X_data_scaled = StandardScaler().fit_transform(X_data)\n",
    "    z_data = TSNE(n_components=n_components).fit_transform(X_data_scaled)\n",
    "    # X_test_scaled = StandardScaler().fit_transform(X_test)\n",
    "    # z_test = TSNE(n_components=2).fit_transform(X_test_scaled)\n",
    "\n",
    "    palette = sns.color_palette(\"hls\", 3)\n",
    "    fig,ax  = plt.subplots(1,1,figsize=(15,10))\n",
    "    sns.scatterplot(x = z_data[:,0],\n",
    "                    y = z_data[:,1],\n",
    "                    hue = Y_data,\n",
    "                    palette = palette,\n",
    "                    legend = 'full')\n",
    "\n",
    "    if (output_file_name): plt.savefig('./Out/' + output_file_name)\n",
    "\n",
    "# output_file_name = './2d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=2)\n",
    "\n",
    "# output_file_name = './2d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=2)\n",
    "\n",
    "# output_file_name = './3d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=3)\n",
    "\n",
    "# output_file_name = './3d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9997e-960d-46ef-9505-0e06add18c11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd452f93-49cb-41a6-aeca-287f08ddba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pesos calculados: [ 0.3801067   2.87420382 47.08695652] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_test_labels = one_hot_to_casualty(Y_test)\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "pesos = class_weight.compute_class_weight('balanced',\n",
    "                                          classes = np.unique(Y_train_original),\n",
    "                                          y = Y_train_original)\n",
    "\n",
    "\n",
    "print('\\nPesos calculados:', pesos, '\\n\\n')\n",
    "\n",
    "\n",
    "# Keras espera un diccionario donde la clave sea el número de clase \n",
    "# y el valor sea el peso calculado. \n",
    "pesos = dict(enumerate(pesos))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6a617ac8-991b-42f4-8c09-7be36e278ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f30e14d-d989-4df9-8f1b-cfc2f0457f1b",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2a3a799-c5eb-401f-aff9-1d35b5593885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "MODEL_NAME = MODELS_NAME[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdf8b87-5b35-4a90-bb72-63be7771e11a",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7ac1ac3c-85d8-485d-8b28-48eb4c74d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_size = list(range(1,10, 2))\n",
    "n_neighbors = list(range(1,25, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d9b1f562-15d0-41bb-a867-c62368d06aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create new KNN object\n",
    "# hyperparameters = dict(leaf_size = leaf_size,\n",
    "#                        n_neighbors = n_neighbors)\n",
    "\n",
    "# # Use GridSearch\n",
    "# knn_2 = KNeighborsClassifier()\n",
    "\n",
    "# # Fit the model\n",
    "# clf = GridSearchCV(knn_2,\n",
    "#                    hyperparameters,\n",
    "#                    cv = 5)\n",
    "\n",
    "# knn = clf.fit(X_train, Y_train)\n",
    "\n",
    "# # Print The value of best Hyperparameters\n",
    "\n",
    "# best_leaf_size = knn.best_estimator_.get_params()['leaf_size']\n",
    "# best_n_neighbors = knn.best_estimator_.get_params()['n_neighbors']\n",
    "\n",
    "# print('Best leaf_size:', best_leaf_size)\n",
    "# print('Best n_neighbors:', best_n_neighbors)\n",
    "\n",
    "# df = pd.DataFrame({'best_leaf_size':[best_leaf_size], 'n_neighbors':[best_n_neighbors]})\n",
    "\n",
    "# FILE_NAME = f\"{MODEL_NAME}/leeds_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "# df.to_csv(HYPERPARAMS_PATH + FILE_NAME, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639e1f7-0d13-4f7e-9343-5a150748483c",
   "metadata": {},
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2062638c-cd68-475a-ab10-9338d001f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"leeds_{MODEL_NAME}_{MODEL_TIMESTAMP}.pkl\"\n",
    "\n",
    "# # Its important to use binary mode \n",
    "# knnPickle = open(MODEL_PATH + MODEL_FILE_NAME, 'wb') \n",
    "\n",
    "# # source, destination \n",
    "# pickle.dump(knn, knnPickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ccdf81-4c4c-4bcb-b10c-d609dc245145",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a50e52bc-3781-4c15-a161-65999d90d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"leeds_{MODEL_NAME}_2022-04-27-21:50:26.pkl\"\n",
    "\n",
    "# # load the model from disk\n",
    "# loaded_model = pickle.load(open(MODEL_PATH + MODEL_FILE_NAME, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd32bbc5-bf0c-484e-bbcd-882884b7bac3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "066b3ae0-0fb4-49ed-b42a-4a2cb3f02b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = tf.argmax(Y_test_onehot, axis=1)\n",
    "# y_predicted = knn.predict(X_test)\n",
    "\n",
    "\n",
    "# ############## SAVE CLASSIFICATION REPORT ##############\n",
    "# report = classification_report(y_true,\n",
    "#                                y_predicted,\n",
    "#                                target_names = Y_test_labels.unique(),\n",
    "#                                output_dict = True)\n",
    "\n",
    "# REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "# REPORT_NAME  = f\"leeds_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "# report_df = pd.DataFrame(report).transpose()\n",
    "# report_df.to_csv(REPORT_PATH + REPORT_NAME, index= True)\n",
    "\n",
    "\n",
    "# ############## SAVE CONFUSION MATRIX ##############\n",
    "\n",
    "# CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "# CONFUSION_MATRIX_NAME  = f\"leeds_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# cm = confusion_matrix(y_true,\n",
    "#                       y_predicted,\n",
    "#                       labels = Y_test.unique())\n",
    "\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix = cm,\n",
    "#                               display_labels = Y_test_labels.unique()).plot()\n",
    "\n",
    "# plt.savefig(CONFUSION_MATRIX_PATH + CONFUSION_MATRIX_NAME, dpi = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3001677-1025-4c6b-95d0-c758cb05cd8f",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolution 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9b2fd87f-5a8a-4acc-bc1e-9674cb389bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS_NAME[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f3ff34-855b-4176-9d89-b3089a202100",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8889a47c-21e2-4965-8713-a30eccde6f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mconvolution_1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_train_original_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_original_onehot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpesos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_test_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test_onehot\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m history\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:963\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 963\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    966\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    967\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:785\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_internal_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    789\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2983\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2981\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m-> 2983\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3292\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3288\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[1;32m   3289\u001b[0m       args, kwargs, flat_args, filtered_flat_args)\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd_call_context(cache_key\u001b[38;5;241m.\u001b[39mcall_context)\n\u001b[0;32m-> 3292\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                          graph_function)\n\u001b[1;32m   3296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3130\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3125\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m   3127\u001b[0m ]\n\u001b[1;32m   3128\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m   3129\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[0;32m-> 3130\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3139\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   3141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m   3142\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m   3143\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m   3144\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m   3145\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   3148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1161\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1159\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1161\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[1;32m   1166\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:677\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    674\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    675\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    676\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 677\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1136\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:427\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m   program_ctx \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mProgramContext(options\u001b[38;5;241m=\u001b[39moptions)\n\u001b[0;32m--> 427\u001b[0m   converted_f \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_actual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_entity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogram_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mhas_verbosity(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    429\u001b[0m     _log_callargs(converted_f, effective_args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:269\u001b[0m, in \u001b[0;36m_convert_actual\u001b[0;34m(entity, program_ctx)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(entity, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__code__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    265\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply autograph to a function that doesn\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mt \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    266\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpose a __code__ object. If this is a @tf.function,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    267\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m try passing f.python_function instead.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 269\u001b[0m transformed, module, source_map \u001b[38;5;241m=\u001b[39m \u001b[43m_TRANSPILER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogram_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_module\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformed, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_source_map\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py:282\u001b[0m, in \u001b[0;36mGenericTranspiler.transform\u001b[0;34m(self, obj, user_context)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m\"\"\"Transforms a Python object.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mUsers typically call this method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m  NotImplementedError: if the type of obj is not handled.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(obj):\n\u001b[0;32m--> 282\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-function: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(obj)))\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py:466\u001b[0m, in \u001b[0;36mPyToPy.transform_function\u001b[0;34m(self, fn, user_context)\u001b[0m\n\u001b[1;32m    464\u001b[0m logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not cached for subkey \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, fn, cache_subkey)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Confusing overloading pattern. Fix.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m nodes, ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPyToPy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nodes, gast\u001b[38;5;241m.\u001b[39mLambda):\n\u001b[1;32m    469\u001b[0m   nodes \u001b[38;5;241m=\u001b[39m gast\u001b[38;5;241m.\u001b[39mAssign(\n\u001b[1;32m    470\u001b[0m       targets\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    471\u001b[0m           gast\u001b[38;5;241m.\u001b[39mName(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    476\u001b[0m       ],\n\u001b[1;32m    477\u001b[0m       value\u001b[38;5;241m=\u001b[39mnodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py:359\u001b[0m, in \u001b[0;36mGenericTranspiler.transform_function\u001b[0;34m(self, fn, user_context)\u001b[0m\n\u001b[1;32m    356\u001b[0m context \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mContext(entity_info, namer, user_context)\n\u001b[1;32m    358\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_erase_arg_defaults(node)\n\u001b[0;32m--> 359\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_ast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, context\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:247\u001b[0m, in \u001b[0;36mPyToTF.transform_ast\u001b[0;34m(self, node, ctx)\u001b[0m\n\u001b[1;32m    243\u001b[0m   node \u001b[38;5;241m=\u001b[39m asserts\u001b[38;5;241m.\u001b[39mtransform(node, ctx)\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# Note: sequencing continue canonicalization before for loop one avoids\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# dealing with the extra loop increment operation that the for\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# canonicalization creates.\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[43mcontinue_statements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m node \u001b[38;5;241m=\u001b[39m return_statements\u001b[38;5;241m.\u001b[39mtransform(node, ctx)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39muser\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39muses(converter\u001b[38;5;241m.\u001b[39mFeature\u001b[38;5;241m.\u001b[39mLISTS):\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/converters/continue_statements.py:161\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m(node, ctx)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(node, ctx):\n\u001b[1;32m    160\u001b[0m   node \u001b[38;5;241m=\u001b[39m qual_names\u001b[38;5;241m.\u001b[39mresolve(node)\n\u001b[0;32m--> 161\u001b[0m   node \u001b[38;5;241m=\u001b[39m \u001b[43mactivity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m   node \u001b[38;5;241m=\u001b[39m ContinueCanonicalizationTransformer(ctx)\u001b[38;5;241m.\u001b[39mvisit(node)\n\u001b[1;32m    164\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py:700\u001b[0m, in \u001b[0;36mresolve\u001b[0;34m(node, context, parent_scope)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolve\u001b[39m(node, context, parent_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 700\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mActivityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_scope\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processing_expr_node:\n\u001b[1;32m    439\u001b[0m   entry_expr_value \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 441\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (processing_expr_node \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, gast\u001b[38;5;241m.\u001b[39mExpr) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     (result\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entry_expr_value)):\n\u001b[1;32m    447\u001b[0m   \u001b[38;5;66;03m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[1;32m    448\u001b[0m   \u001b[38;5;66;03m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[1;32m    449\u001b[0m   \u001b[38;5;66;03m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/ast.py:407\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    405\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    406\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py:592\u001b[0m, in \u001b[0;36mActivityAnalyzer.visit_FunctionDef\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;66;03m# Track the body separately. This is for compatibility reasons, it may not\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;66;03m# be strictly needed.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enter_scope(\u001b[38;5;28;01mFalse\u001b[39;00m, node\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 592\u001b[0m node\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exit_and_record_scope(node, NodeAnno\u001b[38;5;241m.\u001b[39mBODY_SCOPE)\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exit_and_record_scope(node, NodeAnno\u001b[38;5;241m.\u001b[39mARGS_AND_BODY_SCOPE)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py:336\u001b[0m, in \u001b[0;36mNodeStateTracker.visit_block\u001b[0;34m(self, nodes, before_visit, after_visit)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m before_visit:\n\u001b[1;32m    333\u001b[0m   \u001b[38;5;66;03m# TODO(mdan): We can modify node here too, if ever needed.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m   before_visit()\n\u001b[0;32m--> 336\u001b[0m replacement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m after_visit \u001b[38;5;129;01mand\u001b[39;00m replacement:\n\u001b[1;32m    339\u001b[0m   replacement, new_destination \u001b[38;5;241m=\u001b[39m after_visit(replacement)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processing_expr_node:\n\u001b[1;32m    439\u001b[0m   entry_expr_value \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 441\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (processing_expr_node \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, gast\u001b[38;5;241m.\u001b[39mExpr) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     (result\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entry_expr_value)):\n\u001b[1;32m    447\u001b[0m   \u001b[38;5;66;03m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[1;32m    448\u001b[0m   \u001b[38;5;66;03m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[1;32m    449\u001b[0m   \u001b[38;5;66;03m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/ast.py:407\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    405\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    406\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py:642\u001b[0m, in \u001b[0;36mActivityAnalyzer.visit_With\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit_With\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[1;32m    641\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enter_scope(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 642\u001b[0m   node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneric_visit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exit_and_record_scope(node, NodeAnno\u001b[38;5;241m.\u001b[39mBODY_SCOPE)\n\u001b[1;32m    644\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/ast.py:483\u001b[0m, in \u001b[0;36mNodeTransformer.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m old_value:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, AST):\n\u001b[0;32m--> 483\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py:441\u001b[0m, in \u001b[0;36mBase.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processing_expr_node:\n\u001b[1;32m    439\u001b[0m   entry_expr_value \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 441\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Adjust for consistency: replacing the value of an Expr with\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# an Assign node removes the need for the Expr node.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (processing_expr_node \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, gast\u001b[38;5;241m.\u001b[39mExpr) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     (result\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m entry_expr_value)):\n\u001b[1;32m    447\u001b[0m   \u001b[38;5;66;03m# When the replacement is a list, it is assumed that the list came\u001b[39;00m\n\u001b[1;32m    448\u001b[0m   \u001b[38;5;66;03m# from a template that contained a number of statements, which\u001b[39;00m\n\u001b[1;32m    449\u001b[0m   \u001b[38;5;66;03m# themselves are standalone and don't require an enclosing Expr.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/ast.py:407\u001b[0m, in \u001b[0;36mNodeVisitor.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    405\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisit_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m node\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    406\u001b[0m visitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_visit)\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py:647\u001b[0m, in \u001b[0;36mActivityAnalyzer.visit_withitem\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit_withitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[0;32m--> 647\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_statement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py:335\u001b[0m, in \u001b[0;36mActivityAnalyzer._process_statement\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_statement\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enter_scope(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 335\u001b[0m   node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneric_visit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exit_and_record_scope(node)\n\u001b[1;32m    337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/ast.py:492\u001b[0m, in \u001b[0;36mNodeTransformer.generic_visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    490\u001b[0m     old_value[:] \u001b[38;5;241m=\u001b[39m new_values\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(old_value, AST):\n\u001b[0;32m--> 492\u001b[0m     new_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28mdelattr\u001b[39m(node, field)\n",
      "File \u001b[0;32m~/anaconda3/envs/TFM/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transformer.py:417\u001b[0m, in \u001b[0;36mBase.visit\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m       \u001b[38;5;66;03m# TODO(mdan): Look into allowing to rewrite the AST here.\u001b[39;00m\n\u001b[1;32m    415\u001b[0m       apply_fn(target, values)\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[1;32m    418\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, gast\u001b[38;5;241m.\u001b[39mAST):\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;66;03m# This is not that uncommon a mistake: various node bodies are lists, for\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;66;03m# example, posing a land mine for transformers that need to recursively\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# call `visit`.  The error needs to be raised before the exception handler\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# below is installed, because said handler will mess up if `node` is not,\u001b[39;00m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# in fact, a node.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvalid value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mast.AST\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; to\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    425\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m visit lists of nodes, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisit_block\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    426\u001b[0m                \u001b[38;5;28mtype\u001b[39m(node))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = convolution_1d.fit(array_train_images, Y_train_onehot,\n",
    "                             class_weight = pesos,\n",
    "                             batch_size = 128,\n",
    "                             epochs = 100,\n",
    "                             shuffle = True,\n",
    "                             validation_data = (array_test_images, Y_test_onehot))\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeea268-a784-4906-ade9-c0ee7f749e42",
   "metadata": {},
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7866a3-0f39-4874-87bc-4035170e2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"leeds_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\"\n",
    "\n",
    "# tasp_cnn.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee308fe0-3b84-474c-a548-d322cc79cc0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16047c-c9b8-4011-8462-c8a7242d6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "MODEL_FILE_NAME = 'leeds_convolution_1d_2022-05-11-08:53:52.h5'\n",
    "\n",
    "convolution_1d = tf.keras.models.load_model(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ac8ee-0968-473e-8cb8-5c479d15a69a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04864929-acd0-4134-99bd-66514b4124d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = convolution_1d.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_f1_score_history(f1_score_path = F1_SCORE_PATH,\n",
    "                      f1_score_name = F1_SCORE_NAME,\n",
    "                      history = history)\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "\n",
    "REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "plot_classification_report(path = REPORT_PATH,\n",
    "                           file_name = REPORT_NAME,\n",
    "                           y_true = Y_test,\n",
    "                           y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "CONFUSION_MATRIX_NAME = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "                      file_name = CONFUSION_MATRIX_NAME,\n",
    "                      y_true = Y_test,\n",
    "                      y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qWl5nEFhfoLX",
   "metadata": {
    "id": "qWl5nEFhfoLX",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolution 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233b8c68-5a14-4094-8239-7885568b6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS_NAME[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ilUM9ijCge",
   "metadata": {
    "id": "81ilUM9ijCge",
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xQuI-4TFjBEV",
   "metadata": {
    "id": "xQuI-4TFjBEV"
   },
   "outputs": [],
   "source": [
    "history = tasp_cnn.fit(array_train_images, Y_train_onehot,\n",
    "                       class_weight = pesos,\n",
    "                       batch_size = 128,\n",
    "                       epochs = 100,\n",
    "                       shuffle = True,\n",
    "                       validation_data = (array_test_images, Y_test_onehot))\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff2b6f-08b6-4a3e-b286-a13c106ce651",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0147af-5a2a-494f-a286-39476aa5c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "MODEL_FILE_NAME = f\"leeds_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\"\n",
    "\n",
    "tasp_cnn.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aczocEwOkHlA",
   "metadata": {
    "id": "aczocEwOkHlA",
    "tags": []
   },
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v6rxkwFlkNz7",
   "metadata": {
    "id": "v6rxkwFlkNz7"
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = 'leeds_convolution_2d_2022-05-11-08:53:52.h5'\n",
    "# # MODEL_NAME = 'leeds_2022-04-25-08:30:33.h5'\n",
    "\n",
    "# tasp_cnn = tf.keras.models.load_model(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O548TVkJjpp5",
   "metadata": {
    "id": "O548TVkJjpp5",
    "tags": []
   },
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8NivdSBoMFZE",
   "metadata": {
    "id": "8NivdSBoMFZE"
   },
   "outputs": [],
   "source": [
    "Y_predicted = tasp_cnn.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_f1_score_history(f1_score_path = F1_SCORE_PATH,\n",
    "                      f1_score_name = F1_SCORE_NAME,\n",
    "                      history = history)\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "\n",
    "REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "plot_classification_report(path = REPORT_PATH,\n",
    "                           file_name = REPORT_NAME,\n",
    "                           y_true = Y_test,\n",
    "                           y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "CONFUSION_MATRIX_NAME = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "                      file_name = CONFUSION_MATRIX_NAME,\n",
    "                      y_true = Y_test,\n",
    "                      y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a631abf8-5cce-4190-a03e-f6bd5bcb7d12",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdccd8-304b-4dab-a573-2e6a6c44c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'auto_ml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8f9454-1b8d-47b6-b842-4c787f6b59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autokeras as ak\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# # (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# clf = ak.ImageClassifier(num_classes = 3,\n",
    "#                          loss='categorical_crossentropy',\n",
    "#                          metrics = [tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold = 0.1)],\n",
    "#                          overwrite = True,\n",
    "#                          tuner= 'bayesian',\n",
    "#                          max_trials = 20,\n",
    "#                          max_model_size = 3000000\n",
    "#                         )\n",
    "    \n",
    "# clf.fit(array_train_images,\n",
    "#         np.asarray(Y_train),\n",
    "#         epochs = 50,\n",
    "#         batch_size = 128,\n",
    "#         validation_data = (array_test_images, np.asarray(Y_test)))\n",
    "\n",
    "# best_auto_model = clf.export_model()\n",
    "# print(best_auto_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26cd39-0a6c-4cc1-96df-e032270bb193",
   "metadata": {},
   "source": [
    "### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f374090-ed3a-43fe-8e5a-5e23f6e3b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\"\n",
    "\n",
    "# best_auto_model.save(MODEL_PATH + MODEL_FILE_NAME)\n",
    "\n",
    "# # def myprint(s):\n",
    "# #     with open(f\"{MODEL_PATH}_{MODEL_FILE_NAME}_summary.txt\",'w+') as f:\n",
    "# #         print(s, file=f)\n",
    "\n",
    "        \n",
    "# from contextlib import redirect_stdout\n",
    "\n",
    "# MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}_summary.txt\"\n",
    "\n",
    "# # with open(f\"{MODEL_PATH}_{MODEL_FILE_NAME}\", 'w') as f:\n",
    "\n",
    "# #     best_auto_model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "# with open(f\"{MODEL_PATH}_{MODEL_FILE_NAME}\", 'w') as f:\n",
    "#     with redirect_stdout(f):\n",
    "#         best_auto_model.summary()\n",
    "#         f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec24e94-a977-4e6c-b5fb-7a9496787566",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed2da6-f433-4e20-b663-1abe86717d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_predicted = best_auto_model.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "# F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "# F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# plot_f1_score(f1_score_path = F1_SCORE_PATH,\n",
    "#               f1_score_name = F1_SCORE_NAME,\n",
    "#               history = history)\n",
    "\n",
    "# print(\"[INFO] evaluating network...\")\n",
    "\n",
    "# REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "# REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "# plot_classification_report(path = REPORT_PATH,\n",
    "#                            file_name = REPORT_NAME,\n",
    "#                            y_true = Y_test,\n",
    "#                            y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "# CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "# CONFUSION_MATRIX_NAME  = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "#                       file_name = CONFUSION_MATRIX_NAME,\n",
    "#                       y_true = Y_test,\n",
    "#                       y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oMQbX1j_zVOO",
   "metadata": {
    "id": "oMQbX1j_zVOO",
    "tags": []
   },
   "source": [
    "# Madrid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pG-PDMY6zdqE",
   "metadata": {
    "id": "pG-PDMY6zdqE",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Importación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlkJKFfzBd8g",
   "metadata": {
    "id": "wlkJKFfzBd8g"
   },
   "source": [
    "- [Web Dataset](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=7c2843010d9c3610VgnVCM2000001f4a900aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default)\n",
    "\n",
    "- [Web documentación](https://datos.madrid.es/FWProjects/egob/Catalogo/Seguridad/Ficheros/Estructura_DS_Accidentes_trafico_desde_2019.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ic0tgQy47zEr",
   "metadata": {
    "id": "Ic0tgQy47zEr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "city_name = 'madrid'\n",
    "\n",
    "# root_path = '/content/drive/Othercomputers/Mi portátil/Drive/Master UA/TFM/Incidentes de Trafico/Datasets/Madrid/'\n",
    "\n",
    "root_path = './Data/Madrid/'\n",
    "\n",
    "file_name_2019 = '2019_Accidentalidad.csv'\n",
    "file_name_2020 = '2020_Accidentalidad.csv'\n",
    "file_name_2021 = '2021_Accidentalidad.csv'\n",
    "file_name_2022 = '2022_Accidentalidad.csv'\n",
    "\n",
    "file_2019 = pd.read_csv(root_path + file_name_2019, sep=';')\n",
    "file_2020 = pd.read_csv(root_path + file_name_2020, sep=';')\n",
    "file_2021 = pd.read_csv(root_path + file_name_2021, sep=';')\n",
    "file_2022 = pd.read_csv(root_path + file_name_2022, sep=';')\n",
    "\n",
    "# print(len(file_2019[file_2019.cod_lesividad == 4]))\n",
    "# print(len(file_2020[file_2020.cod_lesividad == 4]))\n",
    "# print(len(file_2021[file_2021.lesividad == '4']))\n",
    "# print(len(file_2022[file_2022.lesividad == '4']))\n",
    "\n",
    "COLUMNS_TO_REMOVE = ['cod_distrito',\n",
    "                     'tipo_lesividad'\n",
    "                    ]\n",
    "\n",
    "data_frame = file_2019\n",
    "data_frame = pd.concat([data_frame, file_2020])\n",
    "\n",
    "data_frame.rename(columns={\"cod_lesividad\": \"lesividad\"}, inplace = True)\n",
    "data_frame.rename(columns={\"tipo_vehículo\": \"tipo_vehiculo\"}, inplace = True)\n",
    "data_frame = data_frame.drop(COLUMNS_TO_REMOVE, axis=1)\n",
    "\n",
    "data_frame = pd.concat([data_frame, file_2021])\n",
    "\n",
    "data_frame.dropna(subset=['lesividad'], inplace = True)\n",
    "data_frame.lesividad = data_frame.lesividad.replace(' ', 14).astype(int)\n",
    "data_frame = data_frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01143bfb-843f-4bf4-b558-3ee8c5f190bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5PmJpoCCcxMJ",
   "metadata": {
    "id": "5PmJpoCCcxMJ"
   },
   "source": [
    "### Calcular Vehículos implicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utgDSVryALnm",
   "metadata": {
    "id": "utgDSVryALnm"
   },
   "source": [
    "A partir del número de expediente (un mismo expediente en varias filas quiere decir que se trata del mismo accidente) se hace un `groupby` a partir de él. Como el atributo `positiva_alcohol` no tiene valores nulos en ninguna de las filas, hacemos un conteo a partir de él y se asigna a una nueva columna `positiva_alcohol_rename` que posteriormente será renombrada como `vehiculos_implicados`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qy9UBWGFan1j",
   "metadata": {
    "id": "qy9UBWGFan1j"
   },
   "outputs": [],
   "source": [
    "data_frame = data_frame.join(data_frame.groupby('num_expediente')['positiva_alcohol'].count(), on='num_expediente', rsuffix='_rename')\n",
    "data_frame.rename(columns={\"positiva_alcohol_rename\": \"vehiculos_implicados\"}, errors=\"raise\", inplace=True)\n",
    "data_frame = data_frame.reset_index(drop=True)\n",
    "# data_frame.localizacion.unique()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3lX-EnJ1aby",
   "metadata": {
    "id": "e3lX-EnJ1aby",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ybjvOI7x0PKz",
   "metadata": {
    "id": "ybjvOI7x0PKz",
    "tags": []
   },
   "source": [
    "### Clasificación de carreteras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c43193-d332-4c5e-a173-ce0877ad9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################### SIGUIENTE CELDA #########################\n",
    "\n",
    "# # Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# # A, A(M) y Motorway lo mismo?\n",
    "# # B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "# #               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "# #               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "# #               ¿Carretera como tal?\n",
    "\n",
    "# # C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "# #               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "\n",
    "# # Unclassified\n",
    "\n",
    "\n",
    "# regex = {}\n",
    "# regex['paseo_regex'] = 'PASEO|paseo'\n",
    "# regex['parque_regex'] = 'PARQUE|PQUE'\n",
    "# regex['ronda_regex'] = 'RONDA'\n",
    "# regex['puerta_regex'] = 'PUERTA|PTA|Puerta'\n",
    "# regex['puente_regex'] = 'PNTE|PUENTE'\n",
    "# regex['plaza_regex'] = 'PLAZA|PZA'\n",
    "# regex['camino_regex']= 'CMNO|CAMINO'\n",
    "# regex['bulevard_regex'] = 'BULE'\n",
    "# regex['travesia_regex'] = 'TRVA'\n",
    "# regex['cuesta_regex'] = 'CUSTA|CUESTA'\n",
    "# regex['rotonda_regex'] = 'GTA|gta|GLORIETA|glorieta|ROTONDA'\n",
    "# regex['aeropuerto_regex'] ='AEROPUERTO|AEROP'\n",
    "# regex['calzada_regex'] = 'CALZADA'\n",
    "# regex['poligono_regex'] ='POLIGONO'\n",
    "# regex['highway_regex'] = 'AUTOV.|autovia|A-|M-|M 30|m 30|A\\\\d|M 23|M23' # A,A(M),Motorway\n",
    "# regex['road_regex'] = 'CTRA.|CARRETERA|carretera|CRA.' # B\n",
    "# regex['avenida_regex'] = 'AVDA|AV|AVENIDA|AVDA|Avda.|avenida'\n",
    "# regex['calle_regex']  = 'CALL.|Calle|CALLE|c/|C/|C.|calle'\n",
    "\n",
    "# data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "# for index,regex_values in enumerate(regex.values()):\n",
    "#     print(regex_values)\n",
    "#     regex_indexes = data_frame[data_frame.localizacion.str.contains(regex_values,  case = True, regex=True)].index\n",
    "#     print(len(regex_indexes))\n",
    "#     data_frame.iloc[regex_indexes, data_frame.columns.get_loc('tipo_via')] = str(index)\n",
    "#     data_frame.iloc[regex_indexes, data_frame.columns.get_loc('localizacion')] = str(index)\n",
    "    \n",
    "    \n",
    "    \n",
    "# # street_indexes  = data_frame[data_frame.localizacion.str.contains('CALL.|Calle|CALLE|c/|C/|C.|calle', case = True, regex=True)].index\n",
    "# # highway_indexes = data_frame[data_frame.localizacion.str.contains(highway_regex, case = True, regex=True)].index\n",
    "# # road_indexes    = data_frame[data_frame.localizacion.str.contains(road_regex, case = True, regex=True)].index\n",
    "# # # avenue_indexes  = data_frame[data_frame.localizacion.str.contains(avenue_regex,  case = True, regex=True)].index\n",
    "# # # ride_indexes    = data_frame[data_frame.localizacion.str.contains(ride_regex, case = True, regex=True)].index\n",
    "\n",
    "# # data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "# # data_frame.iloc[street_indexes,  data_frame.columns.get_loc('tipo_via')] = 'Unclassified'\n",
    "# # data_frame.iloc[highway_indexes, data_frame.columns.get_loc('tipo_via')] = 'A'\n",
    "# # data_frame.iloc[road_indexes, data_frame.columns.get_loc('tipo_via')] = 'B'\n",
    "# # # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "# # # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "\n",
    "\n",
    "# # data_frame.iloc[highway_indexes, data_frame.columns.get_loc('localizacion')] = 1\n",
    "# # data_frame.iloc[road_indexes, data_frame.columns.get_loc('localizacion')] = 2\n",
    "# # data_frame.iloc[street_indexes,  data_frame.columns.get_loc('localizacion')] = 3\n",
    "# # # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('localizacion')] = '3'\n",
    "# # # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('localizacion')] = '5'\n",
    "\n",
    "# positive_drug_indexes = data_frame[data_frame.positiva_droga == 1].index\n",
    "# data_frame.iloc[positive_drug_indexes, data_frame.columns.get_loc('positiva_alcohol')] = 'S'\n",
    "\n",
    "# data_frame = data_frame[~(data_frame.tipo_via == 'N/A')]\n",
    "# # # print(data_frame.localizacion.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da252aZ0N3n",
   "metadata": {
    "id": "7da252aZ0N3n"
   },
   "outputs": [],
   "source": [
    "# ######################### SIGUIENTE CELDA #########################\n",
    "\n",
    "# # Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# # A, A(M) y Motorway lo mismo?\n",
    "# # B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "# #               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "# #               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "# #               ¿Carretera como tal?\n",
    "\n",
    "# # C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "# #               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "\n",
    "# # Unclassified\n",
    "# street_regex  = ('CALL.|Calle|CALLE|c/|C/|C.|calle|'\n",
    "#                  'AVDA|AV|AVENIDA|AVDA|avenida|Avda.|'\n",
    "#                  'PASEO|paseo|'\n",
    "#                  'PARQUE|PQUE|'\n",
    "#                  'RONDA|'\n",
    "#                  'PUERTA|PTA|Puerta|'\n",
    "#                  'PNTE|PUENTE|'\n",
    "#                  'PLAZA|PZA|'\n",
    "#                  'CMNO|CAMINO|'\n",
    "#                  'BULE|'\n",
    "#                  'TRVA|'\n",
    "#                  'CUSTA|CUESTA|'\n",
    "#                  'GTA|gta|GLORIETA|glorieta|ROTONDA|'\n",
    "#                  'AEROPUERTO|AEROP'\n",
    "# )\n",
    "\n",
    "# highway_regex = 'AUTOV.|autovia|A-|M-|M 30|m 30|A\\\\d|M 23|M23' # A,A(M),Motorway\n",
    "# road_regex = 'CTRA.|CARRETERA|carretera|CRA.|CALZADA|POLIGONO' # B\n",
    "\n",
    "# street_indexes  = data_frame[data_frame.localizacion.str.contains(street_regex,  case = True, regex=True)].index\n",
    "# highway_indexes = data_frame[data_frame.localizacion.str.contains(highway_regex, case = True, regex=True)].index\n",
    "# road_indexes    = data_frame[data_frame.localizacion.str.contains(road_regex, case = True, regex=True)].index\n",
    "# # avenue_indexes  = data_frame[data_frame.localizacion.str.contains(avenue_regex,  case = True, regex=True)].index\n",
    "# # ride_indexes    = data_frame[data_frame.localizacion.str.contains(ride_regex, case = True, regex=True)].index\n",
    "\n",
    "# data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('tipo_via')] = 'Unclassified'\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('tipo_via')] = 'A'\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('tipo_via')] = 'B'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "\n",
    "\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('localizacion')] = 1\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('localizacion')] = 2\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('localizacion')] = 3\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('localizacion')] = '3'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('localizacion')] = '5'\n",
    "\n",
    "# # positive_drug_indexes = data_frame[data_frame.positiva_droga == 1].index\n",
    "# # data_frame.iloc[positive_drug_indexes, data_frame.columns.get_loc('positiva_alcohol')] = 'S'\n",
    "\n",
    "# data_frame = data_frame[~(data_frame.tipo_via == 'N/A')]\n",
    "# # print(data_frame.localizacion.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc350245-b09e-4de1-9ecb-3d2f8d60ba7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Consideraciones:\n",
    "\n",
    "- Los patinetes se han considerado como ciclomotres de menos de 50cc.\n",
    "- Las furgonetas se consideran como vehículos de menos de 3.5 toneladas.\n",
    "- Maquinaria de obras se considera la misma tipología que maquinaria agrícola.\n",
    "- Cuadriciclos ligeros y no ligeros se consideran como `Motorcycle-Unknown CC`.\n",
    "- Patinetes y Vehículos de Mobilidad Urbana se consideran como `Mobility Scooters`.\n",
    "- `Vehículo articulado` se considera como un vehículo de más de 7.5 toneladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RwdUhUHc1Up4",
   "metadata": {
    "id": "RwdUhUHc1Up4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_conditions_replace = {\n",
    "    'Despejado': 1,\n",
    "    'Nublado': 2,\n",
    "    'Lluvia débil': 3,\n",
    "    'LLuvia intensa': 4,\n",
    "    'Granizando':  5,\n",
    "    'Nevando': 6,\n",
    "    'Se desconoce': 7 \n",
    "}\n",
    "\n",
    "## CUIDADO CON Motocicleta hasta 125cc!!! HEMOS SUPUESTO QUE LOS CICLOMOTORES SON HASTA 50CC!!\n",
    "type_of_vehicle_replace = {\n",
    "    'Bicicleta': 1,\n",
    "    'Ciclo': 1,\n",
    "    'Bicicleta EPAC (pedaleo asistido)': 1,\n",
    "    'Ciclomotor': 2,\n",
    "    'Ciclomotor de dos ruedas L1e-B': 2,\n",
    "    'Ciclomotor de tres ruedas': 2,\n",
    "    'Motocicleta hasta 125cc': 3,\n",
    "    'Moto de tres ruedas hasta 125cc': 3,\n",
    "    'Motocicleta > 125cc': 4,\n",
    "    'Moto de tres ruedas > 125cc': 4,\n",
    "    'Turismo': 5,\n",
    "    'Todo terreno': 5,\n",
    "    'Microbús <= 17 plazas': 5,\n",
    "    'Autobús': 6,\n",
    "    'Autobus EMT': 6,\n",
    "    'Autobús articulado': 6,\n",
    "    'Autobús articulado EMT': 6,\n",
    "    'Maquinaria agrícola': 7,\n",
    "    'Maquinaria de obras': 8,\n",
    "    'Furgoneta': 9,        # Menos de 3.5 toneladas.\n",
    "    'Ambulancia SAMUR': 10,\n",
    "    'Autocaravana': 11,     # Entre 3.5 y 7.5 toneladas.\n",
    "    'Camión rígido': 12,    # Mayor que 7.5 toneladas.\n",
    "    'Tractocamión': 12,\n",
    "    'Vehículo articulado': 12,\n",
    "    'Camión de bomberos': 12,\n",
    "    'VMU eléctrico': 13,\n",
    "    'Patinete': 13,\n",
    "    'Sin especificar': 14,\n",
    "    'Otros vehículos sin motor': 14,\n",
    "    'Remolque': 14,\n",
    "    'Semiremolque': 14,\n",
    "    'Otros vehículos con motor': 15,\n",
    "    'Cuadriciclo ligero': 15,\n",
    "    'Cuadriciclo no ligero': 15,\n",
    "    'Motorcycle - Unknown CC': 15\n",
    "}\n",
    "\n",
    "# type_of_vehicle_replace = {}\n",
    "# for index,tipo_vehiculo in enumerate(data_frame.tipo_vehiculo.unique()):\n",
    "#     if not pd.isna(tipo_vehiculo): type_of_vehicle_replace[tipo_vehiculo] = index\n",
    "\n",
    "casualty_class_replace = {\n",
    "    'Conductor': 1,\n",
    "    'Pasajero': 2,\n",
    "    'Peatón': 3\n",
    "}\n",
    "\n",
    "### CUIDADO CON DESCONOCIDO!!! MEJOR HACER IMPUTACIÓN PARA RELLENENAR LOS DESCONOCIDOS?\n",
    "sex_of_casualty_replace = {\n",
    "    'Hombre': 1,\n",
    "    'Mujer': 2,\n",
    "    'Desconocido': 3\n",
    "}\n",
    "\n",
    "accident_type_replace = {\n",
    "    'Colisión fronto-lateral': 1,\n",
    "    'Alcance': 2,\n",
    "    'Colisión lateral': 3,\n",
    "    'Choque contra obstáculo fijo': 4,\n",
    "    'Colisión múltiple': 5,\n",
    "    'Caída': 5,\n",
    "    'Atropello a persona': 6,\n",
    "    'Colisión frontal': 7,\n",
    "    'Otro': 8,\n",
    "    'Solo salida de la vía': 9,\n",
    "    'Vuelco': 10,\n",
    "    'Atropello a animal': 11,\n",
    "    'Despeñamiento': 12\n",
    "}\n",
    "\n",
    "alcohol_replace = {\n",
    "    'S': 1,\n",
    "    'N': 2,\n",
    "}\n",
    "\n",
    "accident_class_replace = {\n",
    "    1:  'Slight',  # Atención en urgencias sin posterior ingreso. - LEVE\n",
    "    2:  'Slight',  # Ingreso inferior o igual a 24 horas - LEVE\n",
    "    5:  'Slight',  # Asistencia sanitaria ambulatoria con posterioridad - LEVE\n",
    "    6:  'Slight',  # Asistencia sanitaria inmediata en centro de salud o mutua - LEVE\n",
    "    7:  'Slight',  # Asistencia sanitaria sólo en el lugar del accidente - LEVE\n",
    "    14: 'Slight',  # Sin asistencia sanitaria - LEVE O NADA\n",
    "    3:  'Serious', # Ingreso superior a 24 horas. - GRAVE\n",
    "    4:  'Fatal'    # Fallecido 24 horas - FALLECIDO \n",
    "}\n",
    "###################### REEMPLAZOS ######################\n",
    "\n",
    "# ### OJO QUE ESTAMOS REPLICANDO LA ESTRUCTURA DEL DATASET DE LEEDS\n",
    "age_replace = {\n",
    "    'Menor de 5 años': 1,\n",
    "    'De 6 a 9 años': 1,\n",
    "    'De 6  a  9 años': 1,\n",
    "    'De 10 a 14 años': 1,\n",
    "    'De 15 a 17 años': 1,\n",
    "    'De 18 a 20 años': 2,\n",
    "    'De 21 a 24 años': 2,\n",
    "    'De 25 a 29 años': 3,\n",
    "    'De 30 a 34 años': 3,\n",
    "    'De 35 a 39 años': 3,\n",
    "    'De 40 a 44 años': 3,\n",
    "    'De 45 a 49 años': 3,\n",
    "    'De 50 a 54 años': 3,\n",
    "    'De 55 a 59 años': 3,\n",
    "    'De 60 a 64 años': 3,\n",
    "    'De 65 a 69 años': 4,\n",
    "    'De 70 a 74 años': 4,\n",
    "    'Más de 74 años': 4,\n",
    "    'Desconocido': 5,\n",
    "}\n",
    "\n",
    "# age_replace = {\n",
    "#     'Menor de 5 años': 1,\n",
    "#     'De 6 a 9 años': 2,\n",
    "#     'De 6  a  9 años': 3,\n",
    "#     'De 10 a 14 años': 4,\n",
    "#     'De 15 a 17 años': 5,\n",
    "#     'De 18 a 20 años': 6,\n",
    "#     'De 21 a 24 años': 7,\n",
    "#     'De 25 a 29 años': 8,\n",
    "#     'De 30 a 34 años': 9,\n",
    "#     'De 35 a 39 años': 10,\n",
    "#     'De 40 a 44 años': 11,\n",
    "#     'De 45 a 49 años': 12,\n",
    "#     'De 50 a 54 años': 13,\n",
    "#     'De 55 a 59 años': 14,\n",
    "#     'De 60 a 64 años': 15,\n",
    "#     'De 65 a 69 años': 16,\n",
    "#     'De 70 a 74 años': 17,\n",
    "#     'Más de 74 años': 18,\n",
    "#     'Desconocido': 19,\n",
    "# }\n",
    "\n",
    "data_frame['estado_meteorológico'].replace(weather_conditions_replace, inplace = True)\n",
    "print('Estado meteorológico: \\n', data_frame['estado_meteorológico'].value_counts())\n",
    "\n",
    "data_frame['tipo_vehiculo'].replace(type_of_vehicle_replace, inplace = True)\n",
    "print('Tipo vehículo: \\n', data_frame['tipo_vehiculo'].value_counts())\n",
    "\n",
    "data_frame['tipo_persona'].replace(casualty_class_replace, inplace = True)\n",
    "print('Tipo de persona: \\n', data_frame['tipo_persona'].value_counts())\n",
    "\n",
    "data_frame['sexo'].replace(sex_of_casualty_replace, inplace = True)\n",
    "print('Sexo: \\n', data_frame['sexo'].value_counts())\n",
    "\n",
    "data_frame['positiva_alcohol'].replace(alcohol_replace, inplace = True)\n",
    "print('Positivo Alcohol: \\n', data_frame['positiva_alcohol'].value_counts())\n",
    "\n",
    "data_frame['lesividad'].replace(accident_class_replace, inplace = True)\n",
    "print('Gravedad: \\n', data_frame['lesividad'].value_counts())\n",
    "\n",
    "data_frame['rango_edad'].replace(age_replace, inplace = True)\n",
    "print('Edad: \\n', data_frame['rango_edad'].value_counts())\n",
    "\n",
    "data_frame.hora = data_frame.hora.mask(pd.to_datetime(data_frame.hora) < '06:00:00', 2)\n",
    "data_frame.hora = data_frame.hora.mask(pd.to_datetime(data_frame.hora) > '18:00:00', 2)\n",
    "data_frame.hora = data_frame.hora.mask(pd.to_datetime(data_frame.hora).between('06:00:00', '18:00:00'), 1)\n",
    "print('hora:', data_frame['hora'].value_counts())\n",
    "\n",
    "district_replace = {}\n",
    "for index,distrito in enumerate(data_frame.distrito.unique()):\n",
    "  if not pd.isna(distrito): district_replace[distrito] = int(index)\n",
    "\n",
    "accident_type_replace = {}\n",
    "for index,accident_type in enumerate(data_frame.tipo_accidente.unique()):\n",
    "    if not pd.isna(accident_type): accident_type_replace[accident_type] = int(index)\n",
    "\n",
    "data_frame['distrito'].replace(district_replace, inplace = True)\n",
    "print('Distrito: \\n', data_frame['distrito'].value_counts())\n",
    "\n",
    "data_frame['tipo_accidente'].replace(accident_type_replace, inplace = True)\n",
    "print('Tipo Accidente: \\n', data_frame['tipo_accidente'].value_counts())\n",
    "\n",
    "# Eliminamos aquellas lesividades desconocidas i.e. 77.\n",
    "data_frame = data_frame[data_frame.lesividad != 77]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pVPFGQ0AoNRD",
   "metadata": {
    "id": "pVPFGQ0AoNRD",
    "tags": []
   },
   "source": [
    "### Coordenadas UTM a números enteros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nabg28LMAHhW",
   "metadata": {
    "id": "nabg28LMAHhW"
   },
   "source": [
    "Las coordenadas UTM son coordenads que están expresadas en término de X e Y partiendo de la base de que parten desde una determinada localización. Estas coordenadas constan de una parte entera y una decimal.\n",
    "\n",
    "En este dataset el formato que presentan estas coordenadas pueden ser de tres tipos:\n",
    "\n",
    "- **XXX.XXX.XXX**: en este caso los seis primeros dígitos forman la parte entera y los tres útlimos la parte decimal.\n",
    "- **XXXXXX,XX**: los seis primeros dígitos indican la parte entera, mientras que tras la coma aparecen dos dígitos de la parte decimal que habrá que completar añadiendo uno más.\n",
    "- **XXXXXX**: indican la parte entera, sin contar con la parte decimal.\n",
    "\n",
    "Por lo que el objetivo es estandarizar todos los formatos convirtiendo cada una de las coordenadas a un número entero, siendo necesario tratar con cada una de las casuísticas para añadir ceros a la derecha en caso de que falten para que cada una de las coordenadas tenga la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sgVHBwC0Fd1N",
   "metadata": {
    "id": "sgVHBwC0Fd1N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Todos las comas a puntos\n",
    "\n",
    "import re\n",
    "\n",
    "s = data_frame.coordenada_x_utm.str\n",
    "s_y = data_frame.coordenada_y_utm.str\n",
    "\n",
    "# Regex que hace match para dos grupos, la parte entera y la parte decimal.\n",
    "group_integer_and_float_pattern = '(?P<Integer>\\d{3}\\.\\d{3})(?P<Float>\\.\\d{2,3})'\n",
    "all_float_pattern   = '(?P<Number>\\d{6},\\d+)'\n",
    "all_integer_pattern = '(?P<Number>\\d{6}$)'\n",
    "\n",
    "group_integer_and_float_pattern_y = '(?P<Integer>\\d\\.\\d{3}\\.\\d{3})(?P<Float>\\.\\d{2,3})'\n",
    "all_float_pattern_y   = '(?P<Number>\\d{7},\\d+)'\n",
    "all_integer_pattern_y = '(?P<Number>\\d{7}$)'\n",
    "\n",
    "# Se extraen en un dataframe independiente ambas partes, la entera y la decimal\n",
    "index_and_extracted_x1 = s.extract(group_integer_and_float_pattern)\n",
    "index_and_extracted_x2 = s.extract(all_float_pattern)\n",
    "index_and_extracted_x3 = s.extract(all_integer_pattern)\n",
    "\n",
    "index_and_extracted_y1 = s_y.extract(group_integer_and_float_pattern_y)\n",
    "index_and_extracted_y2 = s_y.extract(all_float_pattern_y)\n",
    "index_and_extracted_y3 = s_y.extract(all_integer_pattern_y)\n",
    "\n",
    "# Se seleccionan aquellas que no continenen valores nulos el Float.\n",
    "# Es decir, aquellos con los que el match ha tenido éxito (los que llevan punto)\n",
    "# en lugar de comas.\n",
    "selected_rows_x1 = index_and_extracted_x1[~index_and_extracted_x1['Float'].isnull()]\n",
    "selected_rows_x2 = index_and_extracted_x2[~index_and_extracted_x2['Number'].isnull()]\n",
    "selected_rows_x3 = index_and_extracted_x3[~index_and_extracted_x3['Number'].isnull()]\n",
    "\n",
    "selected_rows_y1 = index_and_extracted_y1[~index_and_extracted_y1['Float'].isnull()]\n",
    "selected_rows_y2 = index_and_extracted_y2[~index_and_extracted_y2['Number'].isnull()]\n",
    "selected_rows_y3 = index_and_extracted_y3[~index_and_extracted_y3['Number'].isnull()]\n",
    "\n",
    "# Se cambia el string de la parte entera a un string sin puntos.\n",
    "selected_rows_x1.Integer = selected_rows_x1.Integer.str.replace('.','')\n",
    "selected_rows_x2.Number  = selected_rows_x2.Number.str.replace(',','.')\n",
    "\n",
    "selected_rows_y1.Integer = selected_rows_y1.Integer.str.replace('.','')\n",
    "selected_rows_y2.Number  = selected_rows_y2.Number.str.replace(',','.')\n",
    "\n",
    "# Se crea una nueva columna en el nuevo dataframe con la unión de la parte\n",
    "# entera y la parte decimal.\n",
    "selected_rows_x1['processed_x_utm'] = selected_rows_x1.Integer + selected_rows_x1.Float\n",
    "selected_rows_x2['processed_x_utm'] = selected_rows_x2.Number\n",
    "selected_rows_x3['processed_x_utm'] = selected_rows_x3.Number\n",
    "\n",
    "selected_rows_y1['processed_y_utm'] = selected_rows_y1.Integer + selected_rows_y1.Float\n",
    "selected_rows_y2['processed_y_utm'] = selected_rows_y2.Number\n",
    "selected_rows_y3['processed_y_utm'] = selected_rows_y3.Number\n",
    "\n",
    "data_frame['processed_x_utm'] = 'N/A'\n",
    "data_frame['processed_y_utm'] = 'N/A'\n",
    "\n",
    "# Si la longitud de alguno de los números es menor a diez, hay que añadirle x 0s\n",
    "# de diferencia\n",
    "selected_rows_x2.processed_x_utm = selected_rows_x2.processed_x_utm.transform(lambda x: x + '0'*(10-len(x)))\n",
    "selected_rows_x3.processed_x_utm = selected_rows_x3.processed_x_utm.transform(lambda x: x + '.000')\n",
    "\n",
    "selected_rows_y2.processed_y_utm = selected_rows_y2.processed_y_utm.transform(lambda x: x + '0'*(11-len(x)))\n",
    "selected_rows_y3.processed_y_utm = selected_rows_y3.processed_y_utm.transform(lambda x: x + '.000')\n",
    "\n",
    "data_frame['processed_x_utm'][selected_rows_x1.index] = selected_rows_x1['processed_x_utm']\n",
    "data_frame['processed_x_utm'][selected_rows_x2.index] = selected_rows_x2['processed_x_utm']\n",
    "data_frame['processed_x_utm'][selected_rows_x3.index] = selected_rows_x3['processed_x_utm']\n",
    "\n",
    "data_frame['processed_y_utm'][selected_rows_y1.index] = selected_rows_y1['processed_y_utm']\n",
    "data_frame['processed_y_utm'][selected_rows_y2.index] = selected_rows_y2['processed_y_utm']\n",
    "data_frame['processed_y_utm'][selected_rows_y3.index] = selected_rows_y3['processed_y_utm']\n",
    "\n",
    "# Eliminamos aquellas filas que no tienen coordenadas\n",
    "data_frame = data_frame[data_frame['coordenada_y_utm'] != '0.000']\n",
    "\n",
    "# Eliminamos el punto de la parte decimal para convertirlo a entero\n",
    "data_frame.processed_x_utm = data_frame.processed_x_utm.str.replace('.','')\n",
    "data_frame.processed_y_utm = data_frame.processed_y_utm.str.replace('.','')\n",
    "\n",
    "# Lo convertimos en entero\n",
    "data_frame.processed_x_utm = data_frame.processed_x_utm.astype(int)\n",
    "data_frame.processed_y_utm = data_frame.processed_y_utm.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Z4nz3ioxtXb",
   "metadata": {
    "id": "_Z4nz3ioxtXb",
    "tags": []
   },
   "source": [
    "### Renombrado y eliminación de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqnlSOcN71Ah",
   "metadata": {
    "id": "tqnlSOcN71Ah"
   },
   "outputs": [],
   "source": [
    "# COLUMNS_TO_REMOVE = ['num_expediente', 'fecha', 'tipo_via', 'numero', 'positiva_droga', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_droga']\n",
    "COLUMNS_TO_REMOVE = ['num_expediente', 'fecha', 'tipo_via', 'localizacion', 'numero', 'positiva_droga', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_droga']\n",
    "\n",
    "data_frame = data_frame.loc[:, ~data_frame.columns.isin(COLUMNS_TO_REMOVE)]\n",
    "\n",
    "# data_frame.rename(columns={\"localizacion\": \"tipo_carretera\"}, errors=\"raise\", inplace=True)\n",
    "data_frame.rename(columns={\"processed_x_utm\": \"coordenada_x_utm\"}, errors=\"raise\", inplace=True)\n",
    "data_frame.rename(columns={\"processed_y_utm\": \"coordenada_y_utm\"}, errors=\"raise\", inplace=True)\n",
    "data_frame.rename(columns={\"positiva_alcohol\": \"drogas_alcohol_positivo\"}, errors=\"raise\", inplace=True)\n",
    "\n",
    "data_frame = data_frame.drop_duplicates()\n",
    "data_frame = data_frame.dropna()\n",
    "data_frame = data_frame.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c4311-643e-4627-8a70-a5d74c4332ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4a2e2-cefb-4732-b3a5-ebbd48963fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.histplot(data=data_frame, x=\"lesividad\", stat='count')\n",
    "plt.savefig('saving-a-seaborn-plot-as-eps-file.svg')\n",
    "data_frame.lesividad.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae87450-809a-4958-b9cb-422a3e91effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data_frame = data_frame.loc[:, ~data_frame.columns.isin(['lesividad'])]\n",
    "# Y_data_frame = data_frame['lesividad']\n",
    "\n",
    "# X_data_frame  = X_data_frame.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qKuGQ1I8078E",
   "metadata": {
    "id": "qKuGQ1I8078E",
    "tags": []
   },
   "source": [
    "## Split de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583a4fb-5fcd-4a15-a78d-6f752f927b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.histplot(data=data_frame, x=\"lesividad\", stat='count')\n",
    "# plt.savefig('saving-a-seaborn-plot-as-eps-file.svg')\n",
    "data_frame.lesividad.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCcJF3i8s3dD",
   "metadata": {
    "id": "NCcJF3i8s3dD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data_frame, test_size=0.2)\n",
    "X_train = X_train_original = train.loc[:, ~train.columns.isin(['lesividad'])]\n",
    "\n",
    "X_train = X_train.astype(int)\n",
    "X_train_original = X_train_original.astype(int)\n",
    "\n",
    "Y_train = Y_train_original = train['lesividad']\n",
    "\n",
    "X_test = test.loc[:, ~test.columns.isin(['lesividad'])]\n",
    "X_test = X_test.astype(int)\n",
    "Y_test = test['lesividad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x2PcFjlBmTlC",
   "metadata": {
    "id": "x2PcFjlBmTlC"
   },
   "outputs": [],
   "source": [
    "# # FILE_NAME = f\"{city_name}_calculated_weights.json\"\n",
    "# FILE_NAME = 'madrid_adapted_leeds_default_weights.json'\n",
    "\n",
    "# feature_vector = load_json(WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde99d2d-727a-4e30-90b3-67dc859075bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature_vector = {}\n",
    "\n",
    "# feature_vector['Accident Features'] = {\n",
    "#     'feature_childs': ['coordenada_x_utm', 'coordenada_y_utm', 'distrito', 'hora', 'vehiculos_implicados'],\n",
    "#     'feature_weights': [0.165774538, 0.171530785, 0.082228259, 0.047771472, 0.060763375]\n",
    "# } \n",
    "\n",
    "# feature_vector['Roadway Features'] = {\n",
    "#     'feature_childs': ['tipo_accidente'], # Road Surface \n",
    "#     'feature_weights': [0.07036541]\n",
    "# }\n",
    "\n",
    "# feature_vector['Environmental Features'] = {\n",
    "#     'feature_childs': ['estado_meteorológico'],\n",
    "#     'feature_weights': [0.04354843]\n",
    "# }\n",
    "\n",
    "# feature_vector['Vehicle Features'] = {\n",
    "#     'feature_childs': ['tipo_vehiculo'],\n",
    "#     'feature_weights': [0.126314657]\n",
    "# }\n",
    "\n",
    "# feature_vector['Casualty Features'] = {\n",
    "#     'feature_childs': ['tipo_persona', 'sexo', 'rango_edad', 'drogas_alcohol_positivo'],\n",
    "#     'feature_weights': [0.067057589, 0.049116389, 0.095220163, 0.059951354]\n",
    "# } \n",
    "# matrix_indexes = fv2gi(feature_vector)\n",
    "\n",
    "# # {'Accident Features': {'feature_childs': ['Easting',\n",
    "# #    'Northing',\n",
    "# #    '1st Road Class',\n",
    "# #    'Accident Time',\n",
    "# #    'Number of Vehicles'],\n",
    "# #   'feature_weights': [0.165774538,\n",
    "# #    0.171530785,\n",
    "# #    0.082228259,\n",
    "# #    0.047771472,\n",
    "# #    0.060763375],\n",
    "# #   'wpi': 0.528068429},\n",
    "# #  'Roadway Features': {'feature_childs': ['Road Surface'],\n",
    "# #   'feature_weights': [0.048847406],\n",
    "# #   'wpi': 0.048847406},\n",
    "# #  'Environmental Features': {'feature_childs': ['Lighting Conditions',\n",
    "# #    'Weather Conditions'],\n",
    "# #   'feature_weights': [0.041826936, 0.04354843],\n",
    "# #   'wpi': 0.08537536600000001},\n",
    "# #  'Vehicle Features': {'feature_childs': ['Type of Vehicle'],\n",
    "# #   'feature_weights': [0.126314657],\n",
    "# #   'wpi': 0.126314657},\n",
    "# #  'Casualty Features': {'feature_childs': ['Casualty Class',\n",
    "# #    'Sex of Casualty',\n",
    "# #    'Age of Casualty'],\n",
    "# #   'feature_weights': [0.067057589, 0.049116389, 0.095220163],\n",
    "# #   'wpi': 0.211394141}}\n",
    "# feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5W4MXdIis6vn",
   "metadata": {
    "id": "5W4MXdIis6vn",
    "tags": []
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tHUfNlw8sdxS",
   "metadata": {
    "id": "tHUfNlw8sdxS"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test  = X_test.astype(int)\n",
    "\n",
    "X_train_original = X_train_original.astype(int)\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_train_original = normalize_data(X_train_original)\n",
    "X_test  = normalize_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kr_UChBJ21Cu",
   "metadata": {
    "id": "kr_UChBJ21Cu",
    "tags": []
   },
   "source": [
    "## Oversampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rXwHLi842zLs",
   "metadata": {
    "id": "rXwHLi842zLs"
   },
   "outputs": [],
   "source": [
    "print('********** Before OverSampling **********')\n",
    "print('Slight: ', (Y_train == 'Slight').sum())\n",
    "print('Serious:', (Y_train == 'Serious').sum())\n",
    "print('Fatal:  ', (Y_train == 'Fatal').sum())\n",
    "print('\\n Total X:', len(X_train), ' Total Y:', len(Y_train), '\\n')\n",
    "\n",
    "X_train, Y_train = oversample_data(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06540ce8-f30e-45fb-b44f-55e1632951d2",
   "metadata": {},
   "source": [
    "## Downsampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a11f0e-a930-4fe5-9c60-a12973ac9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "slight_data  = test[test['lesividad'] == 'Slight']\n",
    "serious_data = test[test['lesividad'] == 'Serious']\n",
    "fatal_data   = test[test['lesividad'] == 'Fatal']\n",
    "\n",
    "X_slight_downsampled = resample(slight_data,\n",
    "                                replace = True,\n",
    "                                n_samples = len(fatal_data))\n",
    "\n",
    "X_serious_downsampled = resample(serious_data,\n",
    "                                 replace = True,\n",
    "                                 n_samples = len(fatal_data))\n",
    "\n",
    "\n",
    "downsampled_dataset = pd.concat([X_slight_downsampled, X_serious_downsampled, fatal_data])\n",
    "\n",
    "downsampled_train, downsampled_test = train_test_split(downsampled_dataset, test_size=0.3)\n",
    "\n",
    "X_train_downsampled = downsampled_train.loc[:, ~downsampled_train.columns.isin(['lesividad'])]\n",
    "Y_train_downsampled = downsampled_train['lesividad']\n",
    "\n",
    "X_test_downsampled = downsampled_test.loc[:, ~downsampled_test.columns.isin(['lesividad'])]\n",
    "Y_test_downsampled = downsampled_test['lesividad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4e4df-7c2d-48fb-b867-0a7066a57076",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test  = X_test.astype(int)\n",
    "\n",
    "X_train_original = X_train_original.astype(int)\n",
    "\n",
    "X_train_downsampled = X_train_downsampled.astype(int)\n",
    "X_test_downsampled  = X_test_downsampled.astype(int)\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_test  = normalize_data(X_test)\n",
    "X_train_downsampled = normalize_data(X_train_downsampled)\n",
    "X_test_downsampled  = normalize_data(X_test_downsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eb99c-7ac9-425b-af2e-735ae2155e03",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525349f-2832-457b-833f-e4f2d549ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccda69-9c08-4e7c-b1fb-20ce276379ae",
   "metadata": {},
   "source": [
    "### Genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e41610-872e-4fdd-ab9b-2d7fdc739d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMS_TO_OPTIMIZE = {'eta': {'type': 'float',\n",
    "#                                    'init': [0.01, 1],\n",
    "#                                    'mutation': [-0.3, 0.3],\n",
    "#                                    'round': 2\n",
    "#                                    },\n",
    "#                            'max_depth': {'type': 'int',\n",
    "#                                          'init': [1, 20],\n",
    "#                                          'mutation': [-4, 4],\n",
    "#                                          'step': 1\n",
    "#                                    },\n",
    "#                            'min_child_weight': {'type': 'float',\n",
    "#                                                 'init': [0.1, 15.0],\n",
    "#                                                 'mutation': [-5, 5],\n",
    "#                                                 'round': 1\n",
    "#                                    },\n",
    "#                            'n_estimators': {'type': 'int',\n",
    "#                                             'init': [0, 2000],\n",
    "#                                             'mutation': [-200, 200],\n",
    "#                                             'step': 150\n",
    "#                                    },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8a6fb-aef6-4186-aae1-89981e83bd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "# import random\n",
    "\n",
    "# Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "# Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "# Y_train_downsampled_onehot = casualty_to_one_hot(Y_train_downsampled)\n",
    "# Y_test_downsampled_onehot  = casualty_to_one_hot(Y_test_downsampled)\n",
    "\n",
    "\n",
    "# # number_of_individuals = 60\n",
    "# # numberOfParentsMating = 15\n",
    "# # number_of_hyperparams = len(HYPERPARAMS_TO_OPTIMIZE)\n",
    "# # number_of_generations = 100\n",
    "\n",
    "# populationSize = (number_of_individuals, number_of_hyperparams)\n",
    "# population = initilialize_population(number_of_individuals   = number_of_individuals,\n",
    "#                                      hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "\n",
    "# fitnessHistory = np.empty([number_of_generations+1, number_of_individuals]) # Define an array to store the value of each parameter for each parent and generation\n",
    "# populationHistory = np.empty([(number_of_generations+1)*number_of_individuals, number_of_hyperparams]) # Insert the value of initial parameters in history\n",
    "\n",
    "# best_solution_history = np.empty([(number_of_generations), number_of_hyperparams+1])\n",
    "# populationHistory[0:number_of_individuals,:] = population\n",
    "\n",
    "\n",
    "# xgbDMatrixTrain = xgb.DMatrix(data  = X_train_downsampled,\n",
    "#                               label = Y_train_downsampled)\n",
    "\n",
    "# xgbDMatrixTest  = xgb.DMatrix(data  = X_test_downsampled, \n",
    "#                               label = Y_test_downsampled)\n",
    "\n",
    "# for generation in range(number_of_generations):\n",
    "\n",
    "#     print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "#     new_population = []\n",
    "    \n",
    "#     unique_individuals = np.unique(population, axis=0)\n",
    "    \n",
    "#     new_individuals_to_create = number_of_individuals - len(unique_individuals)\n",
    "    \n",
    "#     for i in range(new_individuals_to_create):\n",
    "#         new_individual = generate_individual(hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "#         new_population.append(new_individual)\n",
    "    \n",
    "#     new_population = np.array(new_population)\n",
    "\n",
    "#     if (new_individuals_to_create):\n",
    "#         population = np.concatenate((unique_individuals, new_population), axis=0)\n",
    "\n",
    "#     # print(f'Current population is {population}')\n",
    "#     print(f'New population is {len(new_population)}')\n",
    "    \n",
    "#     # Train the dataset and obtain fitness\n",
    "#     fitnessValue = train_population(population = population,\n",
    "#                                     hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE,\n",
    "#                                     dMatrixTrain = xgbDMatrixTrain,\n",
    "#                                     dMatrixTest = xgbDMatrixTest,\n",
    "#                                     Y_test = Y_test_downsampled)\n",
    " \n",
    "#     fitnessHistory[generation,:] = fitnessValue\n",
    "\n",
    "#     # Best score in the current iteration\n",
    "#     max_score_index = np.argmax(fitnessHistory[generation,:])\n",
    "#     max_score_value = np.max(fitnessHistory[generation,:])\n",
    "#     max_score_solution = population[max_score_index]\n",
    "\n",
    "#     max_solution_with_score = []\n",
    "#     max_solution_with_score = np.append(max_score_solution, max_score_value)\n",
    "#     best_solution_history[generation] = max_solution_with_score\n",
    "\n",
    "#     print(f\"Best F1 score in the this iteration = {max_score_value}, best solution {max_score_solution}\") # Survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n",
    "    \n",
    "#     parents = new_parents_selection(population = population,\n",
    "#                                     fitness = fitnessValue,\n",
    "#                                     numParents = numberOfParentsMating)\n",
    "    \n",
    "#     # Mate these parents to create children having parameters from these parents (we are using uniform crossover)\n",
    "#     children = crossover_uniform(parents = parents,\n",
    "#                                  childrenSize = (populationSize[0] - parents.shape[0], number_of_hyperparams))\n",
    "    \n",
    "#     # Add mutation to create genetic diversity\n",
    "#     children_mutated = mutation(children,\n",
    "#                                 hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "    \n",
    "#     '''\n",
    "#     We will create new population, which will contain parents that where selected previously based on the\n",
    "#     fitness score and rest of them  will be children\n",
    "#     '''\n",
    "#     population[0:parents.shape[0], :] = parents # Fittest parents\n",
    "#     population[parents.shape[0]:, :]  = children_mutated # Children\n",
    "    \n",
    "#     populationHistory[(generation+1)*number_of_individuals : (generation+1)*number_of_individuals + number_of_individuals , :] = population # Store parent information\n",
    "    \n",
    "# #Best solution from the final iteration\n",
    "\n",
    "# fitness = train_population(population = population,\n",
    "#                            hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE,\n",
    "#                            dMatrixTrain = xgbDMatrixTrain,\n",
    "#                            dMatrixTest = xgbDMatrixTest,\n",
    "#                            Y_test = Y_test_downsampled)\n",
    "\n",
    "# fitnessHistory[generation+1, :] = fitness # index of the best solution\n",
    "# bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "\n",
    "\n",
    "# best_hyperparams = {}\n",
    "# for n_param, hyperparam in enumerate(HYPERPARAMS_TO_OPTIMIZE):\n",
    "#     best_hyperparams[hyperparam] = population[bestFitnessIndex][n_param]\n",
    "\n",
    "# #### PLOT FITNESS EVOLUTION ####\n",
    "# x_fitness = [np.max(fitnessHistory[i]) for i in range(0,fitnessHistory.shape[0])]\n",
    "\n",
    "# FILE_NAME = 'madrid_ga_' + MODEL_TIMESTAMP  + '.jpg'\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(np.arange(len(x_fitness)), x_fitness)\n",
    "# plt.savefig(GA_SCORES_PATH + FILE_NAME)\n",
    "\n",
    "# #### PLOT HYPERPARAMS EVOLUTION ####\n",
    "# FILE_NAME = f\"madrid_ga_hyperparams_evolution_p{number_of_individuals}_c{numberOfParentsMating}_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# LEGEND_LABELS = HYPERPARAMS_TO_OPTIMIZE.keys()\n",
    "\n",
    "# plt.figure(figsize=(15, 8))\n",
    "# plt.plot(best_solution_history)\n",
    "# plt.legend(LEGEND_LABELS)\n",
    "# plt.savefig(HYPERPARAMS_EVOLUTON_PATH + FILE_NAME, dpi=300)\n",
    "\n",
    "# FILE_NAME = f\"madrid_population_p{number_of_individuals}_c{numberOfParentsMating}_{MODEL_TIMESTAMP}.txt\"\n",
    "\n",
    "# np.savetxt(FINAL_POPULATION_PATH + FILE_NAME, population, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48115b98-c658-4031-bdb1-a0024703952a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ea677-b0e1-43eb-9964-37dcb5c0241f",
   "metadata": {},
   "source": [
    "- [Bayesian Optimization with HYPEROPT](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40166b8f-b982-4cb1-bc9f-989b297970ad",
   "metadata": {},
   "source": [
    "#### Carga hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55470307-b090-4ba0-a615-b200b386b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'madrid_hyperparams2022-04-28-19:07:55.json'\n",
    "\n",
    "best_hyperparams = load_json(HYPERPARAMS_PATH, FILE_NAME)\n",
    "\n",
    "# 0.875 GA\n",
    "# 0.04, 1, 3.9, 900\n",
    "# best_hyperparams = {}\n",
    "# best_hyperparams['eta'] = 0.04\n",
    "# best_hyperparams['max_depth'] = 1\n",
    "# best_hyperparams['min_child_weight'] = 3.9\n",
    "# best_hyperparams['n_estimators'] = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1453d7-ca0f-481e-a7a3-7a157dfa6ae3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Cálculo de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74624b8-92fb-4dd8-b163-6f2489bee52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "# Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "# space={'max_depth': hp.quniform(\"max_depth\", 3, 20, 1),\n",
    "#         'gamma': hp.uniform ('gamma', 1,8),\n",
    "#         'reg_alpha' : hp.quniform('reg_alpha', 40, 150, 1),\n",
    "#         'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "#         'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "#         'min_child_weight' : hp.quniform('min_child_weight', 0, 15, 1),\n",
    "#         'n_estimators': hp.quniform('n_estimators', 100, 200, 100),\n",
    "#         'tree_method': 'gpu_hist'\n",
    "#     }\n",
    "\n",
    "# def objective(space):\n",
    "#     clf = XGBClassifier(n_estimators = int(space['n_estimators']),\n",
    "#                         max_depth = int(space['max_depth']),\n",
    "#                         gamma = space['gamma'],\n",
    "#                         reg_alpha = int(space['reg_alpha']),\n",
    "#                         min_child_weight = int(space['min_child_weight']),\n",
    "#                         colsample_bytree = int(space['colsample_bytree']),\n",
    "#                         tree_method = space['tree_method']\n",
    "#                        )\n",
    "    \n",
    "#     evaluation = [(X_train, Y_train), (X_test, Y_test)]\n",
    "    \n",
    "#     clf.fit(X_train, Y_train,\n",
    "#             eval_set = evaluation, eval_metric = \"auc\",\n",
    "#             early_stopping_rounds = 10, verbose = False)\n",
    "            \n",
    "    \n",
    "#     pred = clf.predict(X_test)\n",
    "#     accuracy = accuracy_score(Y_test, pred>0.5)\n",
    "#     print (\"SCORE:\", accuracy)\n",
    "#     return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "\n",
    "\n",
    "# trials = Trials()\n",
    "\n",
    "# best_hyperparams = fmin(fn = objective,\n",
    "#                         space = space,\n",
    "#                         algo = tpe.suggest,\n",
    "#                         max_evals = 200,\n",
    "#                         trials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1213fe-2a09-4686-8834-edc9ba60d2ea",
   "metadata": {},
   "source": [
    "#### Escritura hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad8965-a095-4b09-9cc3-915c4d2c5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = 'madrid_hyperparams' + MODEL_TIMESTAMP + '.json'\n",
    "\n",
    "# write_json(best_hyperparams, HYPERPARAMS_PATH, FILE_NAME)\n",
    "# print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e290436-2027-4bac-99c1-78870fd94419",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45db8d-ff82-41ac-8ca6-6bcce2d37659",
   "metadata": {},
   "source": [
    "#### Carga definitiva/auxiliar de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49493d-494c-45d1-ad26-5b621d5c4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = 'madrid_adapted_leeds_default_weights.json'\n",
    "FILE_NAME = 'madrid_weights_no_roadClass.json'\n",
    "# FILE_NAME = 'madrid_weights2022-04-14-11:16:13.json'\n",
    "\n",
    "feature_vector = load_json(WEIGHTS_PATH, FILE_NAME)\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b64bd4-bc0c-4798-95a9-e7647b73aacd",
   "metadata": {},
   "source": [
    "#### Cálculo de pesos de caracetrísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d884f-1d98-4169-a633-74316ef68e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost = XGBClassifier(best_hyperparams,\n",
    "#                         tree_method = 'gpu_hist',\n",
    "#                         single_precision_histogram =  True)\n",
    "\n",
    "# xgboost.fit(X_train, Y_train)\n",
    "\n",
    "# child_weights  = np.array(xgboost.feature_importances_)\n",
    "# feature_vector = fill_feature_vector(X_train, child_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f6472-a98c-4801-b5cc-31d0f46c13a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualización pesos calculados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc1536-1b5a-4d6a-922d-5f27f58e7396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FILE_NAME = f\"{city_name}_figure_weights_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# print(xgboost.get_booster().get_score(importance_type= 'weight'))\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.barh(X_train.columns, xgboost.feature_importances_)\n",
    "# plt.savefig(WEIGHTS_PATH + FILE_NAME)\n",
    "\n",
    "# print(xgboost.feature_importances_)\n",
    "\n",
    "# for column, weight in zip(X_train.columns,xgboost.feature_importances_):\n",
    "#   print(column, weight)\n",
    "\n",
    "# child_weights  = np.array(xgboost.feature_importances_)\n",
    "# feature_vector = fill_feature_vector(X_train, child_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278163f-8450-412d-8147-c235f6825646",
   "metadata": {},
   "source": [
    "#### Escritura de pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e3738-44ef-45be-9b24-989e4610267e",
   "metadata": {},
   "source": [
    "- v5: Pesos calculados con hiperparámetros. En el dataset están tipificados los vehículos como en el artículo, las edades no están en rango.\n",
    "- v6: Pesos calculados con hiperparámetros. En el dataset están tipificados los vehículos como en el artículo, las edades están en rango.\n",
    "- v7: hiperparams, tipos de carretera tipificados por vía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d2414-bf56-4519-aaf2-ae6333e69b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_indexes = fv2gi(feature_vector)\n",
    "\n",
    "# FILE_NAME = f\"{city_name}_weights{MODEL_TIMESTAMP}.json\"\n",
    "# # FILE_NAME = 'default_calculated_weights.json'\n",
    "\n",
    "# write_json(feature_vector, WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc80de-39c3-4818-b7eb-8a406aeed4e2",
   "metadata": {},
   "source": [
    "### Cálculo índices de matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd560eed-aa5b-47ad-a8fd-fb142a5d123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dR3Ah2X24fbw",
   "metadata": {
    "id": "dR3Ah2X24fbw"
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cWGYQ82UI4RM",
   "metadata": {
    "id": "cWGYQ82UI4RM"
   },
   "outputs": [],
   "source": [
    "train_bgi = build_gray_images(X_train, 5, matrix_indexes)\n",
    "train_original_bgi = build_gray_images(X_train_original, 5, matrix_indexes)\n",
    "\n",
    "test_bgi  = build_gray_images(X_test, 5, matrix_indexes)\n",
    "\n",
    "pd.DataFrame(train_bgi[:,:,1057])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yQTCws554zZL",
   "metadata": {
    "id": "yQTCws554zZL"
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mRrOKk3a43ZI",
   "metadata": {
    "id": "mRrOKk3a43ZI"
   },
   "outputs": [],
   "source": [
    "train_images = shape_images(X_data = X_train,\n",
    "                            gray_images = train_bgi)\n",
    "\n",
    "train_original_images = shape_images(X_data = X_train_original,\n",
    "                                     gray_images = train_original_bgi)\n",
    "\n",
    "test_images  = shape_images(X_data = X_test,\n",
    "                            gray_images = test_bgi)\n",
    "\n",
    "plt.gray()\n",
    "for i in range(0,3):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(train_bgi[:,:,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2d341-a847-437b-a1fb-0e40e01cced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images = shape_images(X_data = X_train,\n",
    "#                             gray_images = train_bgi)\n",
    "# test_images  = shape_images(X_data = X_test,\n",
    "#                             gray_images = test_bgi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1PdwhQuQ9o_P",
   "metadata": {
    "id": "1PdwhQuQ9o_P"
   },
   "source": [
    "## One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6hTctKZSI3re",
   "metadata": {
    "id": "6hTctKZSI3re"
   },
   "outputs": [],
   "source": [
    "Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "Y_train_original_onehot = casualty_to_one_hot(Y_train_original)\n",
    "Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "array_train_images = np.asarray(train_images)\n",
    "array_train_original_images = np.asarray(train_original_images)\n",
    "array_test_images  = np.asarray(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IStgg--F5L3F",
   "metadata": {
    "id": "IStgg--F5L3F",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rvyfvMPy5L3G",
   "metadata": {
    "id": "rvyfvMPy5L3G"
   },
   "outputs": [],
   "source": [
    "# !conda install -c anaconda seaborn --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dg0d7-k15L3H",
   "metadata": {
    "id": "dg0d7-k15L3H",
    "tags": []
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SN7gwKNO5L3H",
   "metadata": {
    "id": "SN7gwKNO5L3H"
   },
   "outputs": [],
   "source": [
    "# correlation_matrix(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fAIUUg5L3J",
   "metadata": {
    "id": "d5fAIUUg5L3J",
    "tags": []
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lhQElB3I5L3J",
   "metadata": {
    "id": "lhQElB3I5L3J"
   },
   "outputs": [],
   "source": [
    "# pca(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffbfe4-9fce-4e68-81b9-766d485b87ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669feae-6b17-4f05-b43b-6d3aa87a1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_file_name = './2d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=2)\n",
    "\n",
    "# output_file_name = './2d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=2)\n",
    "\n",
    "# output_file_name = './3d_test_tsne.jpg'\n",
    "# plot_TSNE(X_test, Y_test, n_components=3)\n",
    "\n",
    "# output_file_name = './3d_train_tsne.jpg'\n",
    "# plot_TSNE(X_train, Y_train, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XBbgOcIz5L3J",
   "metadata": {
    "id": "XBbgOcIz5L3J",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U84H7J695L3J",
   "metadata": {
    "id": "U84H7J695L3J",
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A2mJaZVn5L3L",
   "metadata": {
    "id": "A2mJaZVn5L3L"
   },
   "outputs": [],
   "source": [
    "# input_img = Input(shape=(25,))\n",
    "\n",
    "# # definimos el encoder, que tendra una entrada de Input_img y una segunda capa con entrada de encoder1 y salida 3\n",
    "# encoder1 = layers.Dense(15, activation='sigmoid')(input_img)\n",
    "# encoder2 = layers.Dense(3, activation='sigmoid')(encoder1)\n",
    "\n",
    "# # definimos el  decoder que tendra una entrada inicial de encoder3 y una salida de 128 y finalmete una capa de salida con los mismos que Input_img\n",
    "# decoder1 = layers.Dense(15, activation='sigmoid')(encoder2)\n",
    "# decoder2 = layers.Dense(25, activation='sigmoid')(decoder1)\n",
    "\n",
    "# # this model maps an input to its reconstruction\n",
    "# autoencoder = tf.keras.Model(inputs=input_img, outputs=decoder2)\n",
    "# autoencoder.summary()\n",
    "\n",
    "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy') #se usan estos dos en estas arquitecturas\n",
    "\n",
    "X_train = array_train_images\n",
    "X_test = array_test_images\n",
    "\n",
    "X_train = X_train.reshape(len(array_train_images), 25)\n",
    "X_test  = X_test.reshape(len(X_test), 25)\n",
    "\n",
    "# autoencoder.fit(X_train, X_train,\n",
    "#                 epochs=15,\n",
    "#                 batch_size=32,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gIiKplwP5L3L",
   "metadata": {
    "id": "gIiKplwP5L3L",
    "tags": []
   },
   "source": [
    "#### Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opBOyrIx5L3M",
   "metadata": {
    "id": "opBOyrIx5L3M"
   },
   "outputs": [],
   "source": [
    "# # create encoder model\n",
    "# encoder = tf.keras.Model(inputs=input_img, outputs=encoder2)\n",
    "# encoder.summary()\n",
    "# # create decoder model\n",
    "# encoded_input = Input(shape=(3,))\n",
    "# #lo que hace aqui es quedarse con las capas que corresponden al decodificador\n",
    "# decoder_layer1 = autoencoder.layers[-2]\n",
    "# decoder_layer2 = autoencoder.layers[-1]\n",
    "# decoder = tf.keras.Model(inputs=encoded_input, outputs=decoder_layer2(decoder_layer1(encoded_input)))\n",
    "# decoder.summary()\n",
    "# # si miramos la salida, son simetricos el uno respecto al otro\n",
    "# # encoder va de input a 3 y decoder de 3 a input\n",
    "\n",
    "# # get latent vector for visualization\n",
    "# latent_vector = encoder.predict(X_test)\n",
    "# # get decoder output to visualize reconstructed image\n",
    "# reconstructed_imgs = decoder.predict(latent_vector)\n",
    "\n",
    "\n",
    "# # visualize in 3D plot\n",
    "# from pylab import rcParams\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "# fig = plt.figure(1)\n",
    "# ax = Axes3D(fig)\n",
    "\n",
    "# xs = latent_vector[:, 0]\n",
    "# ys = latent_vector[:, 1]\n",
    "# zs = latent_vector[:, 2]\n",
    "\n",
    "# # color=['red','green','blue']\n",
    "\n",
    "# # for x, y, z, label in zip(xs, ys, zs, Y_test):\n",
    "# #     c = color[int(label)]\n",
    "# #     ax.text(x, y, z, label, backgroundcolor=c)\n",
    "    \n",
    "# # ax.set_xlim(xs.min(), xs.max())\n",
    "# # ax.set_ylim(ys.min(), ys.max())\n",
    "# # ax.set_zlim(zs.min(), zs.max())\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# # X_test_encoded = encoder.predict(X_test, batch_size=32)\n",
    "# # plt.figure(figsize=(6, 6))\n",
    "# # plt.scatter(X_test_encoded[:, 0], X_test_encoded[:, 1], c=Y_test)\n",
    "# # plt.colorbar()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea5149-aa36-4789-a693-169d65b3c5ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94cf5a-3223-4b1d-8d5e-ca001ccc999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_train_images = np.asarray(train_images)\n",
    "array_test_images  = np.asarray(test_images)\n",
    "\n",
    "input_train_shape = (len(array_train_images), 5, 5, 1)\n",
    "input_test_shape  = (len(array_test_images), 5, 5, 1)\n",
    "\n",
    "array_train_images = array_train_images.reshape(input_train_shape)\n",
    "array_test_images  = array_test_images.reshape(input_test_shape)\n",
    "\n",
    "Y_test_labels = one_hot_to_casualty(Y_test)\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "pesos = class_weight.compute_class_weight('balanced',\n",
    "                                          classes = np.unique(Y_train_original),\n",
    "                                          y = Y_train_original)\n",
    "\n",
    "\n",
    "print('\\nPesos calculados:', pesos, '\\n\\n')\n",
    "\n",
    "\n",
    "# Keras espera un diccionario donde la clave sea el número de clase \n",
    "# y el valor sea el peso calculado. \n",
    "pesos = dict(enumerate(pesos))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a39c2-0589-4599-998e-624c3ae30fb9",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd71172-ec16-4ad8-9911-61f119055c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "MODEL_NAME = MODELS_NAME[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a5324-8a19-4d7a-9936-0528fac6a38b",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cccc3-659c-4d47-b610-4c9bc701b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # leaf_size = list(range(1,10, 2))\n",
    "# # n_neighbors = list(range(1,100, 10))\n",
    "# # p = [1, 2]\n",
    "\n",
    "# # Create new KNN object\n",
    "# hyperparameters = dict(leaf_size = leaf_size,\n",
    "#                        n_neighbors = n_neighbors)\n",
    "\n",
    "# # Use GridSearch\n",
    "# knn_2 = KNeighborsClassifier()\n",
    "\n",
    "# # Fit the model\n",
    "# clf = GridSearchCV(knn_2,\n",
    "#                    hyperparameters,\n",
    "#                    cv = 4)\n",
    "\n",
    "# knn = clf.fit(X_train, Y_train)\n",
    "\n",
    "# # Print The value of best Hyperparameters\n",
    "\n",
    "# best_leaf_size = knn.best_estimator_.get_params()['leaf_size']\n",
    "# best_n_neighbors = knn.best_estimator_.get_params()['n_neighbors']\n",
    "\n",
    "# print('Best leaf_size:', best_leaf_size)\n",
    "# print('Best n_neighbors:', best_n_neighbors)\n",
    "\n",
    "# df = pd.DataFrame({'best_leaf_size':[best_leaf_size], 'n_neighbors':[best_n_neighbors]})\n",
    "\n",
    "# FILE_NAME = f\"{MODEL_NAME}/madrid_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "# df.to_csv(HYPERPARAMS_PATH + FILE_NAME, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fdabd2-a01a-444b-b802-535186798bd3",
   "metadata": {},
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7f92a-3c77-40c7-9d58-82b2d321e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.pkl\"\n",
    "\n",
    "# # Its important to use binary mode \n",
    "# knnPickle = open(MODEL_PATH + MODEL_FILE_NAME, 'wb') \n",
    "\n",
    "# # source, destination \n",
    "# pickle.dump(knn, knnPickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf557e-fae2-429c-8409-7cbb3b48ae85",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9fe88-06b2-47ec-9459-6d931c50ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_2022-04-27-21:50:26.pkl\"\n",
    "\n",
    "# # load the model from disk\n",
    "# loaded_model = pickle.load(open(MODEL_PATH + MODEL_FILE_NAME, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e34364-1192-4115-9796-6b64d6790e73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdf70a-df80-4dce-ad02-c658c8bc197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted = knn.predict(X_test)\n",
    "\n",
    "# Y_predicted = convolution_1d.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "# F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "# F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# # plot_f1_score(f1_score_path = F1_SCORE_PATH,\n",
    "# #               f1_score_name = F1_SCORE_NAME,\n",
    "# #               history = history)\n",
    "\n",
    "# print(\"[INFO] evaluating network...\")\n",
    "\n",
    "# REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "# REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "# plot_classification_report(path = REPORT_PATH,\n",
    "#                            file_name = REPORT_NAME,\n",
    "#                            y_true = Y_test,\n",
    "#                            y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "# CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "# CONFUSION_MATRIX_NAME = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "#                       file_name = CONFUSION_MATRIX_NAME,\n",
    "#                       y_true = Y_test,\n",
    "#                       y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d85ab2-3416-4f93-a44e-bad2c346c015",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolution 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc8277-f9e1-4e68-8337-ae9eb66a3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS_NAME[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca013c7b-48ea-476a-b585-e8c751463f7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aca6bf-90b1-42bd-9fba-d7b8a5ad36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = convolution_1d.fit(array_train_images, Y_train_onehot,\n",
    "                             class_weight = pesos,\n",
    "                             batch_size = 128,\n",
    "                             epochs = 100,\n",
    "                             shuffle = True,\n",
    "                             validation_data = (array_test_images, Y_test_onehot))\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57162dc6-95aa-4c54-88c7-9773e537479e",
   "metadata": {},
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c587c-a817-4892-8bc7-64200abf4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\"\n",
    "\n",
    "tasp_cnn.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a2f07-c130-41aa-88fe-677b9c10ddd8",
   "metadata": {},
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1f493-056a-4491-84c1-c62bae69551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = 'madrid_convolution_1d_2022-05-11-08:53:52.h5'\n",
    "\n",
    "# convolution_1d = tf.keras.models.load_model(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278dca3-abde-4aa5-8c59-42ccd1ee423a",
   "metadata": {},
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d8dac-a50d-454a-8f35-8d0a77292181",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = convolution_1d.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_f1_score_history(f1_score_path = F1_SCORE_PATH,\n",
    "                      f1_score_name = F1_SCORE_NAME,\n",
    "                      history = history)\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "\n",
    "REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "plot_classification_report(path = REPORT_PATH,\n",
    "                           file_name = REPORT_NAME,\n",
    "                           y_true = Y_test,\n",
    "                           y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "CONFUSION_MATRIX_NAME = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "                      file_name = CONFUSION_MATRIX_NAME,\n",
    "                      y_true = Y_test,\n",
    "                      y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PohCQRSm67P0",
   "metadata": {
    "id": "PohCQRSm67P0",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolution 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a48c3-e0a7-4a93-ab64-901503503e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS_NAME[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vPh1ixx-67P2",
   "metadata": {
    "id": "vPh1ixx-67P2"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b-rJTSQC67P2",
   "metadata": {
    "id": "b-rJTSQC67P2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = tasp_cnn.fit(array_train_images, Y_train_onehot,\n",
    "                       class_weight = pesos,\n",
    "                       batch_size = 128,\n",
    "                       epochs = 100,\n",
    "                       shuffle = True,\n",
    "                       validation_data = (array_test_images, Y_test_onehot))\n",
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dca0f9-336a-4113-b58c-1fb300c89608",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac281b-8364-4710-b6e1-d75ae4a401d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\"\n",
    "\n",
    "tasp_cnn.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aT8XDceKGSdi",
   "metadata": {
    "id": "aT8XDceKGSdi",
    "tags": []
   },
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dRaqg9SXGRwb",
   "metadata": {
    "id": "dRaqg9SXGRwb"
   },
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = 'madrid_convolution_2d_2022-05-11-08:53:52.h5'\n",
    "\n",
    "# tasp_cnn = tf.keras.models.load_model(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wD_BOwcwGb4W",
   "metadata": {
    "id": "wD_BOwcwGb4W"
   },
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHVVq0khGato",
   "metadata": {
    "id": "nHVVq0khGato",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_predicted = tasp_cnn.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_f1_score_history(f1_score_path = F1_SCORE_PATH,\n",
    "                      f1_score_name = F1_SCORE_NAME,\n",
    "                      history = history)\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "\n",
    "REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "plot_classification_report(path = REPORT_PATH,\n",
    "                           file_name = REPORT_NAME,\n",
    "                           y_true = Y_test,\n",
    "                           y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "CONFUSION_MATRIX_NAME = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "                      file_name = CONFUSION_MATRIX_NAME,\n",
    "                      y_true = Y_test,\n",
    "                      y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557fd15-2eb8-497f-bd7e-34e8fc596449",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6d7a4-039c-452e-bc4c-78c5199996c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS_NAME[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DoJbgcgC1d83",
   "metadata": {
    "id": "DoJbgcgC1d83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tasp_cnn.save(root_path + 'madrid_model_XGBOOST_predicted.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b65f8d-ba77-42e8-850a-d4b8b5135641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autokeras as ak\n",
    "\n",
    "# # clf = ak.ImageClassifier(num_classes = 3,\n",
    "# #                          loss='categorical_crossentropy',\n",
    "# #                          metrics = [tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold = 0.1)],\n",
    "# #                          overwrite = True,\n",
    "# #                          tuner= 'bayesian',\n",
    "# #                          max_trials = 20,\n",
    "# #                          max_model_size = 3000000\n",
    "# #                         )\n",
    "# clf = ak.StructuredDataClassifier(num_classes = 3,\n",
    "#                              loss='categorical_crossentropy',\n",
    "#                              metrics = [tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold = 0.1)],\n",
    "#                              overwrite = True,\n",
    "#                              tuner= 'bayesian',\n",
    "#                              max_trials = 20\n",
    "#                         )\n",
    "\n",
    "# clf.fit(array_train_images,\n",
    "#         np.asarray(Y_train),\n",
    "#         epochs = 100,\n",
    "#         batch_size = 128,\n",
    "#         validation_data = (array_test_images, np.asarray(Y_test)))\n",
    "\n",
    "# best_auto_model = clf.export_model()\n",
    "# print(best_auto_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bbb5c-e78f-426b-bdac-c679f9ee671c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61062eb-31de-4166-8855-98f5b9277758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\"\n",
    "\n",
    "# best_auto_model.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8663f-213d-4010-9195-6ad9cbc9c9e1",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216cee8-ff56-498d-98db-112eb635c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_predicted = best_auto_model.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "# F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "# F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# # plot_f1_score(f1_score_path = F1_SCORE_PATH,\n",
    "# #               f1_score_name = F1_SCORE_NAME,\n",
    "# #               history = history)\n",
    "\n",
    "# print(\"[INFO] evaluating network...\")\n",
    "\n",
    "# REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "# REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "# plot_classification_report(path = REPORT_PATH,\n",
    "#                            file_name = REPORT_NAME,\n",
    "#                            y_true = Y_test,\n",
    "#                            y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "# CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "# CONFUSION_MATRIX_NAME  = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.jpg\"\n",
    "\n",
    "# plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "#                       file_name = CONFUSION_MATRIX_NAME,\n",
    "#                       y_true = Y_test,\n",
    "#                       y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d656c0a-d4e9-4758-a1f7-1878ed998fa8",
   "metadata": {},
   "source": [
    "# Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60314a42-8590-48e1-9d87-9f41859c01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_TIMESTAMP\n",
    "\n",
    "# WEIGHTS_PATH  = './feature_weights/'\n",
    "# REPORTS_PATH  = 'Reports/'\n",
    "# MODELS_PATH   = 'Models/'\n",
    "# F1_SCORES_PATH = 'F1scores/'\n",
    "# GA_SCORES_PATH = 'GA_Scores/'\n",
    "# HYPERPARAMS_PATH = './hyperparams/'\n",
    "\n",
    "# HYPERPARAMS_EVOLUTON_PATH = './hyperparams_evolution/'\n",
    "# FINAL_POPULATION_PATH = './population/'\n",
    "# CONFUSIONS_MATRIX_PATH = 'confusion_matrix/'\n",
    "\n",
    "# ###### MODELS ######\n",
    "# MODELS_NAME = ['knn', 'convolution_1d', 'convolution_2d']\n",
    "# DATA_PATHS = [REPORTS_PATH]\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "reports_summary = pd.DataFrame()\n",
    "\n",
    "cities = ['leeds', 'madrid']\n",
    "\n",
    "models_renaming = {'knn': 'KNN',\n",
    "                   'convolution_1d': '1D-convolution',\n",
    "                   'convolution_2d': '2D-convolution'}\n",
    "                   # 'auto_ml': 'AutoML'}\n",
    "\n",
    "for model_name in MODELS_NAME:\n",
    "    \n",
    "    REPORT_PATH = f\"{REPORTS_PATH}{model_name}/\"\n",
    "\n",
    "    for city in cities:\n",
    "        REPORT_NAME  = f\"{city}_{model_name}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "        if exists(REPORT_PATH + REPORT_NAME):\n",
    "            report = pd.read_csv(REPORT_PATH + REPORT_NAME, index_col=[0])\n",
    "            report.insert(0, 'city', city)\n",
    "            report.insert(1, 'model', models_renaming[model_name])\n",
    "            \n",
    "            reports_summary = pd.concat([reports_summary, report])\n",
    "\n",
    "reports_summary = reports_summary.sort_values(['city', 'model'], ascending = [True, True])\n",
    "\n",
    "c_m = reports_summary['city'] + '_' + reports_summary['model']\n",
    "reports_summary.insert(0, 'c_m', c_m)\n",
    "\n",
    "# reports_summary.drop(['city', 'model'], axis=1, inplace = True)\n",
    "\n",
    "SAVE_PATH =  f\"{REPORTS_SUMMARY_PATH}{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "reports_summary.to_csv(SAVE_PATH, index= True)\n",
    "\n",
    "reports_summary.insert(0, 'accident_type', reports_summary.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5de0ef-2c61-42fb-886d-cabe6c9bb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "MEASURE_TYPES  = ['precision', 'recall', 'f1-score']\n",
    "ACCIDENT_TYPES = ['Slight', 'Serious', 'Fatal']\n",
    "\n",
    "fig, axs = plt.subplots(len(MEASURE_TYPES), len(cities), figsize=(15,20))\n",
    "\n",
    "leeds_reports_summary  = reports_summary[reports_summary['city'] == 'leeds']\n",
    "madrid_reports_summary = reports_summary[reports_summary['city'] == 'madrid']\n",
    "\n",
    "# print(leeds_reports_summary.loc[ACCIDENT_TYPES])\n",
    "\n",
    "for index, measure_type in enumerate(MEASURE_TYPES):\n",
    "\n",
    "    ax = sns.barplot(x = 'accident_type',\n",
    "                     y = measure_type,\n",
    "                     hue = 'model',\n",
    "                     data = leeds_reports_summary.loc[ACCIDENT_TYPES],\n",
    "                     ax = axs[index, 0]).set(title = f\"{measure_type} Leeds\")\n",
    "\n",
    "    ax = sns.barplot(x = 'accident_type',\n",
    "                     y = measure_type,\n",
    "                     hue = 'model',\n",
    "                     data = madrid_reports_summary.loc[ACCIDENT_TYPES],\n",
    "                     ax = axs[index, 1]).set(title = f\"{measure_type} Madrid\")\n",
    "\n",
    "SAVE_PATH = f\"{REPORTS_SUMMARY_PATH}{MODEL_TIMESTAMP}.png\"\n",
    "\n",
    "fig = fig.get_figure()\n",
    "fig.savefig(SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "V7Azjtl8gRth",
    "qKYh5EeThQ_7",
    "kISRP5AQhWTD",
    "cCo2emMclT8h",
    "gJfbDNO5oB1N",
    "7a4EsWwQhe_i",
    "ycdOBuHSjhSk",
    "5PmJpoCCcxMJ",
    "ybjvOI7x0PKz",
    "pVPFGQ0AoNRD",
    "_Z4nz3ioxtXb",
    "dg0d7-k15L3H",
    "d5fAIUUg5L3J"
   ],
   "name": "TFM_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:TFM]",
   "language": "python",
   "name": "conda-env-TFM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
