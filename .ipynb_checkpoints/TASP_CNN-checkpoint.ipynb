{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19dc680-a426-4afe-911a-46021ed737ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_frame.distrito.value_counts()\n",
    "# # pd.to_datetime(data_frame.fecha[-1])\n",
    "# # pd.to_datetime('')\n",
    "# print(pd.to_datetime(data_frame.fecha.iloc[-1], format='%d/%m/%Y'))\n",
    "# print(pd.to_datetime(octubre_2022.FX_DATOS_INI.iloc[-1], format='%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c7145-b96a-4745-bcf0-6f6d5c516534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agosto_2021 = pd.read_csv('/home/luis/Descargas/Rango_Edades_Seccion_202108.csv',delimiter=';')\n",
    "# # octubre_2022.DESC_DISTRITO.value_counts()\n",
    "# type(agosto_2021.FX_DATOS_INI[0])\n",
    "# agosto_2021.FX_DATOS_INI[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f10a2-a5f3-4e4f-9571-fed710b3182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "# from keras_fsl.models.encoders import BasicCNN\n",
    "# from keras_fsl.layers import GramMatrix\n",
    "# from keras_fsl.losses.gram_matrix_losses import BinaryCrossentropy\n",
    "# from keras_fsl.metrics.gram_matrix_metrics import classification_accuracy, min_eigenvalue\n",
    "# from keras_fsl.utils.tensors import get_dummies\n",
    "\n",
    "# #%% Get data\n",
    "# train_dataset, val_dataset, test_dataset = [\n",
    "#     dataset.shuffle(1024).batch(64).map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), get_dummies(y)[0]))\n",
    "#     for dataset in tfds.load(name=\"omniglot\", split=[\"train[:90%]\", \"train[90%:]\", \"test\"], as_supervised=True)\n",
    "# ]\n",
    "\n",
    "# input_shape = next(tfds.as_numpy(train_dataset.take(1)))[0].shape[1:]  # first shape is batch_size\n",
    "\n",
    "# #%% Training\n",
    "# encoder = BasicCNN(input_shape=input_shape)\n",
    "# support_layer = GramMatrix(kernel=\"DenseSigmoid\")\n",
    "# model = Sequential([encoder, support_layer])\n",
    "# model.compile(optimizer=\"Adam\", loss=BinaryCrossentropy(), metrics=[classification_accuracy(), min_eigenvalue])\n",
    "# model.fit(train_dataset, validation_data=val_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835b171f-bf99-42a4-addd-79c633d23f4b",
   "metadata": {},
   "source": [
    "[TFM](https://github.com/jmrplens/TFG-TFM_EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fb9ae-88b7-4246-8fb7-f7e904b6d049",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Diagrama de flujo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60478eb5-96bc-4235-801e-0bef3c9b2433",
   "metadata": {},
   "source": [
    "<center><img width='600px' src=\"Data/Data_flow.png\"/></center>\n",
    "\n",
    "Metodología\n",
    "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=294849"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2e215-b652-4dbb-a951-8a62aff35046",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Métodos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yabWKtrCSTTp",
   "metadata": {
    "id": "yabWKtrCSTTp",
    "tags": []
   },
   "source": [
    "## Carga Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9kRlrtLcSWSU",
   "metadata": {
    "id": "9kRlrtLcSWSU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509f4f4-6594-4a0c-9bed-927d3922d6a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Versión y especificación de directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b404dc-0e9c-4945-b4e5-d1a77d863a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "MODEL_TIMESTAMP = datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "\n",
    "\n",
    "WEIGHTS_PATH  = './feature_weights/'\n",
    "REPORTS_PATH  = 'Reports/'\n",
    "MODELS_PATH   = 'Models/'\n",
    "F1_SCORES_PATH = 'F1scores/'\n",
    "GA_SCORES_PATH = 'GA_Scores/'\n",
    "HYPERPARAMS_PATH = './hyperparams/'\n",
    "\n",
    "HYPERPARAMS_EVOLUTON_PATH = './hyperparams_evolution/'\n",
    "FINAL_POPULATION_PATH  = './population/'\n",
    "CONFUSIONS_MATRIX_PATH = 'confusion_matrix/'\n",
    "TSNE_PATH = 'tsne/'\n",
    "\n",
    "REPORTS_TIMES_PATH = 'times/'\n",
    "\n",
    "\n",
    "###### MODELS ######\n",
    "MODELS_NAME = ['knn', 'convolution_1d', 'convolution_2d', 'nb', 'svc', 'auto_ml']\n",
    "\n",
    "REPORTS_SUMMARY_PATH = f\"{REPORTS_PATH}summary/\"\n",
    "######## CONFIG ########\n",
    "loaded_timestamp = '2023-03-26-20:53:54'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf93ea-bab8-4846-a12e-bcfa1b14a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "laptop = False\n",
    "calculate_weights = False\n",
    "tsne = False\n",
    "\n",
    "madrid = True\n",
    "\n",
    "tree_method = 'auto' if laptop else 'gpu_hist'\n",
    "\n",
    "train_nn = not laptop\n",
    "other_models = cnn1d = False\n",
    "cnn1d = True\n",
    "cnn2d = True\n",
    "other_models = cnn1d = True\n",
    "\n",
    "# calculate_cnn_hyperparams = True\n",
    "calculate_cnn_hyperparams = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea665cef-e563-4798-8ed2-78a8f2ddb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# laptop = True\n",
    "# calculate_weights = True\n",
    "# tsne = False\n",
    "\n",
    "# leeds  = False\n",
    "# madrid = False\n",
    "# UK = True\n",
    "\n",
    "# tree_method = 'auto' if laptop else 'gpu_hist'\n",
    "# # tree_method = 'gpu_hist'\n",
    "# train_nn = False\n",
    "# other_models = cnn1d = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb628e-a7f4-40e8-a62d-576f7a78c2fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb40e62-06ae-46b9-a711-a5d5667c4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32053d4d-f4d8-4b07-9cd8-bb5bf4ac8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.layers import Input, Lambda, Activation, Conv2D, MaxPooling2D, BatchNormalization, Add, concatenate, Conv2DTranspose, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5732f07-8398-47a1-9d62-53cf7dd45556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))\n",
    "# # !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b591c-8916-415a-a23c-8309f52f56e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Importador/Exportador JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75269bf0-27f7-4b71-b311-256035370133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json(feature_vector, root_path, file_name):\n",
    "    with open(root_path + file_name, 'w') as outfile:\n",
    "        json.dump(feature_vector, outfile)\n",
    "\n",
    "def load_json(root_path, file_name):\n",
    "    with open(root_path + file_name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4388716-87a9-4e2d-ab4a-e3959fa1958f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b5661a-a9ac-4686-9611-6d43711d1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def get_feature_matrix_indexes(sorted_feature_vector,matrix):  \n",
    "\n",
    "    half_row = round((matrix.shape[0] - 1) / 2)\n",
    "    half_column = round((matrix.shape[1] - 1) / 2)\n",
    "\n",
    "    matrix_indexes = {}\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for parent_key in sorted_feature_vector:\n",
    "        normalized_index = math.ceil(index/2)\n",
    "\n",
    "        if (index % 2 != 0): # Impar\n",
    "            current_row = half_row - normalized_index\n",
    "        else: # Par\n",
    "            current_row = half_row + normalized_index\n",
    "\n",
    "        sorted_child_indexes = np.argsort(feature_vector[parent_key]['feature_weights'])[::-1]\n",
    "\n",
    "        child_names   = np.array(feature_vector[parent_key]['feature_childs'])\n",
    "        child_weights = np.array(feature_vector[parent_key]['feature_weights'])\n",
    "\n",
    "        sorted_child_names   = child_names[sorted_child_indexes]\n",
    "        sorted_child_weights = child_weights[sorted_child_indexes]\n",
    "\n",
    "        position = 0\n",
    "        for sorted_child_index in sorted_child_indexes:\n",
    "            normalized_position = math.ceil(position/2)\n",
    "\n",
    "            if (position % 2 != 0): # Impar\n",
    "                current_column = half_column - normalized_position\n",
    "            else: # Par\n",
    "                current_column = half_column + normalized_position\n",
    "\n",
    "            matrix_indexes[child_names[sorted_child_index]] = [current_row, current_column]\n",
    "            position = position + 1 \n",
    "\n",
    "        index = index + 1\n",
    "\n",
    "    return matrix_indexes\n",
    "    \n",
    "def fv2gi(feature_vector):\n",
    "\n",
    "    max_dimension = 0\n",
    "    for key in feature_vector:\n",
    "        childs_number = len(feature_vector[key]['feature_childs'])\n",
    "        max_dimension = max(childs_number, max_dimension)\n",
    "                \n",
    "    matrix = np.zeros((max_dimension, max_dimension))\n",
    "\n",
    "    weights_vector = []\n",
    "    for parent_key in feature_vector:\n",
    "        wpi = sum([float(child_weight) for child_weight in feature_vector[parent_key]['feature_weights']])\n",
    "        feature_vector[parent_key]['wpi'] = wpi\n",
    "        weights_vector.append(wpi)\n",
    "\n",
    "   \n",
    "    sorted_feature_vector = sorted(feature_vector.items(),\n",
    "                                   key = lambda item: item[1]['wpi'],\n",
    "                                   reverse = True)\n",
    "     \n",
    "    sorted_feature_vector = dict(sorted_feature_vector)\n",
    "\n",
    "    \n",
    "    matrix_indexes = get_feature_matrix_indexes(sorted_feature_vector, matrix)\n",
    "\n",
    "    return matrix_indexes\n",
    "\n",
    "# matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb0004-1f9b-493a-9c83-91d943b5309d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b732a-ca4a-440e-8ae2-dbb3d15ba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feature_vector(X_dataset,child_weights):\n",
    "  # Obtenemos el set de columnas del dataset\n",
    "  train_columns_set  = set(X_dataset.columns)\n",
    "\n",
    "  for parent_feature in feature_vector.keys():\n",
    "    # Obtiene el set de características hijas del padre actual\n",
    "    # dict.fromleys para mantener el orden, un set desordena los valores\n",
    "    feature_childs_set = dict.fromkeys(feature_vector[parent_feature]['feature_childs'])\n",
    "\n",
    "    # Obtener el índice de las columnas del actual padre para acceder a los pesos del XGBoost\n",
    "    index_feature_childs = X_dataset.columns.get_indexer(feature_childs_set)\n",
    "\n",
    "    feature_vector[parent_feature]['feature_weights'] = list([str(child_weight) for child_weight in child_weights[index_feature_childs]])\n",
    "\n",
    "  return feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c95ea-ae18-4406-b116-5ce6674b7f6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017e1821-71be-48d9-85cb-07808165a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_data(X_data):\n",
    "\n",
    "    # Create a sample df\n",
    "    normalized_df = X_data\n",
    "\n",
    "    # Calculate the zscores and drop zscores into new column\n",
    "    for column in normalized_df.columns:\n",
    "        normalized_df[column] = zscore(normalized_df[column])\n",
    "    \n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358c9a0-6423-4aa8-92a3-e01a8ac72bb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Oversampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac95c3-8dc1-4acf-b4e0-78d24fa64131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "\n",
    "def oversample_data(X_data, Y_labels):\n",
    "\n",
    "    oversampler = BorderlineSMOTE(kind='borderline-2', random_state = 3)\n",
    "    # oversampler = RandomOverSampler()\n",
    "    # oversampler = KMeansSMOTE()\n",
    "    X_oversampled, Y_oversampled = oversampler.fit_resample(X_data, Y_labels)\n",
    "\n",
    "    print('********** After OverSampling **********')\n",
    "    # print('Slight: ', (Y_oversampled == 'Slight').sum())\n",
    "    # print('Serious:', (Y_oversampled == 'Serious').sum())\n",
    "    # print('Fatal:  ', (Y_oversampled == 'Fatal').sum())\n",
    "    # print('\\n Total X: ', len(X_oversampled), ' Total Y: ', len(Y_oversampled), '\\n')\n",
    "\n",
    "    # print('********** After OverSampling **********')\n",
    "    print('Slight: ', (Y_oversampled == 'Slight').sum())\n",
    "    print('Assistance:', (Y_oversampled == 'Assistance').sum())\n",
    "    print('\\n Total X: ', len(X_oversampled), ' Total Y: ', len(Y_oversampled), '\\n')\n",
    "\n",
    "    return X_oversampled, Y_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e306be-543d-4ba7-b9a4-2cb1cb938f0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b61998b-bd46-4dad-a0e8-8363f92f7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gray_images(dataset, max_dimension, matrix_indexes):\n",
    "\n",
    "    matrix_3d = np.zeros((max_dimension, max_dimension, len(dataset.index)))\n",
    "    print(len(dataset.index))\n",
    "    for feature, value in matrix_indexes.items():\n",
    "        matrix_3d[value[0], value[1],] = dataset[feature]\n",
    "        \n",
    "    return matrix_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc000ada-10b7-43b8-ab9f-789d9d35be8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Algoritmo genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263a40b-6570-42df-8c3c-8f57209e49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS_TO_OPTIMIZE = {'eta': {'type': 'float',\n",
    "                                   'init': [0.01, 1],\n",
    "                                   'mutation': [-0.2, 0.2],\n",
    "                                   'round': 2\n",
    "                                   },\n",
    "                           'max_depth': {'type': 'int',\n",
    "                                         'init': [1, 25],\n",
    "                                         'mutation': [-3, 3],\n",
    "                                         'step': 1\n",
    "                                   },\n",
    "                           'min_child_weight': {'type': 'float',\n",
    "                                                'init': [0.01, 20.0],\n",
    "                                                'mutation': [-4, 4],\n",
    "                                                'round': 1\n",
    "                                   }\n",
    "                          }\n",
    "\n",
    "number_of_individuals = 50\n",
    "numberOfParentsMating = 10\n",
    "number_of_hyperparams = len(HYPERPARAMS_TO_OPTIMIZE)\n",
    "number_of_generations = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabcfd52-f850-40a9-a44f-183653456b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inicializar población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f539cc1-22ad-467b-8e0b-0c892aff1111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.genetic_algorithm import generate_individual\n",
    "from src.genetic_algorithm import initialize_population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80a92b-17eb-4a29-b12e-3852265dde12",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fitness function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5133e38-307e-4138-a664-fd4e28dc89c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.genetic_algorithm import fitness_f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9222dcc-088c-468e-bbc7-a0dec6541004",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluación de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f69b5d-1aba-4bad-a705-adcc672e68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_population(population, hyperparams_to_optimize, dMatrixTrain, dMatrixTest, Y_test):\n",
    "\n",
    "    fScore = []\n",
    "    \n",
    "    params = {'objective':'multi:softprob',\n",
    "              'tree_method': 'auto',\n",
    "              'single_precision_histogram': True,\n",
    "              'num_class': 2\n",
    "             }\n",
    "\n",
    "    # params = {'objective':'binary:logistic',\n",
    "    #           'tree_method': tree_method,\n",
    "    #           'single_precision_histogram': True\n",
    "    #          }\n",
    "    for individual_index in range(population.shape[0]):\n",
    "        # Se almacenan en hyperparams_to_optimize los valores del individuo con su nombre correspondiente de hyperparams_name_to_optimize.\n",
    "        hyperparams = {}\n",
    "\n",
    "        for index, hyperparam_value in enumerate(population[individual_index]):\n",
    "\n",
    "            hyperparam_name_to_optimize = list(hyperparams_to_optimize.keys())[index]\n",
    "            data_type = hyperparams_to_optimize[hyperparam_name_to_optimize]['type']\n",
    "\n",
    "            hyperparams[hyperparam_name_to_optimize] = hyperparam_value\n",
    "            hyperparams[hyperparam_name_to_optimize] = hyperparams[hyperparam_name_to_optimize].astype(data_type)\n",
    "        \n",
    "        params.update(hyperparams)\n",
    "\n",
    "        # num_round = params['n_estimators']\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        xgb.set_config(verbosity = 0)\n",
    "        bst = xgb.train(params,\n",
    "                        dMatrixTrain)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        preds = bst.predict(dMatrixTest)\n",
    "        \n",
    "        single_predictions = [np.argmax(pred) for pred in preds]\n",
    "        # preds = preds > 0.5\n",
    "        fitness_score = fitness_f1score(Y_test, single_predictions)\n",
    "\n",
    "        # print(f\"{individual_index}: {hyperparams} --> time(s): {round(end - start, 2)} --> score: {fitness_score}\")\n",
    "\n",
    "        fScore.append(fitness_score)\n",
    "\n",
    "    return fScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b66fd8-ae70-4f20-84f6-0113ae309a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Selección de padres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d8007-f28e-424c-8ff6-f174822b02a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select parents for mating\n",
    "def new_parents_selection(population, fitness, numParents):\n",
    "    selectedParents = np.empty((numParents, population.shape[1])) # Create an array to store fittest parents.\n",
    "\n",
    "    for parentId in range(numParents):\n",
    "        bestFitnessId = np.where(fitness == np.max(fitness))\n",
    "        bestFitnessId  = bestFitnessId[0][0]\n",
    "        selectedParents[parentId, :] = population[bestFitnessId, :]\n",
    "        fitness[bestFitnessId] = -1 # Set this value to negative, in case of F1-score, so this parent is not selected again\n",
    "\n",
    "    return selectedParents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9b882-ad82-4446-a301-a856dedbd660",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Cruzamiento de población"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db837b53-3d02-446c-aa9c-6c772da6be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mate these parents to create children having parameters from these parents (we are using uniform crossover method)\n",
    "'''\n",
    "def crossover_uniform(parents, childrenSize):\n",
    "    \n",
    "    crossoverPointIndex  = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8) # get all the index\n",
    "    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]/2)) # select half  of the indexes randomly\n",
    "    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) #select leftover indexes\n",
    "    \n",
    "    children = np.empty(childrenSize)\n",
    "    \n",
    "    '''\n",
    "    Create child by choosing parameters from two parents selected using new_parent_selection function. The parameter values\n",
    "    will be picked from the indexes, which were randomly selected above. \n",
    "    '''\n",
    "    for i in range(childrenSize[0]):\n",
    "        \n",
    "        #find parent 1 index \n",
    "        parent1_index = i%parents.shape[0]\n",
    "        #find parent 2 index\n",
    "        parent2_index = (i+1)%parents.shape[0]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n",
    "        #insert parameters based on random selected indexes in parent 1\n",
    "        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n",
    "\n",
    "    return children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d549ad-a75d-4943-a89a-ea9e783054a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Mutación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb741b25-56ed-4e66-adbd-42d3b5607974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(crossover, hyperparams_to_optimize):\n",
    "    \n",
    "    # MUTATION_PROBABILITY = 1/numberOfParameters\n",
    "    \n",
    "    MUTATION_PROBABILITY = 0.4\n",
    "    number_of_parameters = len(hyperparams_to_optimize)\n",
    "\n",
    "    for idx in range(crossover.shape[0]):\n",
    "\n",
    "        mutation_probability = np.random.rand(1)\n",
    "\n",
    "        while MUTATION_PROBABILITY > mutation_probability:\n",
    "\n",
    "            mutationValue = 0\n",
    "\n",
    "            hyperparam_selected_index = np.random.randint(0, number_of_parameters)\n",
    "            hyperparam_selected_name  = list(hyperparams_to_optimize.keys())[hyperparam_selected_index]\n",
    "\n",
    "            min_limit_value = hyperparams_to_optimize[hyperparam_selected_name]['init'][0]\n",
    "            max_limit_value = hyperparams_to_optimize[hyperparam_selected_name]['init'][1]\n",
    "\n",
    "            min_mutation_value = hyperparams_to_optimize[hyperparam_selected_name]['mutation'][0]\n",
    "            max_mutation_value = hyperparams_to_optimize[hyperparam_selected_name]['mutation'][1]\n",
    "\n",
    "            data_type = hyperparams_to_optimize[hyperparam_selected_name]['type']\n",
    "            \n",
    "            if data_type == 'int':\n",
    "                step = hyperparams_to_optimize[hyperparam_selected_name]['step']\n",
    "                mutationValue = int(random.randrange(min_mutation_value, max_mutation_value, step = step))\n",
    "\n",
    "            if data_type == 'float':\n",
    "                round_to = hyperparams_to_optimize[hyperparam_selected_name]['round']\n",
    "                mutationValue = round(random.uniform(min_mutation_value, max_mutation_value), round_to)\n",
    "                \n",
    "            # print(idx, hyperparam_selected_name, mutationValue)\n",
    "\n",
    "            crossover[idx, hyperparam_selected_index] = crossover[idx,hyperparam_selected_index] + mutationValue\n",
    "\n",
    "            if(crossover[idx, hyperparam_selected_index] > max_limit_value):\n",
    "                crossover[idx, hyperparam_selected_index] = max_limit_value\n",
    "\n",
    "            if(crossover[idx, hyperparam_selected_index] < min_limit_value):\n",
    "                crossover[idx, hyperparam_selected_index] = min_limit_value\n",
    "                \n",
    "            mutation_probability = np.random.rand(1)\n",
    "\n",
    "\n",
    "    return crossover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d6d9f-8ae4-4740-9983-e51d39cdeac6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded491b-6d05-44f5-9ee9-44064babfd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one channel\n",
    "# https://machinelearningmastery.com/a-gentle-introduction-to-channels-first-and-channels-last-image-formats-for-deep-learning/\n",
    "\n",
    "# Add one channel to gray images depending of the number of the data\n",
    "def shape_images(X_data, gray_images):\n",
    "  images = []\n",
    "\n",
    "  for i in range(0,len(X_data)):\n",
    "      original_matrix = gray_images[:,:,i]\n",
    "      # print(original_matrix.shape)\n",
    "      shaped_image = np.expand_dims(original_matrix, axis=2)\n",
    "      # print(shaped_image.shape)\n",
    "      images.append(shaped_image)\n",
    "      # plt.matshow(shaped_image)\n",
    "\n",
    "  return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71fa80-2c77-46a3-819c-bb7a0ff9a5a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## One-Hot Encoder/Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2db8a-49a5-4a06-bf66-025a0d472001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casualty_to_one_hot(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        'Slight': 0,\n",
    "        'Serious': 1,\n",
    "        'Fatal': 2\n",
    "    }\n",
    "\n",
    "    Y_labels.replace(transf, inplace = True)\n",
    "\n",
    "    return tf.one_hot(Y_labels, 3)\n",
    "\n",
    "def one_hot_to_casualty(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        0: 'Slight',\n",
    "        1: 'Serious',\n",
    "        2: 'Fatal'\n",
    "    }   \n",
    "\n",
    "    return Y_labels.replace(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b04e1-1d9b-4551-88d1-29fd199e2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casualty_to_one_hot(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        'Slight': 0,\n",
    "        'Assistance': 1\n",
    "    }\n",
    "\n",
    "    Y_labels.replace(transf, inplace = True)\n",
    "\n",
    "    return tf.one_hot(Y_labels, 2)\n",
    "\n",
    "def one_hot_to_casualty(Y_labels):\n",
    "\n",
    "    transf = {\n",
    "        0: 'Slight',\n",
    "        1: 'Assistance'\n",
    "    }   \n",
    "\n",
    "    return Y_labels.replace(transf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021df180-f056-4e21-ac6e-39d41369831b",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a4a49-64d8-4a32-abf7-e626f39d0938",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfa69d-7135-476f-9c1d-4ebf5d30bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def correlation_matrix(X_data):\n",
    "    corrMatrix = X_data.corr()\n",
    "    fig,ax  = plt.subplots(1,1,figsize=(20,15))\n",
    "    sns.heatmap(corrMatrix, annot=True)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 30)\n",
    "    plt.savefig('saving-a-seaborn-plot-as-eps-file.svg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaa5de-34f0-4724-ba36-0fdcf19c64e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d011694-26fe-41e3-9a39-85722f128f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(X_train_data, X_test_data):\n",
    "    pca = PCA()\n",
    "    X_train_pca = pca.fit_transform(X_train_data)\n",
    "    X_test_pca  = pca.transform(X_test_data)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "    figure_name = plt.figure(figsize=(20, 15))\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('number of components')\n",
    "    plt.ylabel('cumulative explained variance')\n",
    "    plt.savefig('saving-a-seaborn-plot-as-eps-file.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401456b5-b7b9-421c-83db-709796ee1d8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c55dca-09c1-4831-a64c-bc04a9cdf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_TSNE(X_data, Y_data, n_components, output_file_name, title):\n",
    "\n",
    "    # X_data_scaled = StandardScaler().fit_transform(X_data)\n",
    "    z_data = TSNE(n_components = n_components).fit_transform(X_data)\n",
    "\n",
    "    # X_test_scaled = StandardScaler().fit_transform(X_test),\n",
    "    # z_test = TSNE(n_components=2).fit_transform(X_test_scaled),\n",
    "\n",
    "    palette = sns.color_palette('husl', 3)\n",
    "    fig,ax  = plt.subplots(1, 1, figsize=(7,4))\n",
    "    sns.scatterplot(x = z_data[:,0],\n",
    "                    y = z_data[:,1],\n",
    "                    hue = Y_data,\n",
    "                    palette = palette,\n",
    "                    legend = 'full'\n",
    "                   ).set(title = title)\n",
    "    plt.xlabel('Dimension 1', weight = 'bold').set_fontsize('10')\n",
    "    plt.ylabel('Dimension 1', weight = 'bold').set_fontsize('10')\n",
    "\n",
    "    if (output_file_name): plt.savefig(output_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b4c26-5100-4723-9342-b7b9aa187d79",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4072a7-e2e5-4e45-9456-4bea3270a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder ():\n",
    "    input_img = Input(shape=(25,))\n",
    "\n",
    "    # definimos el encoder, que tendra una entrada de Input_img y una segunda capa con entrada de encoder1 y salida 3\n",
    "    encoder1 = layers.Dense(15, activation='sigmoid')(input_img)\n",
    "    encoder2 = layers.Dense(3, activation='sigmoid')(encoder1)\n",
    "\n",
    "    # definimos el  decoder que tendra una entrada inicial de encoder3 y una salida de 128 y finalmete una capa de salida con los mismos que Input_img\n",
    "    decoder1 = layers.Dense(15, activation='sigmoid')(encoder2)\n",
    "    decoder2 = layers.Dense(25, activation='sigmoid')(decoder1)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = tf.keras.Model(inputs=input_img, outputs=decoder2)\n",
    "    autoencoder.summary()\n",
    "\n",
    "    autoencoder.compile(optimizer='adam',\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)],) #se usan estos dos en estas arquitecturas\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684781d-95b6-4f1d-a45f-37e7b332e085",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d551aa9-a234-487f-9a27-353f89d344cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class ConditionalGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(ConditionalGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n",
    "        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.gen_loss_tracker, self.disc_loss_tracker]\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(ConditionalGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data.\n",
    "        real_images, one_hot_labels = data\n",
    "\n",
    "        # Add dummy dimensions to the labels so that they can be concatenated with\n",
    "        # the images. This is for the discriminator.\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None] # TASPCNN: shape=(None, 2, 1, 1)\n",
    "        image_one_hot_labels = tf.repeat(\n",
    "            image_one_hot_labels, repeats=[image_size * image_size]\n",
    "        ) # TASPCNN: shape=(None,)\n",
    "        image_one_hot_labels = tf.reshape(\n",
    "            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n",
    "        ) # TASPCNN: shape=(None, 25, 25, 2)\n",
    "\n",
    "        # Sample random points in the latent space and concatenate the labels.\n",
    "        # This is for the generator.\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        ) # TASPCNN: shape=(None, 64)\n",
    "\n",
    "        # Decode the noise (guided by labels) to fake images.\n",
    "        #print(f\"Random vector labels shape: {random_vector_labels.shape}\")\n",
    "        generated_images = self.generator(random_vector_labels) # TASPCNN: shape=(None, 20, 20, 1)\n",
    "        #print(f\"Generated images shape: {generated_images.shape}\")\n",
    "\n",
    "        # Combine them with real images. Note that we are concatenating the labels\n",
    "        # with these images here.\n",
    "        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n",
    "        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n",
    "        combined_images = tf.concat(\n",
    "            [fake_image_and_labels, real_image_and_labels], axis=0\n",
    "        )\n",
    "\n",
    "        # Assemble labels discriminating real from fake images.\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Train the discriminator.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space.\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_vector_labels = tf.concat(\n",
    "            [random_latent_vectors, one_hot_labels], axis=1\n",
    "        )\n",
    "\n",
    "        # Assemble labels that say \"all real images\".\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator(random_vector_labels)\n",
    "            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n",
    "            predictions = self.discriminator(fake_image_and_labels)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        # Monitor loss.\n",
    "        self.gen_loss_tracker.update_state(g_loss)\n",
    "        self.disc_loss_tracker.update_state(d_loss)\n",
    "        return {\n",
    "            \"g_loss\": self.gen_loss_tracker.result(),\n",
    "            \"d_loss\": self.disc_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc6df4-982b-49e8-8649-8da2c48a5af1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1D-Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a8732-da51-49e8-92d5-e27a67fc56a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8eaad-10d3-46ae-89b9-be1f28028268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# lr_init = 0.1\n",
    "\n",
    "# convolution_1d = models.Sequential()\n",
    "# convolution_1d.add(layers.Conv1D(256, 3, strides = 1, activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "# convolution_1d.add(layers.BatchNormalization())\n",
    "# convolution_1d.add(layers.Conv1D(256, 3, strides = 1, activation='relu', padding='same'))\n",
    "# convolution_1d.add(layers.BatchNormalization())\n",
    "# convolution_1d.add(layers.Conv1D(256, 3, strides = 1, activation='relu', padding='same'))\n",
    "# convolution_1d.add(layers.BatchNormalization())\n",
    "# convolution_1d.add(layers.Conv1D(256, 3, strides = 1, activation='relu', padding='same'))\n",
    "# convolution_1d.add(layers.Flatten())\n",
    "# convolution_1d.add(layers.BatchNormalization())\n",
    "# convolution_1d.add(layers.Dense(units=128))\n",
    "# convolution_1d.add(layers.BatchNormalization())\n",
    "# convolution_1d.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# convolution_1d.compile(\n",
    "#     optimizer=Adam(learning_rate = lr_init, epsilon=1e-06),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "#   )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294e009-1114-46bc-b794-8e3d3e608a72",
   "metadata": {},
   "source": [
    "### 3 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2924f-95ff-40c7-9f3f-f25569fc4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def get_1d_conv(fm_one, fm_two, fm_three, fm_four, dense, num_classes=2, dropout=0.2, learnRate=0.01):\n",
    "    cnn_1d_conv = models.Sequential()\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_one, 3, strides=1, activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_two, 3, strides=1, activation='relu', padding='same'))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_three, 3, strides=1, activation='relu', padding='same'))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_four, 3, strides=1, activation='relu', padding='same'))\n",
    "    cnn_1d_conv.add(layers.Flatten())\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Dense(units=dense))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    cnn_1d_conv.compile(\n",
    "        optimizer=Adam(learning_rate = learnRate, epsilon=1e-06),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "    )\n",
    "    \n",
    "    return cnn_1d_conv\n",
    "\n",
    "# convolution_1d.compile(\n",
    "#     optimizer=Adam(learning_rate = lr_init, epsilon=1e-06),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "#   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d40ff-a460-4a5b-9de9-669c7f84981d",
   "metadata": {},
   "source": [
    "### 2 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f48541-4d30-4fe5-ad18-60444ea823c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "def get_1d_conv(fm_one, fm_two, fm_three, fm_four, dense,  num_classes=2, dropout=0.2, learnRate=0.01):\n",
    "    cnn_1d_conv = models.Sequential()\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_one, 3, strides=1, activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_two, 3, strides=1, activation='relu', padding='same'))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_three, 3, strides=1, activation='relu', padding='same'))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Conv1D(fm_four, 3, strides=1, activation='relu', padding='same'))\n",
    "    cnn_1d_conv.add(layers.Flatten())\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Dense(units=dense))\n",
    "    cnn_1d_conv.add(layers.BatchNormalization())\n",
    "    cnn_1d_conv.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    cnn_1d_conv.compile(\n",
    "        optimizer=Adam(learning_rate = learnRate, epsilon=1e-06),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "    )\n",
    "    \n",
    "    return cnn_1d_conv\n",
    "\n",
    "# convolution_1d.compile(\n",
    "#     optimizer=Adam(learning_rate = lr_init, epsilon=1e-06),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "#   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8ea062-c0b7-4486-9ebe-94ae66a58b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TASP-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05497b70-0400-4219-8a07-3a43005066cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_init = 0.0001\n",
    "\n",
    "# tasp_cnn = models.Sequential()\n",
    "# tasp_cnn.add(layers.Conv2D(128, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "# tasp_cnn.add(layers.Dropout(.2,  input_shape=(3, 3, 128)))\n",
    "# tasp_cnn.add(layers.BatchNormalization())\n",
    "# tasp_cnn.add(layers.Conv2D(1024, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 128)))\n",
    "# tasp_cnn.add(layers.Dropout(.2,  input_shape=(3, 3, 128)))\n",
    "# tasp_cnn.add(layers.BatchNormalization())\n",
    "# tasp_cnn.add(layers.Conv2D(512, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 1024)))\n",
    "# tasp_cnn.add(layers.Dropout(.2,  input_shape=(3, 3, 32)))\n",
    "# tasp_cnn.add(layers.BatchNormalization())\n",
    "# tasp_cnn.add(layers.Conv2D(1024, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, 512)))\n",
    "# tasp_cnn.add(layers.Dropout(.2,  input_shape=(3, 3, 1024)))\n",
    "# tasp_cnn.add(layers.Flatten())\n",
    "# tasp_cnn.add(layers.BatchNormalization())\n",
    "# tasp_cnn.add(layers.Dense(units=32))\n",
    "# tasp_cnn.add(layers.Dropout(.5))\n",
    "# tasp_cnn.add(layers.BatchNormalization())\n",
    "# tasp_cnn.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# tasp_cnn.compile(\n",
    "#     optimizer=Adam(learning_rate = lr_init, epsilon=1e-06),\n",
    "#     loss='categorical_crossentropy',\n",
    "#     metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "#   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69459038-c699-4c38-b80a-60e5a99ccf3d",
   "metadata": {},
   "source": [
    "### 3 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18c896-28d5-4da1-877e-2e13bc20879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tasp_cnn(fm_one, fm_two, fm_three, fm_four, fm_five, fm_six, dense, dropout=0.2, learnRate=0.01):\n",
    "def get_tasp_cnn(fm_one, fm_two, fm_three, fm_four, dense,  num_classes=2, dropout=0.2, learnRate=0.01):\n",
    "    tasp_cnn = models.Sequential()\n",
    "    tasp_cnn.add(layers.Conv2D(fm_one, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Conv2D(fm_two, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, fm_one)))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Conv2D(fm_three, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, fm_two)))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Conv2D(fm_four, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, fm_three)))\n",
    "    tasp_cnn.add(layers.Flatten())\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Dense(units=dense))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    tasp_cnn.compile(\n",
    "        optimizer=Adam(learning_rate = learnRate, epsilon=1e-06),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "      )\n",
    "    \n",
    "    return tasp_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c88ab-8b6a-4b3d-b7e0-1762f847eb55",
   "metadata": {},
   "source": [
    "### 2 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a2871-0939-4799-86b2-237c8c0253c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tasp_cnn(fm_one, fm_two, fm_three, fm_four, fm_five, fm_six, dense, dropout=0.2, learnRate=0.01):\n",
    "def get_tasp_cnn(fm_one, fm_two, fm_three, fm_four, dense,  num_classes=2, dropout=0.2, learnRate=0.01):\n",
    "    tasp_cnn = models.Sequential()\n",
    "    tasp_cnn.add(layers.Conv2D(fm_one, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(5, 5, 1)))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Conv2D(fm_two, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, fm_one)))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Conv2D(fm_three, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, fm_two)))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Conv2D(fm_four, (3, 3), strides=(1, 1), activation='relu', padding='same', input_shape=(3, 3, fm_two)))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Flatten())\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Dense(units=dense))\n",
    "    tasp_cnn.add(layers.BatchNormalization())\n",
    "    tasp_cnn.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    tasp_cnn.compile(\n",
    "        optimizer=Adam(learning_rate = learnRate, epsilon=1e-06),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold=0.1)]\n",
    "      )\n",
    "    \n",
    "    return tasp_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99ef66-1c52-47f9-851f-2cab1406b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasp_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d457bb-2db3-4b69-b0b7-bcf867dbeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ea9112-c2a3-474d-87a5-ba64c599ab56",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a5be32-4a2d-457d-bd39-5ab2da01a809",
   "metadata": {
    "tags": []
   },
   "source": [
    "### F1-Score History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa83e3-3b42-4e84-ae09-087c65298e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_score_history(f1_score_path, f1_score_name, history):\n",
    "    figure_name = plt.figure(figsize=(20, 10))\n",
    "\n",
    "    history.history['f1_score'].insert(0, 0)\n",
    "    history.history['val_f1_score'].insert(0, 0)\n",
    "\n",
    "    plt.plot(history.history['f1_score'], label='F1 score (training data)')\n",
    "    plt.plot(history.history['val_f1_score'], label='F1 score (validation data)')\n",
    "    plt.title('F1 score')\n",
    "    plt.ylabel('F1 score value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.savefig(f1_score_path + f1_score_name)\n",
    "    plt.show()\n",
    "    \n",
    "    print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529e0fc-6e5c-46aa-8029-d053d4df3d88",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcc399-bc1c-490d-b537-3fd46d4e49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def plot_classification_report(path, file_name, y_true, y_predicted):\n",
    "\n",
    "    labels = one_hot_to_casualty(y_true)\n",
    "\n",
    "    report = classification_report(y_true,\n",
    "                                   y_predicted,\n",
    "                                   target_names = labels.unique(),\n",
    "                                   output_dict  = True)\n",
    "\n",
    "    report_df = pd.DataFrame(report).transpose().round(3)\n",
    "    report_df.to_csv(path + file_name, index = True)\n",
    "\n",
    "    print(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9703d-abc2-42b0-8dff-31a2a715815e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46827f-b218-49c7-9902-4337c1457d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(path, file_name, y_true, y_predicted):\n",
    "\n",
    "    cm = confusion_matrix(y_true,\n",
    "                          y_predicted,\n",
    "                          labels = y_true.unique())\n",
    "\n",
    "    labels = one_hot_to_casualty(y_true)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm,\n",
    "                                  display_labels = labels.unique()).plot()\n",
    "\n",
    "    plt.savefig(path + file_name, dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf262a8-e7db-40c1-9cca-eeb101e191c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classification_report_and_confussion_matrix(model_name, model_timestamp, y_true, y_predicted, data):\n",
    "    \n",
    "    \n",
    "    report_path = f\"{REPORTS_PATH}{model_name}/{data}/\"\n",
    "    report_name = f\"{city_name}_{MODEL_NAME}_report_{model_timestamp}.csv\"\n",
    "\n",
    "\n",
    "    plot_classification_report(path = report_path,\n",
    "                               file_name = report_name,\n",
    "                               y_true = y_true,\n",
    "                               y_predicted = y_predicted)\n",
    "\n",
    "\n",
    "    confussion_matrix_path = f\"{CONFUSIONS_MATRIX_PATH}{model_name}/{data}/\"\n",
    "    confussion_matrix_name = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{model_timestamp}.svg\"\n",
    "\n",
    "    plot_confusion_matrix(path = confussion_matrix_path,\n",
    "                          file_name = confussion_matrix_name,\n",
    "                          y_true = y_true,\n",
    "                          y_predicted = y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271fbe6a-965a-48b3-902d-384140c96fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Donee!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oMQbX1j_zVOO",
   "metadata": {
    "id": "oMQbX1j_zVOO",
    "tags": []
   },
   "source": [
    "# Madrid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pG-PDMY6zdqE",
   "metadata": {
    "id": "pG-PDMY6zdqE",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Importación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlkJKFfzBd8g",
   "metadata": {
    "id": "wlkJKFfzBd8g"
   },
   "source": [
    "- [Web Dataset](https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=7c2843010d9c3610VgnVCM2000001f4a900aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default)\n",
    "\n",
    "- [Web documentación](https://datos.madrid.es/FWProjects/egob/Catalogo/Seguridad/Ficheros/Estructura_DS_Accidentes_trafico_desde_2019.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f204c-3787-4461-8a0d-b787570d7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_timestamp = '2022-05-24-11:01:39'\n",
    "model_version = '2022-05-17-20:07:36'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ic0tgQy47zEr",
   "metadata": {
    "id": "Ic0tgQy47zEr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data import read_madrid_data\n",
    "\n",
    "\n",
    "city_name = 'madrid'\n",
    "city = madrid\n",
    "\n",
    "# root_path = '/content/drive/Othercomputers/Mi portátil/Drive/Master UA/TFM/Incidentes de Trafico/Datasets/Madrid/'\n",
    "\n",
    "root_path = './Data/Madrid/'\n",
    "\n",
    "data_frame = read_madrid_data(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5PmJpoCCcxMJ",
   "metadata": {
    "id": "5PmJpoCCcxMJ"
   },
   "source": [
    "### Calcular Vehículos implicados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utgDSVryALnm",
   "metadata": {
    "id": "utgDSVryALnm"
   },
   "source": [
    "A partir del número de expediente (un mismo expediente en varias filas quiere decir que se trata del mismo accidente) se hace un `groupby` a partir de él. Como el atributo `positiva_alcohol` no tiene valores nulos en ninguna de las filas, hacemos un conteo a partir de él y se asigna a una nueva columna `positiva_alcohol_rename` que posteriormente será renombrada como `vehiculos_implicados`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qy9UBWGFan1j",
   "metadata": {
    "id": "qy9UBWGFan1j"
   },
   "outputs": [],
   "source": [
    "data_frame = data_frame.join(data_frame.groupby('num_expediente')['positiva_alcohol'].count(), on='num_expediente', rsuffix='_rename')\n",
    "data_frame.rename(columns={\"positiva_alcohol_rename\": \"vehiculos_implicados\"}, errors=\"raise\", inplace=True)\n",
    "data_frame = data_frame.reset_index(drop=True)\n",
    "# data_frame.localizacion.unique()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3lX-EnJ1aby",
   "metadata": {
    "id": "e3lX-EnJ1aby",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ybjvOI7x0PKz",
   "metadata": {
    "id": "ybjvOI7x0PKz",
    "tags": []
   },
   "source": [
    "### Clasificación de carreteras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c43193-d332-4c5e-a173-ce0877ad9ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ######################### SIGUIENTE CELDA #########################\n",
    "\n",
    "# # Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# # A, A(M) y Motorway lo mismo?\n",
    "# # B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "# #               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "# #               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "# #               ¿Carretera como tal?\n",
    "\n",
    "# # C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "# #               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "\n",
    "# # Unclassified\n",
    "\n",
    "\n",
    "regex = {}\n",
    "regex['parking'] = 'PK|P.K|parking'\n",
    "regex['aeropuerto_regex'] = 'AEROPUERTO|AEROP'\n",
    "regex['cuesta_regex'] = 'CUSTA|CUESTA'\n",
    "regex['paseo_regex'] = 'PASEO|paseo'\n",
    "regex['parque_regex'] = 'PARQUE|PQUE|RETIRO'\n",
    "regex['tunel_regex'] = 'TUNEL|TÚNEL'\n",
    "regex['poligono_regex'] ='POLIGONO'\n",
    "regex['camino_regex']= 'CMNO|CAMINO'\n",
    "regex['ronda_regex'] = 'RONDA'\n",
    "regex['rotonda_regex'] = 'GTA|gta|GLORIETA|glorieta|ROTONDA|FUENT'\n",
    "regex['puerta_regex'] = 'PUERTA|PTA|Puerta'\n",
    "regex['puente_regex'] = 'PNTE|PUENTE'\n",
    "regex['plaza_regex'] = 'PLAZA|PZA'\n",
    "regex['bulevard_regex'] = 'BULE'\n",
    "regex['travesia_regex'] = 'TRVA'\n",
    "regex['calzada_regex'] = 'CALZADA'\n",
    "regex['road_regex'] = 'CTRA.|CARRETERA|carretera|CRA.' # B\n",
    "regex['avenida_regex'] = 'AVDA|AV|AVENIDA|AVDA|avenida|Avda.'\n",
    "regex['highway_regex'] = 'AUTOV.|autovia|A-|M-|M 30|m 30|A\\\\d|M 23|M23|KILOMETRO' # A,A(M),Motorway\n",
    "regex['calle_regex']  = 'CALL.|Calle|CALLE|c/|C/|C.|calle'\n",
    "\n",
    "data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "for index,regex_values in enumerate(regex.values()):\n",
    "    \n",
    "    print(regex_values)\n",
    "    regex_indexes = data_frame[data_frame.localizacion.str.contains(regex_values,  case = True, regex=True)].index\n",
    "    print(len(regex_indexes))\n",
    "    data_frame.iloc[regex_indexes, data_frame.columns.get_loc('tipo_via')] = str(index)\n",
    "    data_frame.iloc[regex_indexes, data_frame.columns.get_loc('localizacion')] = str(index)\n",
    "    \n",
    "    \n",
    "    \n",
    "# # street_indexes  = data_frame[data_frame.localizacion.str.contains('CALL.|Calle|CALLE|c/|C/|C.|calle', case = True, regex=True)].index\n",
    "# # highway_indexes = data_frame[data_frame.localizacion.str.contains(highway_regex, case = True, regex=True)].index\n",
    "# # road_indexes    = data_frame[data_frame.localizacion.str.contains(road_regex, case = True, regex=True)].index\n",
    "# # # avenue_indexes  = data_frame[data_frame.localizacion.str.contains(avenue_regex,  case = True, regex=True)].index\n",
    "# # # ride_indexes    = data_frame[data_frame.localizacion.str.contains(ride_regex, case = True, regex=True)].index\n",
    "\n",
    "# # data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "# # data_frame.iloc[street_indexes,  data_frame.columns.get_loc('tipo_via')] = 'Unclassified'\n",
    "# # data_frame.iloc[highway_indexes, data_frame.columns.get_loc('tipo_via')] = 'A'\n",
    "# # data_frame.iloc[road_indexes, data_frame.columns.get_loc('tipo_via')] = 'B'\n",
    "# # # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "# # # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "\n",
    "\n",
    "# # data_frame.iloc[highway_indexes, data_frame.columns.get_loc('localizacion')] = 1\n",
    "# # data_frame.iloc[road_indexes, data_frame.columns.get_loc('localizacion')] = 2\n",
    "# # data_frame.iloc[street_indexes,  data_frame.columns.get_loc('localizacion')] = 3\n",
    "# # # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('localizacion')] = '3'\n",
    "# # # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('localizacion')] = '5'\n",
    "\n",
    "# positive_drug_indexes = data_frame[data_frame.positiva_droga == 1].index\n",
    "# data_frame.iloc[positive_drug_indexes, data_frame.columns.get_loc('positiva_alcohol')] = 'S'\n",
    "\n",
    "# data_frame = data_frame[~(data_frame.tipo_via == 'N/A')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2095094-b3b4-46ba-ae44-2225d3068135",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_assigned_location_values = data_frame[~data_frame.localizacion.str.isnumeric()].index\n",
    "data_frame.loc[index_of_assigned_location_values, 'localizacion'] = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da252aZ0N3n",
   "metadata": {
    "id": "7da252aZ0N3n"
   },
   "outputs": [],
   "source": [
    "# ######################### SIGUIENTE CELDA #########################\n",
    "\n",
    "# # Unclassified: Carreteras locales sin destino definido. Sin embargo, los destinos locales pueden estar señalizados a lo largo de ellos.\n",
    "# # A, A(M) y Motorway lo mismo?\n",
    "# # B:            De carácter regional y utilizado para conectar zonas de menor importancia.\n",
    "# #               Por lo general, se muestran de color marrón o amarillo en los mapas y tienen las mismas señales blancas que las rutas de clase A que no son primarias.\n",
    "# #               Si la ruta es primaria, como la B6261, se mostrará igual que una ruta Clase A primaria.\n",
    "# #               ¿Carretera como tal?\n",
    "\n",
    "# # C:            Designaciones de autoridades locales para rutas dentro de su área con fines administrativos.\n",
    "# #               Estas rutas no se muestran en mapas de carreteras a pequeña escala, pero se sabe que ocasionalmente aparecen en las señales de tráfico.\n",
    "\n",
    "# # Unclassified\n",
    "# street_regex  = ('CALL.|Calle|CALLE|c/|C/|C.|calle|'\n",
    "#                  'AVDA|AV|AVENIDA|AVDA|avenida|Avda.|'\n",
    "#                  'PASEO|paseo|'\n",
    "#                  'PARQUE|PQUE|'\n",
    "#                  'RONDA|'\n",
    "#                  'PUERTA|PTA|Puerta|'\n",
    "#                  'PNTE|PUENTE|'\n",
    "#                  'PLAZA|PZA|'\n",
    "#                  'CMNO|CAMINO|'\n",
    "#                  'BULE|'\n",
    "#                  'TRVA|'\n",
    "#                  'CUSTA|CUESTA|'\n",
    "#                  'GTA|gta|GLORIETA|glorieta|ROTONDA|'\n",
    "#                  'AEROPUERTO|AEROP'\n",
    "# )\n",
    "\n",
    "# highway_regex = 'AUTOV.|autovia|A-|M-|M 30|m 30|A\\\\d|M 23|M23' # A,A(M),Motorway\n",
    "# road_regex = 'CTRA.|CARRETERA|carretera|CRA.|CALZADA|POLIGONO' # B\n",
    "\n",
    "# street_indexes  = data_frame[data_frame.localizacion.str.contains(street_regex,  case = True, regex=True)].index\n",
    "# highway_indexes = data_frame[data_frame.localizacion.str.contains(highway_regex, case = True, regex=True)].index\n",
    "# road_indexes    = data_frame[data_frame.localizacion.str.contains(road_regex, case = True, regex=True)].index\n",
    "# # avenue_indexes  = data_frame[data_frame.localizacion.str.contains(avenue_regex,  case = True, regex=True)].index\n",
    "# # ride_indexes    = data_frame[data_frame.localizacion.str.contains(ride_regex, case = True, regex=True)].index\n",
    "\n",
    "# data_frame['tipo_via'] = 'N/A'\n",
    "\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('tipo_via')] = 'Unclassified'\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('tipo_via')] = 'A'\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('tipo_via')] = 'B'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('tipo_via')] = 'AVENIDA'\n",
    "\n",
    "\n",
    "# data_frame.iloc[highway_indexes, data_frame.columns.get_loc('localizacion')] = 1\n",
    "# data_frame.iloc[road_indexes, data_frame.columns.get_loc('localizacion')] = 2\n",
    "# data_frame.iloc[street_indexes,  data_frame.columns.get_loc('localizacion')] = 3\n",
    "# # data_frame.iloc[avenue_indexes,  data_frame.columns.get_loc('localizacion')] = '3'\n",
    "# # data_frame.iloc[ride_indexes, data_frame.columns.get_loc('localizacion')] = '5'\n",
    "\n",
    "# # positive_drug_indexes = data_frame[data_frame.positiva_droga == 1].index\n",
    "# # data_frame.iloc[positive_drug_indexes, data_frame.columns.get_loc('positiva_alcohol')] = 'S'\n",
    "\n",
    "# data_frame = data_frame[~(data_frame.tipo_via == 'N/A')]\n",
    "# # print(data_frame.localizacion.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc350245-b09e-4de1-9ecb-3d2f8d60ba7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Consideraciones:\n",
    "\n",
    "- Los patinetes se han considerado como ciclomotres de menos de 50cc.\n",
    "- Las furgonetas se consideran como vehículos de menos de 3.5 toneladas.\n",
    "- Maquinaria de obras se considera la misma tipología que maquinaria agrícola.\n",
    "- Cuadriciclos ligeros y no ligeros se consideran como `Motorcycle-Unknown CC`.\n",
    "- Patinetes y Vehículos de Mobilidad Urbana se consideran como `Mobility Scooters`.\n",
    "- `Vehículo articulado` se considera como un vehículo de más de 7.5 toneladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RwdUhUHc1Up4",
   "metadata": {
    "id": "RwdUhUHc1Up4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.data import categorize_features\n",
    "\n",
    "# YES\n",
    "data_frame = categorize_features(data_frame) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90795a91-8746-45e9-ab74-a76721899889",
   "metadata": {},
   "source": [
    "### Añadir dia semana y semana en año"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fe6c6b-d239-4a04-afdc-2e02d9fa6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import get_to_iso_calendar\n",
    "\n",
    "# YES\n",
    "data_frame['dia_semana'] = data_frame.apply(lambda row: get_to_iso_calendar(row).weekday, axis=1)\n",
    "# YES\n",
    "data_frame['semana_en_año'] = data_frame.apply(lambda row: get_to_iso_calendar(row).week, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pVPFGQ0AoNRD",
   "metadata": {
    "id": "pVPFGQ0AoNRD",
    "tags": []
   },
   "source": [
    "### Coordenadas UTM a números enteros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nabg28LMAHhW",
   "metadata": {
    "id": "nabg28LMAHhW"
   },
   "source": [
    "Las coordenadas UTM son coordenads que están expresadas en término de X e Y partiendo de la base de que parten desde una determinada localización. Estas coordenadas constan de una parte entera y una decimal.\n",
    "\n",
    "En este dataset el formato que presentan estas coordenadas pueden ser de tres tipos:\n",
    "\n",
    "- **XXX.XXX.XXX**: en este caso los seis primeros dígitos forman la parte entera y los tres útlimos la parte decimal.\n",
    "- **XXXXXX,XX**: los seis primeros dígitos indican la parte entera, mientras que tras la coma aparecen dos dígitos de la parte decimal que habrá que completar añadiendo uno más.\n",
    "- **XXXXXX**: indican la parte entera, sin contar con la parte decimal.\n",
    "\n",
    "Por lo que el objetivo es estandarizar todos los formatos convirtiendo cada una de las coordenadas a un número entero, siendo necesario tratar con cada una de las casuísticas para añadir ceros a la derecha en caso de que falten para que cada una de las coordenadas tenga la misma longitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sgVHBwC0Fd1N",
   "metadata": {
    "id": "sgVHBwC0Fd1N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.data import utm_to_int\n",
    "\n",
    "# YES\n",
    "data_frame = utm_to_int(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d7b12-6ba9-4f03-89bb-a383531fb266",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import contextily as cx\n",
    "\n",
    "# gdf = gpd.GeoDataFrame(data_frame,\n",
    "#                        geometry = gpd.points_from_xy(data_frame.Longitude, data_frame.Latitude),\n",
    "#                        crs = \"EPSG:4326\")\n",
    "# df_wm = gdf.to_crs(epsg=3857)\n",
    "# ax = df_wm.plot(figsize=(20, 20), column='lesividad', edgecolor=\"k\", legend=True)\n",
    "# cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Z4nz3ioxtXb",
   "metadata": {
    "id": "_Z4nz3ioxtXb",
    "tags": []
   },
   "source": [
    "### Renombrado y eliminación de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tqnlSOcN71Ah",
   "metadata": {
    "id": "tqnlSOcN71Ah"
   },
   "outputs": [],
   "source": [
    "# COLUMNS_TO_REMOVE = ['num_expediente', 'fecha', 'tipo_via', 'numero', 'positiva_droga', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_droga']\n",
    "# SIN LOCALIZACION COLUMNS_TO_REMOVE = ['num_expediente', 'fecha', 'tipo_via', 'localizacion', 'numero', 'positiva_droga', 'coordenada_x_utm', 'coordenada_y_utm', 'positiva_droga']\n",
    "from src.data import remove_features\n",
    "\n",
    "# YES\n",
    "data_frame = remove_features(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae87450-809a-4958-b9cb-422a3e91effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data_frame = data_frame.loc[:, ~data_frame.columns.isin(['lesividad'])]\n",
    "# Y_data_frame = data_frame['lesividad']\n",
    "\n",
    "# X_data_frame  = X_data_frame.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8c589-4e2b-4340-8dd5-44c665a642e0",
   "metadata": {},
   "source": [
    "## Eliminar regiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2510c89-0dac-44f9-8e15-ad1b6c61034f",
   "metadata": {},
   "source": [
    "Se proyectarán los accidentes en dos dimensiones en función de sus coordenadas. Esta proyección se dividirá en n sub-áreas iguales en función de las coordenadas que estas presenten.\n",
    "Se eliminarán todos aquellos accidentes leves que no estén contenidos en algún sub-área en la que se encuentre algún accidente leve o serio, de tal forma que se reducirá considerablemente el número de accidentes leves, evitando así el sobreajuste de la red sobre esta clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cd993-ab79-408f-b49c-bdfba9da1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slight = data_frame[data_frame.lesividad == 'Slight'][:1000]\n",
    "# serious = data_frame[data_frame.lesividad == 'Serious'][:100]\n",
    "# fatal = data_frame[data_frame.lesividad == 'Fatal']\n",
    "\n",
    "# import matplotlib\n",
    "\n",
    "# matplotlib.rc('figure', figsize=(30, 15))\n",
    "\n",
    "# x = fatal.coordenada_x_utm\n",
    "# y = fatal.coordenada_y_utm\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "# colour = np.arctan2(x, y)\n",
    "\n",
    "\n",
    "# plt.scatter(slight.coordenada_x_utm, slight.coordenada_y_utm, s = 50, alpha = 1)\n",
    "# plt.scatter(serious.coordenada_x_utm, serious.coordenada_y_utm, s = 50, alpha = 1)\n",
    "# plt.scatter(x, y, s = 50, alpha = 1)\n",
    "\n",
    "\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccb6c0-53d6-4e40-868a-40d82fd03bef",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c74a52-0c97-44ad-bb13-75ac540d0980",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = {'Serious': 'Assistance',\n",
    "          'Fatal': 'Assistance'}\n",
    "\n",
    "data_frame.replace(transf, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50011124-e555-4503-a222-1ecd1e8a68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "x_name = 'coordenada_x_utm'\n",
    "y_name = 'coordenada_y_utm'\n",
    "\n",
    "data_frame[x_name] = data_frame[x_name].astype(int)\n",
    "data_frame[y_name] = data_frame[y_name].astype(int)\n",
    "\n",
    "casualty_name = 'lesividad'\n",
    "\n",
    "casualty_target_names = ['Assistance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c868d5-7974-45e0-a673-6145c8c1f701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "from src.data import remove_outliers\n",
    "\n",
    "data_frame = remove_outliers(data_frame, x_name, y_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad72a51-b24f-4be5-b417-a059db338782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.data import get_intervals\n",
    "from src.data import get_divisible_numbers\n",
    "\n",
    "interval_x, interval_y = get_intervals(data_frame, x_name, y_name)\n",
    "\n",
    "print('X')\n",
    "get_divisible_numbers(interval_x)\n",
    "print('Y')\n",
    "get_divisible_numbers(interval_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6d527-cf39-4e70-9a5b-1a7f896711d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_load = f'{city_name}_filtered_iii_paper.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f10d6-405c-443e-b694-539c81abf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import get_rows_by_removing_areas\n",
    "\n",
    "\n",
    "x_offset = 15\n",
    "y_offset = 14\n",
    "\n",
    "new_dataframe = get_rows_by_removing_areas(data_frame,\n",
    "                                           x_name = x_name,\n",
    "                                           y_name = y_name,\n",
    "                                           x_offset = x_offset,\n",
    "                                           y_offset = y_offset,\n",
    "                                           casualty_name = casualty_name,\n",
    "                                           casualty_target_names = casualty_target_names)\n",
    "new_dataframe.to_csv(filename_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b963b1d-da10-4108-a3ec-4bfa7ce8f966",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import contextily as cx\n",
    "import numpy as np\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70af350-a281-4d18-bf06-3c4f3263654a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_dataframe_2 = new_dataframe[new_dataframe.lesividad == 'Serious']\n",
    "# gdf = gpd.GeoDataFrame(new_dataframe_2,\n",
    "#                        geometry = gpd.points_from_xy(new_dataframe_2.Longitude, new_dataframe_2.Latitude),\n",
    "#                        crs = \"EPSG:4326\")\n",
    "# df_wm = gdf.to_crs(epsg=3857)\n",
    "# ax = df_wm.plot(figsize=(20, 20), column='lesividad', edgecolor=\"k\", legend=True)\n",
    "# cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a107f-6a74-4055-9eb1-c9b0614b9ee4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gdf = gpd.GeoDataFrame(new_dataframe,\n",
    "#                        geometry = gpd.points_from_xy(new_dataframe.Longitude, new_dataframe.Latitude),\n",
    "#                        crs = \"EPSG:4326\")\n",
    "# df_wm = gdf.to_crs(epsg=3857)\n",
    "# ax = df_wm.plot(figsize=(20, 20), column='lesividad', edgecolor=\"k\", legend=True)\n",
    "# cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386e7c4-b258-4c77-a08a-12c84e363d7e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_wm.explore(column='lesividad', legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64e7aa-98f2-45ac-9a40-a464f451faca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b2b9e-72a9-4f69-9042-c782a08fa911",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from src.map_visualization import build_grid\n",
    "\n",
    "# x_offset = 15\n",
    "# y_offset = 16\n",
    "\n",
    "# grid = build_grid(gdf, x_offset, y_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173617e-5470-4d72-af51-9e61b0d943d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ax = gdf.plot(markersize=.1, figsize=(12, 8), column='lesividad', cmap='jet')\n",
    "# plt.autoscale(False)\n",
    "# cell.plot(ax=ax, facecolor=\"none\", edgecolor='grey')\n",
    "# ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbaf98d-7f90-4d5b-af37-c9cfa58380b3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell[0:1].geometry[0].contains(gdf[0:1].geometry) --> False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b2f5a-addf-4537-bd60-2ad5fecb730d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b6a2c-42b0-447b-889b-6864a6376e1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gdf = gpd.GeoDataFrame(new_dataframe,\n",
    "#                        geometry = gpd.points_from_xy(new_dataframe.Longitude, new_dataframe.Latitude),\n",
    "#                        crs = \"EPSG:4326\")\n",
    "# df_wm = gdf.to_crs(epsg=3857)\n",
    "# # ax = df_wm.plot(figsize=(20, 20), column='lesividad', edgecolor=\"k\", legend=True)\n",
    "# grid.plot(facecolor=\"none\", edgecolor='grey')\n",
    "# cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a13f8f-f7c1-489b-8828-455c3d29f509",
   "metadata": {},
   "source": [
    "# END TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1607b650-8adc-44d8-8672-d0e902879980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.data import get_rows_by_removing_areas\n",
    "\n",
    "# # data_frame = get_rows_by_removing_areas(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22fddf-171c-45a1-81ab-ad91d3c3a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_frame.to_csv('Data/Madrid/filtered_areas_2.csv')\n",
    "# data_frame = pd.read_csv('Data/Madrid/filtered_areas_2.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3882ab-b368-4e71-9a4e-1365add2dbaf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.histplot(data=data_frame.lesividad,stat='count')\n",
    "# plt.savefig('histograms_images/original.svg')\n",
    "# one_hot_to_casualty(Y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755814a8-cd7a-4dee-a7e3-8c15b3932b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qKuGQ1I8078E",
   "metadata": {
    "id": "qKuGQ1I8078E",
    "tags": []
   },
   "source": [
    "## Split de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce67dd-a875-441f-9ba1-4f276cd34ea5",
   "metadata": {},
   "source": [
    "Histograma de desbalanceo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51959888-c8ce-41f6-a3c2-07ae974dff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCcJF3i8s3dD",
   "metadata": {
    "id": "NCcJF3i8s3dD"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train, test = train_test_split(data_frame, test_size=0.2, random_state = 2)\n",
    "X_train = X_train_original = train.loc[:, ~train.columns.isin(['lesividad'])]\n",
    "\n",
    "X_train = X_train.astype(int)\n",
    "X_train_original = X_train_original.astype(int)\n",
    "\n",
    "Y_train = Y_train_original = train['lesividad']\n",
    "\n",
    "X_test = test.loc[:, ~test.columns.isin(['lesividad'])]\n",
    "X_test = X_test.astype(int)\n",
    "Y_test = test['lesividad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833572d7-b380-4b6e-ae9b-ed5b13c0cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.histplot(data=one_hot_to_casualty(Y_train),stat='count')\n",
    "plt.savefig('histograms_images/original.svg')\n",
    "one_hot_to_casualty(Y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x2PcFjlBmTlC",
   "metadata": {
    "id": "x2PcFjlBmTlC"
   },
   "outputs": [],
   "source": [
    "# # FILE_NAME = f\"{city_name}_calculated_weights.json\"\n",
    "# FILE_NAME = 'madrid_adapted_leeds_default_weights.json'\n",
    "\n",
    "# feature_vector = load_json(WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde99d2d-727a-4e30-90b3-67dc859075bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feature_vector = {}\n",
    "\n",
    "# feature_vector['Accident Features'] = {\n",
    "#     'feature_childs': ['coordenada_x_utm', 'coordenada_y_utm', 'distrito', 'hora', 'vehiculos_implicados'],\n",
    "#     'feature_weights': [0.165774538, 0.171530785, 0.082228259, 0.047771472, 0.060763375]\n",
    "# } \n",
    "\n",
    "# feature_vector['Roadway Features'] = {\n",
    "#     'feature_childs': ['tipo_accidente'], # Road Surface \n",
    "#     'feature_weights': [0.07036541]\n",
    "# }\n",
    "\n",
    "# feature_vector['Environmental Features'] = {\n",
    "#     'feature_childs': ['estado_meteorológico'],\n",
    "#     'feature_weights': [0.04354843]\n",
    "# }\n",
    "\n",
    "# feature_vector['Vehicle Features'] = {\n",
    "#     'feature_childs': ['tipo_vehiculo'],\n",
    "#     'feature_weights': [0.126314657]\n",
    "# }\n",
    "\n",
    "# feature_vector['Casualty Features'] = {\n",
    "#     'feature_childs': ['tipo_persona', 'sexo', 'rango_edad', 'drogas_alcohol_positivo'],\n",
    "#     'feature_weights': [0.067057589, 0.049116389, 0.095220163, 0.059951354]\n",
    "# } \n",
    "# matrix_indexes = fv2gi(feature_vector)\n",
    "\n",
    "# # {'Accident Features': {'feature_childs': ['Easting',\n",
    "# #    'Northing',\n",
    "# #    '1st Road Class',\n",
    "# #    'Accident Time',\n",
    "# #    'Number of Vehicles'],\n",
    "# #   'feature_weights': [0.165774538,\n",
    "# #    0.171530785,\n",
    "# #    0.082228259,\n",
    "# #    0.047771472,\n",
    "# #    0.060763375],\n",
    "# #   'wpi': 0.528068429},\n",
    "# #  'Roadway Features': {'feature_childs': ['Road Surface'],\n",
    "# #   'feature_weights': [0.048847406],\n",
    "# #   'wpi': 0.048847406},\n",
    "# #  'Environmental Features': {'feature_childs': ['Lighting Conditions',\n",
    "# #    'Weather Conditions'],\n",
    "# #   'feature_weights': [0.041826936, 0.04354843],\n",
    "# #   'wpi': 0.08537536600000001},\n",
    "# #  'Vehicle Features': {'feature_childs': ['Type of Vehicle'],\n",
    "# #   'feature_weights': [0.126314657],\n",
    "# #   'wpi': 0.126314657},\n",
    "# #  'Casualty Features': {'feature_childs': ['Casualty Class',\n",
    "# #    'Sex of Casualty',\n",
    "# #    'Age of Casualty'],\n",
    "# #   'feature_weights': [0.067057589, 0.049116389, 0.095220163],\n",
    "# #   'wpi': 0.211394141}}\n",
    "# feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5W4MXdIis6vn",
   "metadata": {
    "id": "5W4MXdIis6vn",
    "tags": []
   },
   "source": [
    "## Normalización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tHUfNlw8sdxS",
   "metadata": {
    "id": "tHUfNlw8sdxS"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test  = X_test.astype(int)\n",
    "\n",
    "X_train_original = X_train_original.astype(int)\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_train_original = normalize_data(X_train_original)\n",
    "X_test  = normalize_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kr_UChBJ21Cu",
   "metadata": {
    "id": "kr_UChBJ21Cu",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Oversampling de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6d2ae-d4d4-4f20-95da-551f452c50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oversampling_methods.taspcnn_cGAN import cGAN\n",
    "\n",
    "# cGAN(X_train,Y_train,10)\n",
    "# cGAN.call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7ec60-725f-45c0-bedb-cd26f543ac9f",
   "metadata": {},
   "source": [
    "### SMOTE-II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rXwHLi842zLs",
   "metadata": {
    "id": "rXwHLi842zLs"
   },
   "outputs": [],
   "source": [
    "# print('********** Before OverSampling **********')\n",
    "# print('Slight: ', (Y_train == 'Slight').sum())\n",
    "# print('Serious:', (Y_train == 'Serious').sum())\n",
    "# print('Fatal:  ', (Y_train == 'Fatal').sum())\n",
    "# print('\\n Total X:', len(X_train), ' Total Y:', len(Y_train), '\\n')\n",
    "\n",
    "# X_train, Y_train = oversample_data(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb8d95-7e6a-4f25-aa33-ee9aa6e82530",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.histplot(data=one_hot_to_casualty(Y_train),stat='count')\n",
    "plt.savefig('histograms_images/smote-ii.svg')\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06540ce8-f30e-45fb-b44f-55e1632951d2",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Downsampling de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f52a26-b2a4-4f5e-a31f-bffaefb8c73d",
   "metadata": {},
   "source": [
    "### Two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf96a1-04f5-4c28-89ff-a69bb6ecc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "slight_data  = test[test['lesividad'] == 'Slight']\n",
    "serious_data = test[test['lesividad'] == 'Assistance']\n",
    "# fatal_data   = test[test['lesividad'] == 'Fatal']\n",
    "\n",
    "X_slight_downsampled = resample(slight_data,\n",
    "                                replace = True,\n",
    "                                n_samples = len(serious_data))\n",
    "\n",
    "# X_serious_downsampled = resample(serious_data,\n",
    "#                                  replace = True,\n",
    "#                                  n_samples = len(fatal_data))\n",
    "\n",
    "\n",
    "downsampled_dataset = pd.concat([X_slight_downsampled, serious_data])\n",
    "\n",
    "downsampled_train, downsampled_test = train_test_split(downsampled_dataset, test_size=0.2)\n",
    "\n",
    "X_train_downsampled = downsampled_train.loc[:, ~downsampled_train.columns.isin(['lesividad'])]\n",
    "Y_train_downsampled = downsampled_train['lesividad']\n",
    "\n",
    "X_test_downsampled = downsampled_test.loc[:, ~downsampled_test.columns.isin(['lesividad'])]\n",
    "Y_test_downsampled = downsampled_test['lesividad']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f5158-bbc7-4205-88ac-d3b0921130b9",
   "metadata": {},
   "source": [
    "### Three Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a11f0e-a930-4fe5-9c60-a12973ac9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.utils import resample\n",
    "\n",
    "# slight_data  = test[test['lesividad'] == 'Slight']\n",
    "# serious_data = test[test['lesividad'] == 'Serious']\n",
    "# fatal_data   = test[test['lesividad'] == 'Fatal']\n",
    "\n",
    "# X_slight_downsampled = resample(slight_data,\n",
    "#                                 replace = True,\n",
    "#                                 n_samples = len(fatal_data))\n",
    "\n",
    "# X_serious_downsampled = resample(serious_data,\n",
    "#                                  replace = True,\n",
    "#                                  n_samples = len(fatal_data))\n",
    "\n",
    "\n",
    "# downsampled_dataset = pd.concat([X_slight_downsampled, X_serious_downsampled, fatal_data])\n",
    "\n",
    "# downsampled_train, downsampled_test = train_test_split(downsampled_dataset, test_size=0.2)\n",
    "\n",
    "# X_train_downsampled = downsampled_train.loc[:, ~downsampled_train.columns.isin(['lesividad'])]\n",
    "# Y_train_downsampled = downsampled_train['lesividad']\n",
    "\n",
    "# X_test_downsampled = downsampled_test.loc[:, ~downsampled_test.columns.isin(['lesividad'])]\n",
    "# Y_test_downsampled = downsampled_test['lesividad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdf9bd-bcdf-4407-a199-87674d07267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# sns.histplot(data=one_hot_to_casualty(Y_test_downsampled),stat='count')\n",
    "# plt.savefig('histograms_images/downsampled-test.svg')\n",
    "# Y_test_downsampled.value_counts()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(20, 10))\n",
    "\n",
    "# sns.set_theme(style=\"whitegrid\")\n",
    "# sns.histplot(data=one_hot_to_casualty(Y_train_downsampled),stat='count')\n",
    "# plt.savefig('histograms_images/downsampled-train.svg')\n",
    "# Y_train_downsampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4e4df-7c2d-48fb-b867-0a7066a57076",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(int)\n",
    "X_test  = X_test.astype(int)\n",
    "\n",
    "X_train_original = X_train_original.astype(int)\n",
    "\n",
    "X_train_downsampled = X_train_downsampled.astype(int)\n",
    "X_test_downsampled  = X_test_downsampled.astype(int)\n",
    "\n",
    "X_train = normalize_data(X_train)\n",
    "X_test  = normalize_data(X_test)\n",
    "X_train_downsampled = normalize_data(X_train_downsampled)\n",
    "X_test_downsampled  = normalize_data(X_test_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ac202-7211-4fd1-837a-6c41752ebefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5eb99c-7ac9-425b-af2e-735ae2155e03",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525349f-2832-457b-833f-e4f2d549ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccda69-9c08-4e7c-b1fb-20ce276379ae",
   "metadata": {},
   "source": [
    "### Genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8a6fb-aef6-4186-aae1-89981e83bd3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if calculate_weights:\n",
    "    Y_train_downsampled_copy = Y_train_downsampled.copy()\n",
    "    Y_test_downsampled_copy  = Y_test_downsampled.copy()\n",
    "    Y_test_copy = Y_test.copy()\n",
    "    Y_val_copy  = Y_val.copy()\n",
    "    Y_train_copy = Y_train.copy()\n",
    "\n",
    "    Y_train_downsampled_onehot = casualty_to_one_hot(Y_train_downsampled_copy)\n",
    "    Y_test_downsampled_onehot  = casualty_to_one_hot(Y_test_downsampled_copy)\n",
    "    Y_val_onehot  = casualty_to_one_hot(Y_val_copy)\n",
    "    Y_test_onehot  = casualty_to_one_hot(Y_test_copy)\n",
    "    Y_train_onehot  = casualty_to_one_hot(Y_train_copy)\n",
    "\n",
    "    populationSize = (number_of_individuals, number_of_hyperparams)\n",
    "    population = initialize_population(number_of_individuals   = number_of_individuals,\n",
    "                                       hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "\n",
    "    fitnessHistory = np.empty([number_of_generations+1, number_of_individuals]) # Define an array to store the value of each parameter for each parent and generation\n",
    "    populationHistory = np.empty([(number_of_generations+1)*number_of_individuals, number_of_hyperparams]) # Insert the value of initial parameters in history\n",
    "\n",
    "    best_solution_history = np.empty([(number_of_generations), number_of_hyperparams+1])\n",
    "    populationHistory[0:number_of_individuals,:] = population\n",
    "\n",
    "    dtrain = xgb.DMatrix(data  = X_train,\n",
    "                         label = Y_train_copy)\n",
    "\n",
    "    dtest  = xgb.DMatrix(data  = X_test, \n",
    "                         label = Y_test_copy)\n",
    "\n",
    "    for generation in range(number_of_generations):\n",
    "\n",
    "        print(\"This is number %s generation\" % (generation))\n",
    "\n",
    "        new_population = []\n",
    "\n",
    "        unique_individuals = np.unique(population, axis=0)\n",
    "\n",
    "        new_individuals_to_create = number_of_individuals - len(unique_individuals)\n",
    "\n",
    "        for i in range(new_individuals_to_create):\n",
    "            new_individual = generate_individual(hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "            new_population.append(new_individual)\n",
    "\n",
    "        new_population = np.array(new_population)\n",
    "\n",
    "        if (new_individuals_to_create):\n",
    "            population = np.concatenate((unique_individuals, new_population), axis=0)\n",
    "\n",
    "        # print(f'Current population is {population}')\n",
    "        print(f'New population is {len(new_population)}')\n",
    "\n",
    "        # Train the dataset and obtain fitness\n",
    "        fitnessValue = train_population(population = population,\n",
    "                                        hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE,\n",
    "                                        dMatrixTrain = dtrain,\n",
    "                                        dMatrixTest = dtest,\n",
    "                                        Y_test = Y_test_copy)\n",
    "\n",
    "        fitnessHistory[generation,:] = fitnessValue\n",
    "\n",
    "        # Best score in the current iteration\n",
    "        max_score_index = np.argmax(fitnessHistory[generation,:])\n",
    "        max_score_value = np.max(fitnessHistory[generation,:])\n",
    "        max_score_solution = population[max_score_index]\n",
    "\n",
    "        max_solution_with_score = []\n",
    "        max_solution_with_score = np.append(max_score_solution, max_score_value)\n",
    "        best_solution_history[generation] = max_solution_with_score\n",
    "\n",
    "        print(f\"Best F1 score in the this iteration = {max_score_value}, best solution {max_score_solution}\") # Survival of the fittest - take the top parents, based on the fitness value and number of parents needed to be selected\n",
    "\n",
    "        parents = new_parents_selection(population = population,\n",
    "                                        fitness = fitnessValue,\n",
    "                                        numParents = numberOfParentsMating)\n",
    "\n",
    "        # Mate these parents to create children having parameters from these parents (we are using uniform crossover)\n",
    "        children = crossover_uniform(parents = parents,\n",
    "                                     childrenSize = (populationSize[0] - parents.shape[0], number_of_hyperparams))\n",
    "\n",
    "        # Add mutation to create genetic diversity\n",
    "        children_mutated = mutation(children,\n",
    "                                    hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE)\n",
    "\n",
    "        '''\n",
    "        We will create new population, which will contain parents that where selected previously based on the\n",
    "        fitness score and rest of them  will be children\n",
    "        '''\n",
    "        population[0:parents.shape[0], :] = parents # Fittest parents\n",
    "        population[parents.shape[0]:, :]  = children_mutated # Children\n",
    "\n",
    "        populationHistory[(generation+1)*number_of_individuals : (generation+1)*number_of_individuals + number_of_individuals , :] = population # Store parent information\n",
    "\n",
    "    #Best solution from the final iteration\n",
    "\n",
    "    fitness = train_population(population = population,\n",
    "                               hyperparams_to_optimize = HYPERPARAMS_TO_OPTIMIZE,\n",
    "                               dMatrixTrain = dtrain,\n",
    "                               dMatrixTest = dtest,\n",
    "                               Y_test = Y_test_copy)\n",
    "\n",
    "    fitnessHistory[generation+1, :] = fitness # index of the best solution\n",
    "    bestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n",
    "\n",
    "\n",
    "    best_hyperparams = {}\n",
    "    for n_param, hyperparam in enumerate(HYPERPARAMS_TO_OPTIMIZE):\n",
    "        best_hyperparams[hyperparam] = population[bestFitnessIndex][n_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cff61d-c0c4-43d1-8aba-f0128c249852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if calculate_weights and madrid:\n",
    "    #### PLOT FITNESS EVOLUTION ####\n",
    "    x_fitness = [np.max(fitnessHistory[i]) for i in range(0,fitnessHistory.shape[0])]\n",
    "\n",
    "    FILE_NAME = f\"{city_name}_ga_{MODEL_TIMESTAMP}.svg\"\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.plot(np.arange(len(x_fitness)), x_fitness)\n",
    "    plt.savefig(GA_SCORES_PATH + FILE_NAME)\n",
    "\n",
    "    #### PLOT HYPERPARAMS EVOLUTION ####\n",
    "    FILE_NAME = f\"{city_name}_ga_hyperparams_evolution_p{number_of_individuals}_c{numberOfParentsMating}_{MODEL_TIMESTAMP}.svg\"\n",
    "\n",
    "    LEGEND_LABELS = HYPERPARAMS_TO_OPTIMIZE.keys()\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    best_solution_history_aux = best_solution_history\n",
    "    best_solution_history_aux[:,1] = best_solution_history[:,1]/2\n",
    "    best_solution_history_aux[:,3] = best_solution_history[:,3]/100\n",
    "    plt.plot(best_solution_history_aux[:,:3])\n",
    "    plt.ylabel('Factor')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.legend(LEGEND_LABELS)\n",
    "    plt.savefig(HYPERPARAMS_EVOLUTON_PATH + FILE_NAME, dpi=300)\n",
    "\n",
    "    FILE_NAME = f\"{city_name}_population_p{number_of_individuals}_c{numberOfParentsMating}_{MODEL_TIMESTAMP}.txt\"\n",
    "\n",
    "    np.savetxt(FINAL_POPULATION_PATH + FILE_NAME, population, fmt='%s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adde09b-86d0-4525-ad0f-55a841e5a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best_solution_history_2 = best_solution_history.copy()\n",
    "# # best_solution_history_2 =  np.append([[0, 0, 0]] ,best_solution_history)\n",
    "\n",
    "FILE_NAME = \"save2.json\"\n",
    "\n",
    "# # np.savetxt(FILE_NAME, best_solution_history, fmt='%s')\n",
    "vector = load_json('./', FILE_NAME)\n",
    "with open(FILE_NAME) as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac216f1-f86f-4d4d-8117-2814e3643637",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_array_to_append = [ [0.02, 4.0, 1.6099999999999999, 0.007437]]*30\n",
    "np_array_to_append = np.asarray(np_array_to_append)\n",
    "data = np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd2eb8-0250-4e02-90e2-1cdac060714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([data, np_array_to_append])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f16908-603e-4d1a-9955-acba7da05813",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = []\n",
    "for one_list in data:\n",
    "    new_array.append(np.asarray(one_list))\n",
    "\n",
    "new_array = np.asarray(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa1101-5ea0-44d2-97e6-9d545a784030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,4):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e608b-fffc-464c-ad41-8947dcf3c1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LEGEND_LABELS = HYPERPARAMS_TO_OPTIMIZE.keys()\n",
    "\n",
    "vector = np.array(data).reshape(-1, 4)\n",
    "\n",
    "vector[:,1] = vector[:,1]/2\n",
    "vector[:,3] = vector[:,3]/100\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "    \n",
    "for i in range(0,3):\n",
    "    plt.plot(vector[:,i])\n",
    "\n",
    "    plt.ylabel('Factor')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.legend(LEGEND_LABELS)\n",
    "    plt.ylim(ymin=0, ymax=12)\n",
    "    plt.xlim(xmin=0, xmax=80)\n",
    "\n",
    "    plt.savefig('new.svg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446f940-aa2e-4580-9e19-53b92378785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48115b98-c658-4031-bdb1-a0024703952a",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4ea677-b0e1-43eb-9964-37dcb5c0241f",
   "metadata": {},
   "source": [
    "- [Bayesian Optimization with HYPEROPT](https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40166b8f-b982-4cb1-bc9f-989b297970ad",
   "metadata": {},
   "source": [
    "#### Carga hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55470307-b090-4ba0-a615-b200b386b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not calculate_weights and city:\n",
    "\n",
    "    # FILE_NAME = f\"{city_name}_hyperparams{loaded_timestamp}.json\"\n",
    "    FILE_NAME = f\"{city_name}_hyperparams2023-04-22-14:13:41.json\"\n",
    "\n",
    "    best_hyperparams = load_json(f\"{HYPERPARAMS_PATH}{city_name}/\", FILE_NAME)\n",
    "\n",
    "# # # 0.875 GA\n",
    "# # # 0.04, 1, 3.9, 900\n",
    "# # # best_hyperparams = {}\n",
    "# # # best_hyperparams['eta'] = 0.04\n",
    "# # # best_hyperparams['max_depth'] = 1\n",
    "# # # best_hyperparams['min_child_weight'] = 3.9\n",
    "# # # best_hyperparams['n_estimators'] = 900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1213fe-2a09-4686-8834-edc9ba60d2ea",
   "metadata": {},
   "source": [
    "#### Escritura hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dad8965-a095-4b09-9cc3-915c4d2c5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_weights and city:\n",
    "    FILE_NAME = f\"{city_name}_hyperparams{MODEL_TIMESTAMP}.json\"\n",
    "\n",
    "    write_json(best_hyperparams, f\"{HYPERPARAMS_PATH}{city_name}/\", FILE_NAME)\n",
    "    print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e290436-2027-4bac-99c1-78870fd94419",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca5c496-8106-4310-acb2-0b9b8dd5ecaf",
   "metadata": {},
   "source": [
    "#### Carga definitiva/auxiliar de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69adbb52-3650-4cde-8322-032613a81f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_NAME = 'madrid_adapted_leeds_default_weights.json'\n",
    "# # FILE_NAME = 'madrid_weights_no_roadClass.json'\n",
    "FILE_NAME = f\"{city_name}_weights{loaded_timestamp}.json\"\n",
    "\n",
    "feature_vector = load_json(WEIGHTS_PATH, FILE_NAME)\n",
    "\n",
    "\n",
    "FILE_NAME =  f\"{city_name}_weights2023-04-22-14:13:41.json\"\n",
    "\n",
    "\n",
    "feature_vector = load_json(WEIGHTS_PATH, FILE_NAME)\n",
    "########## 17-05-2022 ##########\n",
    "# Se ha cambiado 1stRoadClass (distrito) a RoadwayFeatures --> EN LEEDS NO ES ASÍ, PERO PARA Mí TIENE SENTIDO\n",
    "# Se ha cambiado tipo accidente de RoadwayFeatures a Accident --> EN LEEDS NO EXISTE ESTE CAMPO, PERO PARA Mí TIENE SENTIDO\n",
    "# feature_vector = {'Accident Features': {'feature_childs': ['coordenada_x_utm',\n",
    "#    'coordenada_y_utm',\n",
    "#    'hora',\n",
    "#    'vehiculos_implicados',\n",
    "#    'tipo_accidente'],\n",
    "#   'feature_weights': ['0.03979435',\n",
    "#    '0.061120145',\n",
    "#    '0.02675103',\n",
    "#    '0.083673365',\n",
    "#    '0.03753465'],\n",
    "#   'wpi': 0.255731688},\n",
    "#  'Roadway Features': {'feature_childs': ['distrito', 'tipo_carretera'],\n",
    "#   'feature_weights': ['0.044392798', '9999.9'],\n",
    "#   'wpi': 0.03753465},\n",
    "#  'Environmental Features': {'feature_childs': ['estado_meteorológico'],\n",
    "#   'feature_weights': ['0.024665328'],\n",
    "#   'wpi': 0.024665328},\n",
    "#  'Vehicle Features': {'feature_childs': ['tipo_vehiculo'],\n",
    "#   'feature_weights': ['0.048645806'],\n",
    "#   'wpi': 0.048645806},\n",
    "#  'Casualty Features': {'feature_childs': ['tipo_persona',\n",
    "#    'sexo',\n",
    "#    'rango_edad',\n",
    "#    'drogas_alcohol_positivo'],\n",
    "#   'feature_weights': ['0.2764425', '0.2201619', '0.06879471', '0.03230864'],\n",
    "#   'wpi': 0.59770775}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5b938-bfc9-4d1b-94e6-571720665d88",
   "metadata": {},
   "source": [
    "#### Cálculo de pesos de caracetrísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174b40a-9c9c-45f0-b709-92ea5dba4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_weights and city:\n",
    "    best_hyperparams['max_depth'] =  int(best_hyperparams['max_depth'])\n",
    "\n",
    "    xgboost = XGBClassifier(**best_hyperparams,\n",
    "                            tree_method = tree_method,\n",
    "                            single_precision_histogram =  True)\n",
    "\n",
    "    xgboost.fit(X_train, Y_train_onehot)\n",
    "\n",
    "    child_weights  = np.array(xgboost.feature_importances_)\n",
    "    feature_vector = fill_feature_vector(X_train, child_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab3115f-6ec7-44dd-aa5f-a12a666b8db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f6472-a98c-4801-b5cc-31d0f46c13a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualización pesos calculados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fc1536-1b5a-4d6a-922d-5f27f58e7396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if calculate_weights and city:\n",
    "    FILE_NAME = f\"{city_name}_figure_weights_{MODEL_TIMESTAMP}.svg\"\n",
    "\n",
    "    print(xgboost.get_booster().get_score(importance_type= 'weight'))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(X_train_downsampled.columns, xgboost.feature_importances_)\n",
    "    plt.savefig(WEIGHTS_PATH + FILE_NAME)\n",
    "\n",
    "    print(xgboost.feature_importances_)\n",
    "\n",
    "    for column, weight in zip(X_train_downsampled.columns,xgboost.feature_importances_):\n",
    "      print(column, weight)\n",
    "\n",
    "    child_weights  = np.array(xgboost.feature_importances_)\n",
    "    feature_vector = fill_feature_vector(X_train_downsampled, child_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278163f-8450-412d-8147-c235f6825646",
   "metadata": {},
   "source": [
    "#### Escritura de pesos de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e3738-44ef-45be-9b24-989e4610267e",
   "metadata": {},
   "source": [
    "- v5: Pesos calculados con hiperparámetros. En el dataset están tipificados los vehículos como en el artículo, las edades no están en rango.\n",
    "- v6: Pesos calculados con hiperparámetros. En el dataset están tipificados los vehículos como en el artículo, las edades están en rango.\n",
    "- v7: hiperparams, tipos de carretera tipificados por vía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d2414-bf56-4519-aaf2-ae6333e69b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_weights and madrid:\n",
    "    matrix_indexes = fv2gi(feature_vector)\n",
    "\n",
    "    FILE_NAME = f\"{city_name}_weights{MODEL_TIMESTAMP}.json\"\n",
    "    # FILE_NAME = 'default_calculated_weights.json'\n",
    "\n",
    "    write_json(feature_vector, WEIGHTS_PATH, FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0996e0b-b416-49a6-b0e4-ebc3a7918519",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc80de-39c3-4818-b7eb-8a406aeed4e2",
   "metadata": {},
   "source": [
    "### Cálculo índices de matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd560eed-aa5b-47ad-a8fd-fb142a5d123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "    matrix_indexes = fv2gi(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f6ee4-aaa8-49d8-b70c-2fc235d68e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dR3Ah2X24fbw",
   "metadata": {
    "id": "dR3Ah2X24fbw"
   },
   "source": [
    "## Construcción de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cWGYQ82UI4RM",
   "metadata": {
    "id": "cWGYQ82UI4RM"
   },
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    train_bgi = build_gray_images(X_train, 5, matrix_indexes)\n",
    "    train_original_bgi = build_gray_images(X_train_original, 5, matrix_indexes)\n",
    "\n",
    "    # val_bgi  = build_gray_images(X_val, 5, matrix_indexes)\n",
    "    test_bgi = build_gray_images(X_test, 5, matrix_indexes)\n",
    "\n",
    "    pd.DataFrame(train_bgi[:,:,1057])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yQTCws554zZL",
   "metadata": {
    "id": "yQTCws554zZL"
   },
   "source": [
    "## Reshape de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mRrOKk3a43ZI",
   "metadata": {
    "id": "mRrOKk3a43ZI"
   },
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    train_images = shape_images(X_data = X_train,\n",
    "                            gray_images = train_bgi)\n",
    "\n",
    "    train_original_images = shape_images(X_data = X_train_original,\n",
    "                                         gray_images = train_original_bgi)\n",
    "\n",
    "    # val_images  = shape_images(X_data = X_val,\n",
    "    #                            gray_images = val_bgi)\n",
    "\n",
    "    test_images  = shape_images(X_data = X_test,\n",
    "                                gray_images = test_bgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4864c-4aa9-4200-bd96-264b1aa0aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bgi[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991a700-e8db-47f1-aeaf-847129f13355",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    plt.gray()\n",
    "    for i in range(100,103):\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.grid(b=None)\n",
    "        plt.imshow(train_bgi[:,:,i+6])\n",
    "        # plt.savefig(f\"{city_name}_image_example_{i}.svg\",transparent=True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbab23-7641-4c42-a516-926167396edf",
   "metadata": {},
   "source": [
    "## CT-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffc887-e993-4d22-8bf3-06e820563de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e65ec-1818-487b-a33e-f69b1759624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = {\n",
    "    'Slight': 0,\n",
    "    'Assistance': 1\n",
    "}   \n",
    "\n",
    "train = train.replace(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459ed61-bd9c-4e87-93af-c029e7cc8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctgan import CTGAN\n",
    "\n",
    "train['distrito'] = train['distrito'].astype(int)\n",
    "train['estado_meteorológico'] = train['estado_meteorológico'].astype(int)\n",
    "train['tipo_vehiculo'] = train['tipo_vehiculo'].astype(int)\n",
    "train['tipo_persona'] = train['tipo_persona'].astype(int)\n",
    "train['dia_semana'] = train['tipo_persona'].astype(int)\n",
    "train['semana_en_año'] = train['tipo_persona'].astype(int)\n",
    "train['coordenada_x_utm'] = train['coordenada_x_utm'].astype(float)\n",
    "train['coordenada_y_utm'] = train['coordenada_y_utm'].astype(float)\n",
    "train['lesividad'] = train['lesividad'].astype(int)\n",
    "\n",
    "ctgan = CTGAN(epochs=10)\n",
    "# ctgan.fit(train, ['lesividad'], epochs=1500)\n",
    "\n",
    "# Create synthetic data\n",
    "# synthetic_data = ctgan.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11997f2a-d9a3-4b2c-8dc6-b432f3eb5a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic_data.lesividad.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bc47d-fbac-497e-9c4d-ffca135562dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## C-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a8218-ecd2-45fc-b2b3-ce209e3406a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "# num_channels = 1\n",
    "# num_classes = 2\n",
    "# image_size = 8\n",
    "# latent_dim = 32\n",
    "# epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec703cf-bb7d-447c-b6bb-7f06eb055ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transf = {\n",
    "#     'Slight': 0,\n",
    "#     'Assistance': 1\n",
    "# }   \n",
    "\n",
    "# Y_train_categorical = Y_train.replace(transf)\n",
    "# Y_test_categorical = Y_test.replace(transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac338214-c5a3-402b-9536-c7f7d722520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We'll use all the available examples from both the training and test\n",
    "# # sets.\n",
    "# # (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# all_train = np.concatenate([train_images, test_images])\n",
    "# npad = ((0, 0), (1, 2), (1, 2),(0,0))\n",
    "# all_train = np.pad(all_train, pad_width=npad, mode='constant', constant_values=0)\n",
    "# all_labels = np.concatenate([Y_train_categorical, Y_test_categorical])\n",
    "\n",
    "# # Scale the pixel values to [0, 1] range, add a channel dimension to\n",
    "# # the images, and one-hot encode the labels.\n",
    "# all_train = all_train.astype(\"float32\")\n",
    "# all_train = np.reshape(all_train, (-1, 8, 8, 1))\n",
    "# all_labels = keras.utils.to_categorical(all_labels, num_classes)\n",
    "\n",
    "# # Create tf.data.Dataset.\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((all_train, all_labels))\n",
    "# dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# print(f\"Shape of training images: {all_train.shape}\")\n",
    "# print(f\"Shape of training labels: {all_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c88ab-6bf2-4800-b92b-bc8319688283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "\n",
    "# generator_in_channels = latent_dim + num_classes\n",
    "# discriminator_in_channels = num_channels + num_classes\n",
    "# print(generator_in_channels, discriminator_in_channels)\n",
    "\n",
    "# # Create the discriminator.\n",
    "# discriminator = keras.Sequential(\n",
    "#     [\n",
    "#         keras.layers.InputLayer((8, 8, discriminator_in_channels)),\n",
    "#         layers.Conv2D(4, (2, 2), strides=(2, 2), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2D(4, (2, 2), strides=(3, 3), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.GlobalMaxPooling2D(),\n",
    "#         layers.Dense(1),\n",
    "#     ],\n",
    "#     name=\"discriminator\",\n",
    "# )\n",
    "\n",
    "# # Create the generator.\n",
    "# generator = keras.Sequential(\n",
    "#     [\n",
    "#         keras.layers.InputLayer((generator_in_channels,)),\n",
    "#         # We want to generate 128 + num_classes coefficients to reshape into a\n",
    "#         # 7x7x(128 + num_classes) map.\n",
    "#         layers.Dense(8 * 8 * generator_in_channels),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Reshape((2, 2, generator_in_channels)),\n",
    "#         layers.Conv2DTranspose(16, (2,2), strides=(1, 1), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2DTranspose(16, (2,2), strides=(1, 1), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2DTranspose(32, (2,2), strides=(1, 1), padding=\"same\"),\n",
    "#         layers.LeakyReLU(alpha=0.2),\n",
    "#         layers.Conv2D(1, (2, 2), padding=\"same\", activation=\"sigmoid\"),\n",
    "#     ],\n",
    "#     name=\"generator\",\n",
    "# )\n",
    "\n",
    "# cond_gan = ConditionalGAN(\n",
    "#     discriminator=discriminator, generator=generator, latent_dim=latent_dim\n",
    "# )\n",
    "\n",
    "# cond_gan.compile(\n",
    "#     d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#     g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "#     loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "# )\n",
    "\n",
    "# cond_gan.fit(dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5e693-9956-4659-ab7e-b65be1c81cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# from tensorflow_docs.vis import embed\n",
    "\n",
    "# # We first extract the trained generator from our Conditiona GAN.\n",
    "# trained_gen = cond_gan.generator\n",
    "\n",
    "# # Choose the number of intermediate images that would be generated in\n",
    "# # between the interpolation + 2 (start and last images).\n",
    "# num_interpolation = 2  # @param {type:\"integer\"}\n",
    "\n",
    "# # Sample noise for the interpolation.\n",
    "# interpolation_noise = tf.random.normal(shape=(1, latent_dim))\n",
    "# interpolation_noise = tf.repeat(interpolation_noise, repeats=num_interpolation)\n",
    "# interpolation_noise = tf.reshape(interpolation_noise, (num_interpolation, latent_dim))\n",
    "\n",
    "\n",
    "# def interpolate_class(first_number, second_number):\n",
    "#     # Convert the start and end labels to one-hot encoded vectors.\n",
    "#     first_label = keras.utils.to_categorical([first_number], num_classes)\n",
    "#     second_label = keras.utils.to_categorical([second_number], num_classes)\n",
    "#     first_label = tf.cast(first_label, tf.float32)\n",
    "#     second_label = tf.cast(second_label, tf.float32)\n",
    "\n",
    "#     # Calculate the interpolation vector between the two labels.\n",
    "#     percent_second_label = tf.linspace(0, 1, num_interpolation)[:, None]\n",
    "#     percent_second_label = tf.cast(percent_second_label, tf.float32)\n",
    "#     interpolation_labels = (\n",
    "#         first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "#     )\n",
    "\n",
    "#     # Combine the noise and the labels and run inference with the generator.\n",
    "#     noise_and_labels = tf.concat([interpolation_noise, interpolation_labels], 1)\n",
    "#     fake = trained_gen.predict(noise_and_labels)\n",
    "#     return fake\n",
    "\n",
    "\n",
    "# start_class = 0  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "# end_class = 1  # @param {type:\"slider\", min:0, max:9, step:1}\n",
    "\n",
    "# fake_images = interpolate_class(start_class, end_class)\n",
    "\n",
    "# # fake_images *= 255.0\n",
    "# converted_images = fake_images.astype(np.float)\n",
    "# converted_images = tf.image.resize(converted_images, (96, 96)).numpy().astype(np.uint8)\n",
    "# imageio.mimsave(\"animation.gif\", converted_images, fps=1)\n",
    "# embed.embed_file(\"animation.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695add8e-e674-435f-90cb-d4eafd1f9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(train_images[0])\n",
    "# # plt.savefig(f\"{city_name}_image_example_{i}.svg\",transparent=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bfe5c-fe77-40e6-87d7-69368402816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(3, 3))\n",
    "# plt.grid(b=None)\n",
    "# plt.imshow(fake_images[0])\n",
    "# # plt.savefig(f\"{city_name}_image_example_{i}.svg\",transparent=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb2d341-a847-437b-a1fb-0e40e01cced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images = shape_images(X_data = X_train,\n",
    "#                             gray_images = train_bgi)\n",
    "# test_images  = shape_images(X_data = X_test,\n",
    "#                             gray_images = test_bgi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IStgg--F5L3F",
   "metadata": {
    "id": "IStgg--F5L3F",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rvyfvMPy5L3G",
   "metadata": {
    "id": "rvyfvMPy5L3G"
   },
   "outputs": [],
   "source": [
    "# !conda install -c anaconda seaborn --y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dg0d7-k15L3H",
   "metadata": {
    "id": "dg0d7-k15L3H",
    "tags": []
   },
   "source": [
    "### Matriz de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89065a3-06a7-4270-b078-047e0746b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SN7gwKNO5L3H",
   "metadata": {
    "id": "SN7gwKNO5L3H"
   },
   "outputs": [],
   "source": [
    "# data_frame = ['', '', '', '', 'Weather conditions', 'Vehicle']\n",
    "# correlation_matrix(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fAIUUg5L3J",
   "metadata": {
    "id": "d5fAIUUg5L3J",
    "tags": []
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lhQElB3I5L3J",
   "metadata": {
    "id": "lhQElB3I5L3J"
   },
   "outputs": [],
   "source": [
    "# pca(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffbfe4-9fce-4e68-81b9-766d485b87ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b224d1-ca07-4099-b308-4708c2d533e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "    n_samples = 150\n",
    "    index_slight  = Y_train[Y_train == 'Slight'][:n_samples].index\n",
    "    index_serious = Y_train[Y_train == 'Serious'][:n_samples].index\n",
    "    index_fatal   = Y_train[Y_train == 'Fatal'][:n_samples].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebce2f-081d-4e3e-901c-f23b7f3ecedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    # Get same number of class samples from SMOTEII\n",
    "    X_slight_train_tsne  = X_train.loc[index_slight]\n",
    "    X_serious_train_tsne = X_train.loc[index_serious]\n",
    "    X_fatal_train_tsne   = X_train.loc[index_fatal]\n",
    "\n",
    "    X_train_tsne = pd.concat([X_slight_train_tsne, X_serious_train_tsne, X_fatal_train_tsne])\n",
    "\n",
    "    Y_slight_train_tsne  = Y_train[index_slight]\n",
    "    Y_serious_train_tsne = Y_train[index_serious]\n",
    "    Y_fatal_train_tsne   = Y_train[index_fatal]\n",
    "\n",
    "    Y_train_tsne = pd.concat([Y_slight_train_tsne, Y_serious_train_tsne, Y_fatal_train_tsne])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878dd80-e602-4a9f-94c8-3608a8b2417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    n_samples = len(Y_train_original[Y_train_original == 'Fatal'])\n",
    "\n",
    "    index_slight  = Y_train_original[Y_train_original == 'Slight'][:n_samples].index\n",
    "    index_serious = Y_train_original[Y_train_original == 'Serious'][:n_samples].index\n",
    "    index_fatal   = Y_train_original[Y_train_original == 'Fatal'][:n_samples].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5563a-01fd-443b-9c39-d62f4e7067e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    # Get same number of class samples from original\n",
    "    X_slight_clean_tsne  = X_train_original.loc[index_slight]\n",
    "    X_serious_clean_tsne = X_train_original.loc[index_serious]\n",
    "    X_fatal_clean_tsne   = X_train_original.loc[index_fatal]\n",
    "\n",
    "    X_clean_tsne = pd.concat([X_slight_clean_tsne, X_serious_clean_tsne, X_fatal_clean_tsne])\n",
    "\n",
    "    Y_slight_clean_tsne  = Y_train_original[index_slight]\n",
    "    Y_serious_clean_tsne = Y_train_original[index_serious]\n",
    "    Y_fatal_clean_tsne   = Y_train_original[index_fatal]\n",
    "\n",
    "    Y_clean_tsne = pd.concat([Y_slight_clean_tsne, Y_serious_clean_tsne, Y_fatal_clean_tsne])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669feae-6b17-4f05-b43b-6d3aa87a1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tsne and city:\n",
    "    FILE_NAME = f\"{TSNE_PATH}{city_name}/2d_tsne_clean.svg\"\n",
    "    plot_TSNE(X_clean_tsne, Y_clean_tsne, n_components = 2, output_file_name = FILE_NAME, title = 'Muestras originales 2 Componentes')\n",
    "\n",
    "    FILE_NAME = f\"{TSNE_PATH}{city_name}/3d_tsne_clean.svg\"\n",
    "    plot_TSNE(X_clean_tsne, Y_clean_tsne, n_components = 3, output_file_name = FILE_NAME, title = 'Muestras originales 3 Componentes')\n",
    "\n",
    "    FILE_NAME = f\"{TSNE_PATH}{city_name}/2d_tsne_train.svg\"\n",
    "    plot_TSNE(X_train_tsne, Y_train_tsne, n_components = 2, output_file_name = FILE_NAME, title = 'Muestras SMOTE-II 2 Componentes')\n",
    "\n",
    "    FILE_NAME = f\"{TSNE_PATH}{city_name}/3d_tsne_train.svg\"\n",
    "    plot_TSNE(X_train_tsne, Y_train_tsne, n_components = 3, output_file_name = FILE_NAME, title = 'Muestras SMOTE-II 3 Componentes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XBbgOcIz5L3J",
   "metadata": {
    "id": "XBbgOcIz5L3J",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U84H7J695L3J",
   "metadata": {
    "id": "U84H7J695L3J",
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A2mJaZVn5L3L",
   "metadata": {
    "id": "A2mJaZVn5L3L"
   },
   "outputs": [],
   "source": [
    "# # input_img = Input(shape=(25,))\n",
    "\n",
    "# # # definimos el encoder, que tendra una entrada de Input_img y una segunda capa con entrada de encoder1 y salida 3\n",
    "# # encoder1 = layers.Dense(15, activation='sigmoid')(input_img)\n",
    "# # encoder2 = layers.Dense(3, activation='sigmoid')(encoder1)\n",
    "\n",
    "# # # definimos el  decoder que tendra una entrada inicial de encoder3 y una salida de 128 y finalmete una capa de salida con los mismos que Input_img\n",
    "# # decoder1 = layers.Dense(15, activation='sigmoid')(encoder2)\n",
    "# # decoder2 = layers.Dense(25, activation='sigmoid')(decoder1)\n",
    "\n",
    "# # # this model maps an input to its reconstruction\n",
    "# # autoencoder = tf.keras.Model(inputs=input_img, outputs=decoder2)\n",
    "# # autoencoder.summary()\n",
    "\n",
    "# # autoencoder.compile(optimizer='adam', loss='binary_crossentropy') #se usan estos dos en estas arquitecturas\n",
    "\n",
    "# X_train = array_train_images\n",
    "# X_test = array_test_images\n",
    "\n",
    "# X_train = X_train.reshape(len(array_train_images), 25)\n",
    "# X_test  = X_test.reshape(len(X_test), 25)\n",
    "\n",
    "# autoencoder.fit(X_train, X_train,\n",
    "#                 epochs=120,\n",
    "#                 batch_size=32,\n",
    "#                 shuffle=True,\n",
    "#                 validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gIiKplwP5L3L",
   "metadata": {
    "id": "gIiKplwP5L3L",
    "tags": []
   },
   "source": [
    "#### Visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opBOyrIx5L3M",
   "metadata": {
    "id": "opBOyrIx5L3M"
   },
   "outputs": [],
   "source": [
    "# # create encoder model\n",
    "# encoder = tf.keras.Model(inputs=input_img, outputs=encoder2)\n",
    "# encoder.summary()\n",
    "# # create decoder model\n",
    "# encoded_input = Input(shape=(3,))\n",
    "# #lo que hace aqui es quedarse con las capas que corresponden al decodificador\n",
    "# decoder_layer1 = autoencoder.layers[-2]\n",
    "# decoder_layer2 = autoencoder.layers[-1]\n",
    "# decoder = tf.keras.Model(inputs=encoded_input, outputs=decoder_layer2(decoder_layer1(encoded_input)))\n",
    "# decoder.summary()\n",
    "# # si miramos la salida, son simetricos el uno respecto al otro\n",
    "# # encoder va de input a 3 y decoder de 3 a input\n",
    "\n",
    "# # get latent vector for visualization\n",
    "# latent_vector = encoder.predict(X_test)\n",
    "# # get decoder output to visualize reconstructed image\n",
    "# reconstructed_imgs = decoder.predict(latent_vector)\n",
    "\n",
    "\n",
    "# # visualize in 3D plot\n",
    "# from pylab import rcParams\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "# fig = plt.figure(1)\n",
    "# ax = Axes3D(fig)\n",
    "\n",
    "# xs = latent_vector[:, 0]\n",
    "# ys = latent_vector[:, 1]\n",
    "# zs = latent_vector[:, 2]\n",
    "\n",
    "# # color=['red','green','blue']\n",
    "\n",
    "# # for x, y, z, label in zip(xs, ys, zs, Y_test):\n",
    "# #     c = color[int(label)]\n",
    "# #     ax.text(x, y, z, label, backgroundcolor=c)\n",
    "    \n",
    "# # ax.set_xlim(xs.min(), xs.max())\n",
    "# # ax.set_ylim(ys.min(), ys.max())\n",
    "# # ax.set_zlim(zs.min(), zs.max())\n",
    "\n",
    "# # plt.show()\n",
    "\n",
    "# # X_test_encoded = encoder.predict(X_test, batch_size=32)\n",
    "# # plt.figure(figsize=(6, 6))\n",
    "# # plt.scatter(X_test_encoded[:, 0], X_test_encoded[:, 1], c=Y_test)\n",
    "# # plt.colorbar()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593cb648-7d62-47a7-975f-3d1c718c5a05",
   "metadata": {
    "id": "1PdwhQuQ9o_P"
   },
   "source": [
    "## One-Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6hTctKZSI3re",
   "metadata": {
    "id": "6hTctKZSI3re"
   },
   "outputs": [],
   "source": [
    "if city:\n",
    "    Y_train_onehot = casualty_to_one_hot(Y_train)\n",
    "    Y_train_original_onehot = casualty_to_one_hot(Y_train_original)\n",
    "    # Y_val_onehot   = casualty_to_one_hot(Y_val)\n",
    "    Y_test_onehot  = casualty_to_one_hot(Y_test)\n",
    "\n",
    "    array_train_images = np.asarray(train_images)\n",
    "    array_train_original_images = np.asarray(train_original_images)\n",
    "    # array_val_images   = np.asarray(val_images)\n",
    "    array_test_images  = np.asarray(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478f583-9071-4aec-9bb6-b4f65db21bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if city:\n",
    "#     array_val_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea5149-aa36-4789-a693-169d65b3c5ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94cf5a-3223-4b1d-8d5e-ca001ccc999e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "    array_train_images = np.asarray(train_images)\n",
    "    # array_val_images   = np.asarray(val_images)\n",
    "    array_test_images  = np.asarray(test_images)\n",
    "\n",
    "    input_train_shape = (len(array_train_images), 5, 5, 1)\n",
    "    # input_val_shape = (len(array_val_images), 5, 5, 1)\n",
    "    input_test_shape  = (len(array_test_images), 5, 5, 1)\n",
    "\n",
    "    array_train_images = array_train_images.reshape(input_train_shape)\n",
    "    # array_val_images   = array_val_images.reshape(input_val_shape)\n",
    "    array_test_images  = array_test_images.reshape(input_test_shape)\n",
    "\n",
    "    Y_test_labels = one_hot_to_casualty(Y_test)\n",
    "\n",
    "    from sklearn.utils import class_weight\n",
    "\n",
    "    pesos = class_weight.compute_class_weight('balanced',\n",
    "                                              classes = np.unique(Y_train_original),\n",
    "                                              y = Y_train_original)\n",
    "\n",
    "\n",
    "    print('\\nPesos calculados:', pesos, '\\n\\n')\n",
    "\n",
    "\n",
    "    # Keras espera un diccionario donde la clave sea el número de clase \n",
    "    # y el valor sea el peso calculado. \n",
    "    pesos = dict(enumerate(pesos))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079cfec-3bb9-4a49-8bb9-915a9f16c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "if city:\n",
    "\n",
    "    times = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370e813-f071-4ef1-8272-3da770c04bb8",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82f890-343d-436a-bbac-306316b12f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    MODEL_NAME = MODELS_NAME[3]\n",
    "\n",
    "    MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "    MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.joblib\"\n",
    "\n",
    "    sns.reset_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13057516-8157-433b-840f-69e6de7373e4",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac81c6b-220c-43f0-ba37-33b300083e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if city and train_nn and other_models:\n",
    "    start = time.time()\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    gnb = gnb.fit(X_train, Y_train)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    ellapsed_time = round(end - start, 2)\n",
    "\n",
    "    model_time = pd.DataFrame({'city': [city_name], 'model': [MODEL_NAME], 'time': [ellapsed_time]})\n",
    "    times = times.append(model_time)\n",
    "\n",
    "\n",
    "    print(f\"Done! {MODEL_NAME} in {ellapsed_time} (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d312b-5707-4d35-8af1-b40005708628",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cef99-7333-428c-9410-8d9d6260d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and train_nn and other_models:\n",
    "\n",
    "    dump(gnb, MODEL_PATH + MODEL_FILE_NAME) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1e22a-8576-4b07-8383-e52967f19450",
   "metadata": {},
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4d9bb-2f02-481f-8110-2875933772e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and not train_nn and other_models:\n",
    "\n",
    "    gnb = load(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ab5b8-02b9-4df7-b3fe-3d524f9ed53f",
   "metadata": {},
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c6827-d015-42c2-9ac9-9e7d8100368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and other_models:\n",
    "    print(\"[INFO] evaluating model...\")\n",
    "    if train_nn:\n",
    "        Y_train_predicted = gnb.predict(X_train)\n",
    "        save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                         model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                         y_true = Y_train,\n",
    "                                                         y_predicted = Y_train_predicted,\n",
    "                                                         data = 'train')\n",
    "    Y_predicted = gnb.predict(X_test)\n",
    "\n",
    "    save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                     model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                     y_true = Y_test,\n",
    "                                                     y_predicted = Y_predicted,\n",
    "                                                     data = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1431141-a1f7-482c-93df-8c58336641e9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f1b72-1393-4a93-9914-b8eef3ee03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "if city:\n",
    "\n",
    "    MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "    MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.joblib\"\n",
    "    MODEL_NAME = MODELS_NAME[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835cb07-a647-4f28-aa47-53eacd809311",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and train_nn and other_models:\n",
    "    start = time.time()\n",
    "\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    ellapsed_time = round(end - start, 2)\n",
    "\n",
    "\n",
    "    model_time = pd.DataFrame({'city': [city_name], 'model': [MODEL_NAME], 'time': [ellapsed_time]})\n",
    "    times = times.append(model_time)\n",
    "\n",
    "    print(f\"Done! {MODEL_NAME} in {ellapsed_time} (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7cb10-252d-4f34-9831-fd34013a9a6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830cecd0-a029-42a3-9381-10a04c74b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and train_nn and other_models:\n",
    "\n",
    "    dump(clf, MODEL_PATH + MODEL_FILE_NAME) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc2a75-c746-4736-b888-62a5e8d93182",
   "metadata": {},
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58fd62-021d-482f-a95b-c1e2c036ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and not train_nn and other_models:\n",
    "    MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{timestamp_load}.joblib\"\n",
    "\n",
    "    clf = load(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0101b979-5f76-4f51-ab6d-31d933cb6ea8",
   "metadata": {},
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3405c4c4-a5cf-4339-b991-79bfb145017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and other_models:\n",
    "    print(\"[INFO] evaluating model...\")\n",
    "\n",
    "    if train_nn:\n",
    "        Y_train_predicted = clf.predict(X_train)\n",
    "        save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                         model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                         y_true = Y_train,\n",
    "                                                         y_predicted = Y_train_predicted,\n",
    "                                                         data = 'train')\n",
    "\n",
    "    Y_predicted = clf.predict(X_test)\n",
    "\n",
    "    save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                     model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                     y_true = Y_test,\n",
    "                                                     y_predicted = Y_predicted,\n",
    "                                                     data = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a39c2-0589-4599-998e-624c3ae30fb9",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd71172-ec16-4ad8-9911-61f119055c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "if city:\n",
    "\n",
    "    MODEL_NAME = MODELS_NAME[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55a5324-8a19-4d7a-9936-0528fac6a38b",
   "metadata": {},
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cccc3-659c-4d47-b610-4c9bc701b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and other_models:\n",
    "    knn = KNeighborsClassifier(leaf_size = 7, n_neighbors = 91)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    knn.fit(X_train, Y_train)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    ellapsed_time = round(end - start, 2)\n",
    "\n",
    "    model_time = pd.DataFrame({'city': [city_name], 'model': [MODEL_NAME], 'time': [ellapsed_time]})\n",
    "    times = times.append(model_time)\n",
    "\n",
    "# leaf_size = list(range(1,10, 2))\n",
    "# n_neighbors = list(range(1,100, 10))\n",
    "# p = [1, 2]\n",
    "\n",
    "# if city and train_nn and other_models:\n",
    "\n",
    "#     start = time.time()\n",
    "\n",
    "#     # Create new KNN object\n",
    "#     hyperparameters = dict(leaf_size = leaf_size,\n",
    "#                            n_neighbors = n_neighbors)\n",
    "\n",
    "#     # Use GridSearch\n",
    "#     knn_2 = KNeighborsClassifier(leaf_size = 7, n_neighbors = 91)\n",
    "\n",
    "#     # Fit the model\n",
    "#     clf = GridSearchCV(knn_2,\n",
    "#                        hyperparameters,\n",
    "#                        cv = 4)\n",
    "\n",
    "#     knn = clf.fit(X_train, Y_train)\n",
    "\n",
    "#     end = time.time()\n",
    "\n",
    "#     ellapsed_time = round(end - start, 2)\n",
    "\n",
    "\n",
    "#     model_time = pd.DataFrame({'city': [city_name], 'model': [MODEL_NAME], 'time': [ellapsed_time]})\n",
    "#     times = times.append(model_time)\n",
    "\n",
    "#     # Print The value of best Hyperparameters\n",
    "\n",
    "#     best_leaf_size  = knn.best_estimator_.get_params()['leaf_size']\n",
    "#     best_n_neighbors = knn.best_estimator_.get_params()['n_neighbors']\n",
    "\n",
    "#     print('Best leaf_size:', best_leaf_size)\n",
    "#     print('Best n_neighbors:', best_n_neighbors)\n",
    "\n",
    "#     df = pd.DataFrame({'best_leaf_size':[best_leaf_size], 'n_neighbors':[best_n_neighbors]})\n",
    "\n",
    "#     FILE_NAME = f\"{MODEL_NAME}/madrid_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "#     df.to_csv(HYPERPARAMS_PATH + FILE_NAME, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fdabd2-a01a-444b-b802-535186798bd3",
   "metadata": {},
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7f92a-3c77-40c7-9d58-82b2d321e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if city and train_nn and other_models:\n",
    "\n",
    "#     MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "#     MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.joblib\"\n",
    "\n",
    "#     dump(knn, MODEL_PATH + MODEL_FILE_NAME) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf557e-fae2-429c-8409-7cbb3b48ae85",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9fe88-06b2-47ec-9459-6d931c50ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if city and not train_nn and other_models:\n",
    "\n",
    "#     version = 'X'\n",
    "#     MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "#     MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{model_version}.joblib\"\n",
    "\n",
    "#     knn = load(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e34364-1192-4115-9796-6b64d6790e73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdf70a-df80-4dce-ad02-c658c8bc197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and other_models:\n",
    "    print(\"[INFO] evaluating model...\")\n",
    "\n",
    "    if train_nn:\n",
    "        Y_train_predicted = knn.predict(X_train)\n",
    "        save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                         model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                         y_true = Y_train,\n",
    "                                                         y_predicted = Y_train_predicted,\n",
    "                                                         data = 'train')\n",
    "    Y_predicted = knn.predict(X_test)\n",
    "\n",
    "    save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                     model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                     y_true = Y_test,\n",
    "                                                     y_predicted = Y_predicted,\n",
    "                                                     data = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d85ab2-3416-4f93-a44e-bad2c346c015",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolution 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc8277-f9e1-4e68-8337-ae9eb66a3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    MODEL_NAME = MODELS_NAME[1]\n",
    "\n",
    "    MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "    MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca013c7b-48ea-476a-b585-e8c751463f7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9e59f2-4185-4de6-9cd7-fb26cd7b729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aca6bf-90b1-42bd-9fba-d7b8a5ad36c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if city and train_nn and cnn1d:\n",
    "    start = time.time()\n",
    "\n",
    "    history = convolution_1d.fit(array_train_images, Y_train_onehot,\n",
    "                                 # class_weight = pesos,\n",
    "                                 batch_size = 128,\n",
    "                                 epochs = 100,\n",
    "                                 shuffle = True,\n",
    "                                 validation_data = (array_test_images, Y_test_onehot))\n",
    "    end = time.time()\n",
    "\n",
    "    ellapsed_time = round(end - start, 2)\n",
    "\n",
    "    model_time = pd.DataFrame({'city': [city_name],\n",
    "                               'model': [MODEL_NAME],\n",
    "                               'time': [ellapsed_time]})\n",
    "\n",
    "    times = times.append(model_time)\n",
    "\n",
    "    history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f2da5-5e07-4244-8e64-5d70a28aed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     model_time = pd.DataFrame({'city': [city_name],\n",
    "#                                'model': [MODEL_NAME],\n",
    "#                                'time': 0})\n",
    "#     times = times.append(model_time)\n",
    "#     # wrap our model into a scikit-learn compatible classifier\n",
    "#     print(\"[INFO] initializing model...\")\n",
    "#     model = KerasClassifier(build_fn=get_1d_conv, verbose=10)\n",
    "\n",
    "#     # define a grid of the hyperparameter search space\n",
    "\n",
    "#     # fm_one = fm_two = fm_three = fm_four = fm_five = fm_six = [32, 64, 128, 256, 512]\n",
    "#     fm_one = fm_two = fm_three = fm_four = [32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "#     dense  = [32, 64, 128, 256]\n",
    "\n",
    "#     learnRate = [0.1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "#     batchSize = [32, 64]\n",
    "\n",
    "#     epochs = [1]\n",
    "\n",
    "#     # create a dictionary from the hyperparameter grid\n",
    "#     grid = dict(\n",
    "#         fm_one = fm_one,\n",
    "#         fm_two = fm_two,\n",
    "#         fm_three = fm_three,\n",
    "#         fm_four = fm_four,\n",
    "#         # fm_five = fm_five,\n",
    "#         # fm_six = fm_six,\n",
    "#         dense = dense,\n",
    "#         learnRate=learnRate,\n",
    "#         batch_size=batchSize,\n",
    "#         epochs=epochs\n",
    "#     )\n",
    "\n",
    "#     # initialize a random search with a 3-fold cross-validation and then\n",
    "#     # start the hyperparameter search process\n",
    "#     print(\"[INFO] performing random search...\")\n",
    "#     searcher = RandomizedSearchCV(estimator = model,\n",
    "#                                   n_iter = 1,\n",
    "#                                   cv = 2,\n",
    "#                                   param_distributions = grid,\n",
    "#                                   scoring = 'f1_micro')\n",
    "\n",
    "#     searchResults = searcher.fit(array_train_images, Y_train)\n",
    "\n",
    "#     # summarize grid search information\n",
    "#     bestScore = searchResults.best_score_\n",
    "#     bestParams = searchResults.best_params_\n",
    "\n",
    "#     print(\"[INFO] best score is {:.2f} using {}\".format(bestScore,\tbestParams))\n",
    "\n",
    "#     print(\"[INFO] evaluating the best model...\")\n",
    "#     taspcnn = bestModel = searchResults.best_estimator_\n",
    "#     # accuracy = bestModel.score(array_test_images, Y_test)\n",
    "#     # print(\"accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "#     text_file = open(f\"./CNN2D-{MODEL_TIMESTAMP}.txt\", \"w\")\n",
    "#     n = text_file.write(str(searchResults.cv_results_))\n",
    "#     text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57162dc6-95aa-4c54-88c7-9773e537479e",
   "metadata": {},
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c587c-a817-4892-8bc7-64200abf4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if city and train_nn and cnn1d:\n",
    "\n",
    "#     convolution_1d.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a2f07-c130-41aa-88fe-677b9c10ddd8",
   "metadata": {},
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1f493-056a-4491-84c1-c62bae69551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and not train_nn and not laptop and cnn1d:\n",
    "    # MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{timestamp_load}.joblib\"\n",
    "    MODEL_FILE_NAME = 'madrid_convolution_1d_2022-05-19-06:33:55.h5'\n",
    "\n",
    "    convolution_1d = tf.keras.models.load_model(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278dca3-abde-4aa5-8c59-42ccd1ee423a",
   "metadata": {},
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d8dac-a50d-454a-8f35-8d0a77292181",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if city and not laptop and cnn1d:\n",
    "\n",
    "    print(\"[INFO] evaluating network...\")\n",
    "\n",
    "    Y_predicted = convolution_1d.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "    if train_nn:\n",
    "        F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "        F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.svg\"\n",
    "\n",
    "        plot_f1_score_history(f1_score_path = F1_SCORE_PATH,\n",
    "                              f1_score_name = F1_SCORE_NAME,\n",
    "                              history = history)\n",
    "\n",
    "        Y_train_predicted = convolution_1d.predict(x = array_train_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "        save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                         model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                         y_true = Y_train,\n",
    "                                                         y_predicted = Y_train_predicted,\n",
    "                                                         data = 'train')\n",
    "\n",
    "    save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                     model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                     y_true = Y_test,\n",
    "                                                     y_predicted = Y_predicted,\n",
    "                                                     data = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PohCQRSm67P0",
   "metadata": {
    "id": "PohCQRSm67P0",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolution 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a48c3-e0a7-4a93-ab64-901503503e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city:\n",
    "\n",
    "    MODEL_NAME = MODELS_NAME[2]\n",
    "\n",
    "    MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "    MODEL_FILE_NAME = 'madrid_convolution_2d_2022-05-19-06:33:55.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vPh1ixx-67P2",
   "metadata": {
    "id": "vPh1ixx-67P2"
   },
   "source": [
    "#### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b-rJTSQC67P2",
   "metadata": {
    "id": "b-rJTSQC67P2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if city and train_nn:\n",
    "    \n",
    "    fm_one, fm_two, fm_three, fm_four = (256, 512, 512, 256)\n",
    "\n",
    "    dense  = 32\n",
    "\n",
    "    learnRate = 0.00001\n",
    "\n",
    "    batchSize = 128\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    tasp_cnn = get_tasp_cnn(fm_one = fm_one,\n",
    "                            fm_two = fm_two,\n",
    "                            fm_three = fm_three,\n",
    "                            fm_four = fm_four,\n",
    "                            dense = dense,\n",
    "                            dropout = 0.2,\n",
    "                            learnRate = learnRate)\n",
    "\n",
    "    history = tasp_cnn.fit(array_train_images, Y_train_onehot,\n",
    "                           # class_weight = pesos,\n",
    "                           batch_size = batchSize,\n",
    "                           epochs = 100,\n",
    "                           shuffle = True,\n",
    "                           validation_data = (array_test_images, Y_test_onehot))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    ellapsed_time = round(end - start, 2)\n",
    "\n",
    "    model_time = pd.DataFrame({'city': [city_name],\n",
    "                               'model': [MODEL_NAME],\n",
    "                               'time': [ellapsed_time]})\n",
    "    times = times.append(model_time)    \n",
    "\n",
    "    history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d19f33-9636-459c-a511-0a4ddcdde665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "model_time = pd.DataFrame({'city': [city_name],\n",
    "                           'model': [MODEL_NAME],\n",
    "                           'time': 0})\n",
    "times = times.append(model_time)\n",
    "# wrap our model into a scikit-learn compatible classifier\n",
    "print(\"[INFO] initializing model...\")\n",
    "model = KerasClassifier(build_fn=get_tasp_cnn, verbose=10)\n",
    "\n",
    "# define a grid of the hyperparameter search space\n",
    "\n",
    "# fm_one = fm_two = fm_three = fm_four = fm_five = fm_six = [32, 64, 128, 256, 512]\n",
    "fm_one = fm_two = fm_three = fm_four = [32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "dense  = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "learnRate = [0.1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "batchSize = [128]\n",
    "\n",
    "epochs = [50]\n",
    "\n",
    "num_classes = [2]\n",
    "\n",
    "# create a dictionary from the hyperparameter grid\n",
    "grid = dict(\n",
    "\tfm_one = fm_one,\n",
    "    fm_two = fm_two,\n",
    "    fm_three = fm_three,\n",
    "    fm_four = fm_four,\n",
    "    # fm_five = fm_five,\n",
    "    # fm_six = fm_six,\n",
    "    dense = dense,\n",
    "    num_classes=num_classes,\n",
    "\tlearnRate=learnRate,\n",
    "\tbatch_size=batchSize,\n",
    "\tepochs=epochs\n",
    ")\n",
    "\n",
    "# initialize a random search with a 3-fold cross-validation and then\n",
    "# start the hyperparameter search process\n",
    "print(\"[INFO] performing random search...\")\n",
    "searcher = RandomizedSearchCV(estimator = model,\n",
    "                              n_iter = 150,\n",
    "                              cv = 3,\n",
    "                              param_distributions = grid,\n",
    "                              scoring = 'f1_micro',\n",
    "                              verbose = 0)\n",
    "\n",
    "searchResults = searcher.fit(array_train_images, Y_train)\n",
    "\n",
    "# summarize grid search information\n",
    "bestScore = searchResults.best_score_\n",
    "bestParams = searchResults.best_params_\n",
    "\n",
    "print(\"[INFO] best score is {:.2f} using {}\".format(bestScore,\tbestParams))\n",
    "\n",
    "print(\"[INFO] evaluating the best model...\")\n",
    "tasp_cnn = bestModel = searchResults.best_estimator_\n",
    "# accuracy = bestModel.score(array_test_images, Y_test)\n",
    "# print(\"accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "text_file = open(f\"./madrid-CNN2D-{MODEL_TIMESTAMP}.txt\", \"w\")\n",
    "n = text_file.write(str(searchResults.cv_results_))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dca0f9-336a-4113-b58c-1fb300c89608",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5f6c4a-b22f-4416-bc32-5c4cf1188578",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasp_cnn = bestModel = searchResults.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac281b-8364-4710-b6e1-d75ae4a401d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if city and train_nn:\n",
    "    MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "    MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}_1.h5\"\n",
    "\n",
    "    tasp_cnn.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aT8XDceKGSdi",
   "metadata": {
    "id": "aT8XDceKGSdi",
    "tags": []
   },
   "source": [
    "#### Carga de modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dRaqg9SXGRwb",
   "metadata": {
    "id": "dRaqg9SXGRwb"
   },
   "outputs": [],
   "source": [
    "if city and not train_nn and not laptop:\n",
    "    # MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{timestamp_load}.joblib\"\n",
    "    MODEL_FILE_NAME = 'madrid_convolution_2d_2022-05-18-19:50:16.h5'\n",
    "\n",
    "    tasp_cnn = tf.keras.models.load_model(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29ab0f-d59b-4646-98af-d26e1b1398dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Exportar los kernels\n",
    "\n",
    "# n_samples = 3\n",
    "# layers = [0, 2, 4, 6]\n",
    "\n",
    "# for layer_number in layers:\n",
    "#     filters, biases = tasp_cnn.layers[layer_number].get_weights()\n",
    "    \n",
    "#     layer_name = layer_number//2 + 1\n",
    "\n",
    "#     for i in range(n_samples):\n",
    "#         # X,Y, channel, filter_number\n",
    "#         current_filter = filters[:,:,0, i]\n",
    "\n",
    "#         plt.figure(figsize=(3, 3))\n",
    "#         plt.grid(b = None)\n",
    "#         plt.imshow(current_filter, cmap='gray')\n",
    "\n",
    "#         # plt.savefig(f\"filters/{city_name}_filter_layer_{layer_name}_{i}.svg\", transparent=True)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69cd710-082b-4800-a989-8b24142de759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import tf.keras.mo.Model\n",
    "# tasp_cnn_feature_maps = tf.keras.models.Model(inputs = tasp_cnn.inputs, outputs=tasp_cnn.layers[0].output)\n",
    "\n",
    "# tasp_cnn_feature_maps.predict(array_train_images[:3]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17830ff4-9577-4f72-bd97-943844300fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_maps = tasp_cnn.predict(array_train_images)\n",
    "# # plot all 64 maps in an 8x8 squares\n",
    "# square = 5\n",
    "# ix = 1\n",
    "# for _ in range(square):\n",
    "#     for _ in range(square):\n",
    "#         # specify subplot and turn of axis\n",
    "\n",
    "#         # plot filter channel in grayscale\n",
    "#         plt.imshow(tasp_cnn_feature_maps[ix-1,:,:,:], cmap='gray')\n",
    "#         ix += 1\n",
    "# # show the figure\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wD_BOwcwGb4W",
   "metadata": {
    "id": "wD_BOwcwGb4W"
   },
   "source": [
    "#### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHVVq0khGato",
   "metadata": {
    "id": "nHVVq0khGato",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if city and not laptop:\n",
    "\n",
    "    print(\"[INFO] evaluating network...\")\n",
    "\n",
    "    Y_predicted = tasp_cnn.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "    rep = {'Slight': 0, 'Assistance': 1}\n",
    "    Y_train.replace(rep, inplace=True)\n",
    "    Y_test.replace(rep, inplace=True)\n",
    "\n",
    "    if train_nn:\n",
    "        F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "        F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}_1.svg\"\n",
    "\n",
    "        # plot_f1_score_history(f1_score_path = F1_SCORE_PATH,\n",
    "        #                       f1_score_name = F1_SCORE_NAME,\n",
    "        #                       history = history)\n",
    "\n",
    "        Y_train_predicted = tasp_cnn.predict(x = array_train_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "        save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                         model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                         y_true = Y_train,\n",
    "                                                         y_predicted = Y_train_predicted,\n",
    "                                                         data = 'train')\n",
    "\n",
    "    save_classification_report_and_confussion_matrix(model_name = MODEL_NAME,\n",
    "                                                     model_timestamp = MODEL_TIMESTAMP,\n",
    "                                                     y_true = Y_test,\n",
    "                                                     y_predicted = Y_predicted,\n",
    "                                                     data = 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24be139-d394-439a-ad3a-91c026d98fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bdd334-024d-4846-b9bf-018e09a432dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a557fd15-2eb8-497f-bd7e-34e8fc596449",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6d7a4-039c-452e-bc4c-78c5199996c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS_NAME[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DoJbgcgC1d83",
   "metadata": {
    "id": "DoJbgcgC1d83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tasp_cnn.save(root_path + 'madrid_model_XGBOOST_predicted.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b65f8d-ba77-42e8-850a-d4b8b5135641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autokeras as ak\n",
    "\n",
    "# # clf = ak.ImageClassifier(num_classes = 3,\n",
    "# #                          loss='categorical_crossentropy',\n",
    "# #                          metrics = [tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold = 0.1)],\n",
    "# #                          overwrite = True,\n",
    "# #                          tuner= 'bayesian',\n",
    "# #                          max_trials = 20,\n",
    "# #                          max_model_size = 3000000\n",
    "# #                         )\n",
    "# clf = ak.StructuredDataClassifier(num_classes = 3,\n",
    "#                              loss='categorical_crossentropy',\n",
    "#                              metrics = [tfa.metrics.F1Score(num_classes = num_classes, average='micro', threshold = 0.1)],\n",
    "#                              overwrite = True,\n",
    "#                              tuner= 'bayesian',\n",
    "#                              max_trials = 20\n",
    "#                         )\n",
    "\n",
    "# clf.fit(array_train_images,\n",
    "#         np.asarray(Y_train),\n",
    "#         epochs = 100,\n",
    "#         batch_size = 128,\n",
    "#         validation_data = (array_test_images, np.asarray(Y_test)))\n",
    "\n",
    "# best_auto_model = clf.export_model()\n",
    "# print(best_auto_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bbb5c-e78f-426b-bdac-c679f9ee671c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Escritura del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61062eb-31de-4166-8855-98f5b9277758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = f\"{MODELS_PATH}{MODEL_NAME}/\"\n",
    "# MODEL_FILE_NAME = f\"{city_name}_{MODEL_NAME}_{MODEL_TIMESTAMP}.h5\"\n",
    "\n",
    "# best_auto_model.save(MODEL_PATH + MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8663f-213d-4010-9195-6ad9cbc9c9e1",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216cee8-ff56-498d-98db-112eb635c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_predicted = best_auto_model.predict(x = array_test_images, batch_size = 128).argmax(axis = 1)\n",
    "\n",
    "# F1_SCORE_PATH = f\"{F1_SCORES_PATH}{MODEL_NAME}/\"\n",
    "# F1_SCORE_NAME = f\"{city_name}_{MODEL_NAME}_f1_score_{MODEL_TIMESTAMP}.svg\"\n",
    "\n",
    "# # plot_f1_score(f1_score_path = F1_SCORE_PATH,\n",
    "# #               f1_score_name = F1_SCORE_NAME,\n",
    "# #               history = history)\n",
    "\n",
    "# print(\"[INFO] evaluating network...\")\n",
    "\n",
    "# REPORT_PATH = f\"{REPORTS_PATH}{MODEL_NAME}/\"\n",
    "# REPORT_NAME  = f\"{city_name}_{MODEL_NAME}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "# plot_classification_report(path = REPORT_PATH,\n",
    "#                            file_name = REPORT_NAME,\n",
    "#                            y_true = Y_test,\n",
    "#                            y_predicted = Y_predicted)\n",
    "\n",
    "\n",
    "# CONFUSION_MATRIX_PATH = f\"{CONFUSIONS_MATRIX_PATH}{MODEL_NAME}/\"\n",
    "# CONFUSION_MATRIX_NAME  = f\"{city_name}_{MODEL_NAME}_confusion_matrix_{MODEL_TIMESTAMP}.svg\"\n",
    "\n",
    "# plot_confusion_matrix(path = CONFUSION_MATRIX_PATH,\n",
    "#                       file_name = CONFUSION_MATRIX_NAME,\n",
    "#                       y_true = Y_test,\n",
    "#                       y_predicted = Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe2ca4-bce3-4c90-9bc8-19752de1b923",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02492821-6fb2-497f-a9c0-f4a2ea2764c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODELS_NAME[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c44a9-dfca-4de5-a9c3-413d7ef4ab51",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28db0f9-8fdd-4900-9e53-af0f0bb12f0a",
   "metadata": {},
   "source": [
    "## Models times plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0299b-0261-43d6-9227-89be76889b77",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa79304-4111-4c44-8578-ecbd30daef2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "times = times.sort_values('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ecd0c3-4678-46e6-a7c5-669ded20e4a3",
   "metadata": {},
   "source": [
    "### Save csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd871a3-23bc-4efb-a3fa-8f87d9129f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = f\"{REPORTS_TIMES_PATH}{MODEL_TIMESTAMP}.csv\"\n",
    "times.to_csv(SAVE_PATH, index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8660de1-1623-450b-9179-6cfc87fab725",
   "metadata": {},
   "source": [
    "### Save fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72551f93-ced6-4c0f-bedf-73d1205fd17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD_PATH = f\"{REPORTS_TIMES_PATH}2022-05-23-15:28:04.csv\"\n",
    "# times = pd.read_csv(LOAD_PATH)\n",
    "\n",
    "# ax = sns.barplot(x = 'time',\n",
    "#                  y = 'model',\n",
    "#                  palette='deep',\n",
    "#                  data = times).set(title = f\"Models Fitting Time (s)\")\n",
    "# plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# SAVE_PATH = f\"{REPORTS_TIMES_PATH}{MODEL_TIMESTAMP}.png\"\n",
    "# plt.savefig(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5612e-df65-4d90-bf1d-b34f624c4c98",
   "metadata": {},
   "source": [
    "## Models metrics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60314a42-8590-48e1-9d87-9f41859c01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "reports_summary = pd.DataFrame()\n",
    "\n",
    "cities = []\n",
    "# MODEL_TIMESTAMP = '2022-08-02-10:10:19'\n",
    "\n",
    "# cities.append('leeds')  if leeds else None\n",
    "cities.append('madrid') if madrid else None\n",
    "# cities.append('UK') if UK else None\n",
    "\n",
    "models_renaming = {'knn': 'KNN',\n",
    "                   'convolution_1d': '1D-convolution',\n",
    "                   'convolution_2d': '2D-convolution',\n",
    "                   'nb': 'NB',\n",
    "                   'svc': 'SVC'}\n",
    "                   # 'auto_ml': 'AutoML'}\n",
    "\n",
    "splits = ['train', 'test']\n",
    "sorted_by_time_models_name = times.model\n",
    "\n",
    "for split in splits:\n",
    "    reports_summary = pd.DataFrame()\n",
    "\n",
    "    for model_name in sorted_by_time_models_name:\n",
    "\n",
    "        REPORT_PATH = f\"{REPORTS_PATH}{model_name}/{split}/\"\n",
    "\n",
    "        for city_name in cities:\n",
    "\n",
    "            REPORT_NAME  = f\"{city_name}_{model_name}_report_{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "            if exists(REPORT_PATH + REPORT_NAME):\n",
    "                print(f\"Found: {model_name} for {split}\")\n",
    "                report = pd.read_csv(REPORT_PATH + REPORT_NAME, index_col=[0])\n",
    "                report.insert(0, 'split', split)\n",
    "                report.insert(1, 'city', city_name)\n",
    "                report.insert(2, 'model', models_renaming[model_name])\n",
    "\n",
    "                reports_summary = pd.concat([reports_summary, report])\n",
    "\n",
    "                reports_summary = reports_summary.sort_values(['city', 'model'], ascending = [True, True])\n",
    "\n",
    "    if not reports_summary.empty:\n",
    "        c_m = reports_summary['city'] + '_' + reports_summary['model']\n",
    "        reports_summary.insert(0, 'c_m', c_m)\n",
    "\n",
    "        SAVE_PATH =  f\"{REPORTS_SUMMARY_PATH}/{split}/{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "        reports_summary.insert(0, 'accident_type', reports_summary.index)\n",
    "        reports_summary.to_csv(SAVE_PATH, index= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee8508-988e-41c1-b1fa-53c45e6fea55",
   "metadata": {},
   "source": [
    "## Models scores plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff41810-6cfc-4e31-a054-87cd725c9a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5de0ef-2c61-42fb-886d-cabe6c9bb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "MEASURE_TYPES  = ['precision', 'recall', 'f1-score']\n",
    "# ACCIDENT_TYPES = ['Slight', 'Serious', 'Fatal']\n",
    "\n",
    "ACCIDENT_TYPES = ['Slight', 'Assistance']\n",
    "\n",
    "\n",
    "# if leeds:\n",
    "#     leeds_reports_summary  = reports_summary[reports_summary['city'] == 'leeds']\n",
    "if madrid:\n",
    "    madrid_reports_summary = reports_summary[reports_summary['city'] == 'madrid']\n",
    "# if UK:\n",
    "#     UK_reports_summary = reports_summary[reports_summary['city'] == 'UK']\n",
    "\n",
    "# print(leeds_reports_summary.loc[ACCIDENT_TYPES])\n",
    "\n",
    "for split in splits:\n",
    "    \n",
    "    REPORT_PATH = f\"{REPORTS_SUMMARY_PATH}{split}/{MODEL_TIMESTAMP}.csv\"\n",
    "\n",
    "    if exists(REPORT_PATH):\n",
    "        fig, axs = plt.subplots(len(MEASURE_TYPES), len(cities), figsize=(15,20))\n",
    "\n",
    "        print(f\"Found: {REPORT_PATH}\")\n",
    "\n",
    "        report = pd.read_csv(REPORT_PATH, index_col=[0])\n",
    "\n",
    "#         if leeds:\n",
    "#             leeds_reports_summary  = report[report['city'] == 'leeds']\n",
    "        if madrid:\n",
    "            madrid_reports_summary = report[report['city'] == 'madrid']\n",
    "        # if UK:\n",
    "        #     UK_reports_summary = report[report['city'] == 'UK']\n",
    "\n",
    "        for index, measure_type in enumerate(MEASURE_TYPES):\n",
    "\n",
    "            # Si son dos ciudades el plot es bidimensional.\n",
    "            if len(cities) > 1:\n",
    "                axis_leeds = axs[index, 0]\n",
    "                axis_madrid = axs[index, 1]\n",
    "            else:\n",
    "                axis_leeds = axis_madrid = axis_UK = axs[index]\n",
    "\n",
    "#             if leeds:\n",
    "#                 ax = sns.barplot(x = 'accident_type',\n",
    "#                                  y = measure_type,\n",
    "#                                  hue = 'model',\n",
    "#                                  palette = 'deep',\n",
    "#                                  data = leeds_reports_summary.loc[ACCIDENT_TYPES],\n",
    "#                                  ax = axis_leeds).set(title = f\"{measure_type} Leeds\")\n",
    "                \n",
    "\n",
    "            if madrid:\n",
    "                ax = sns.barplot(x = 'accident_type',\n",
    "                                 y = measure_type,\n",
    "                                 hue = 'model',\n",
    "                                 palette = 'deep',\n",
    "                                 data = madrid_reports_summary.loc[ACCIDENT_TYPES],\n",
    "                                 ax = axis_madrid).set(title = f\"{measure_type} Madrid\")\n",
    "            \n",
    "#             if UK:\n",
    "#                 ax = sns.barplot(x = 'accident_type',\n",
    "#                                  y = measure_type,\n",
    "#                                  hue = 'model',\n",
    "#                                  palette = 'deep',\n",
    "#                                  data = UK_reports_summary.loc[ACCIDENT_TYPES],\n",
    "#                                  ax = axis_UK).set(title = f\"{measure_type} UK\")                \n",
    "\n",
    "        SAVE_PATH = f\"{REPORTS_SUMMARY_PATH}{split}/{MODEL_TIMESTAMP}.png\"\n",
    "\n",
    "        fig = fig.get_figure()\n",
    "        fig.savefig(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13321220-f5d7-4a0a-aee7-75e16bfa0226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0790f1e-f6d3-433c-89db-aca64f59c5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "V7Azjtl8gRth",
    "qKYh5EeThQ_7",
    "kISRP5AQhWTD",
    "cCo2emMclT8h",
    "gJfbDNO5oB1N",
    "7a4EsWwQhe_i",
    "ycdOBuHSjhSk",
    "5PmJpoCCcxMJ",
    "ybjvOI7x0PKz",
    "pVPFGQ0AoNRD",
    "_Z4nz3ioxtXb",
    "dg0d7-k15L3H",
    "d5fAIUUg5L3J"
   ],
   "name": "TFM_final.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
