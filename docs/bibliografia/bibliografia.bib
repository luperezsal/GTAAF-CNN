% Encoding: UTF-8

% TECNOLOGIAS

%% Modelos

%%% Algoritmos Genéticos


@article{GA,
author = {Lingaraj, Haldurai},
year = {2016},
month = {10},
pages = {139-143},
title = {A Study on Genetic Algorithm and its Applications},
volume = {4},
journal = {International Journal of Computer Sciences and Engineering}
}


%%% XGBoost

@InProceedings{XGBoostTutorial,
  doi = {10.1145/2939672.2939785},  
  url = {https://doi.org/10.1145%2F2939672.2939785},  
  year = 2016,
  month = {aug},  
  publisher = {{ACM}},  
  author = {Tianqi Chen and Carlos Guestrin},  
  title = {{XGBoost}},  
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}
}


@online{NvidiaXGBoost,
  author = {Nvidia},
  title = {Nvidia XGBoost},
  url = {https://www.nvidia.com/en-us/glossary/data-science/xgboost/}
}

%%% NN

@ARTICLE{NNReview,
  AUTHOR={Emmert-Streib, Frank and Yang, Zhen and Feng, Han and Tripathi, Shailesh and Dehmer, Matthias},
  TITLE={An Introductory Review of Deep Learning for Prediction Models With Big Data},
  JOURNAL={Frontiers in Artificial Intelligence},
  VOLUME={3},
  YEAR={2020},
  URL={https://www.frontiersin.org/article/10.3389/frai.2020.00004},
  DOI={10.3389/frai.2020.00004},
  ISSN={2624-8212},
}

@online{Softmax,
  author = {Wikipedia},
  title = {Softmax Function},
  url = {https://en.wikipedia.org/wiki/Softmax_function}
}

@online{Cross-Entropy,
  author = {Kiprono Elijah Koech},
  title = {Cross-Entropy Loss Function},
  url = {https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e}
}

@misc{LearningRate,
  doi = {10.48550/ARXIV.1803.09820},
  url = {https://arxiv.org/abs/1803.09820},  
  author = {Smith, Leslie N.},  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},  
  title = {A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay},  
  publisher = {arXiv},  
  year = {2018},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{BatchNormalization,
  doi = {10.48550/ARXIV.1502.03167},
  url = {https://arxiv.org/abs/1502.03167},
  author = {Ioffe, Sergey and Szegedy, Christian},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{KL-Divergence,
  title={Information theory and statistics},
  author={Kullback, Solomon},
  year={1997},
  publisher={Courier Corporation}
}


@online{FiltersFeatureMaps,
  author = {Renu Khandelwal},
  title = {Filters and Feature Maps},
  url = {https://towardsdatascience.com/convolutional-neural-network-feature-map-and-filter-visualization-f75012a5a49c}
}



%%%% Conv1D

@misc{Conv1D_Survey,
  doi = {10.48550/ARXIV.1905.03554},
  url = {https://arxiv.org/abs/1905.03554},
  author = {Kiranyaz, Serkan and Avci, Onur and Abdeljaber, Osama and Ince, Turker and Gabbouj, Moncef and Inman, Daniel J.},
  keywords = {Signal Processing (eess.SP), Artificial Intelligence (cs.AI), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {1D Convolutional Neural Networks and Applications: A Survey},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@online{Kernels,
  author = {Prakhar Ganesh},
  title = {Types of Convolution Kernels : Simplified},
  url = {https://towardsdatascience.com/types-of-convolution-kernels-simplified-f040cb307c37}
}

%%%  KNN

@article{KNN,
  title={An introduction to kernel and nearest-neighbor nonparametric regression},
  author={Altman, Naomi S},
  journal={The American Statistician},
  volume={46},
  number={3},
  pages={175--185},
  year={1992},
  publisher={Taylor \& Francis}
}

%% Especificaciones Técnicas

%%% Herramientas Utilizadas

@online{Python,
  author = {Python},
  title = {Python Language},
  url = {https://www.python.org/about/quotes/}
}


@online{Tensorflow,
  author = {tensorflow},
  title = {Tensorflow Library},
  url = {https://www.tensorflow.org}
}

@online{Pandas,
  author = {pandas-dev},
  title = {Pandas Library},
  url = {https://pandas.pydata.org/docs/getting_started/overview.html}
}

@online{Scikit-Learn,
  author = {scikit-learn},
  title = {Scikit-learn Library},
  url = {https://scikit-learn.org/stable/}
}

@online{XGBoostLibrary,
  author = {dmlc},
  title = {Xgboost Library},
  url = {https://xgboost.readthedocs.io/en/stable/}
}

@online{CUDA,
  author = {Nvidia},
  title = {CUDA Library},
  url = {https://developer.nvidia.com/cuda-zone}
}

@online{Anaconda,
  author = {Anaconda, Inc},
  title = {Anaconda library},
  url = {https://www.anaconda.com/}
}

@online{DiagramsNet,
  author = {jGraph},
  title = {DiagramsNet Software},
  url = {https://www.diagrams.net/}
}

@online{GoogleMeet,
  author = {Google},
  title = {Google Meets Application},
  url = {https://meet.google.com/}
}

@online{Github,
  author = {Github},
  title = {Github SVC},
  url = {https://github.com/}
}

@online{JupyterNotebook,
  author = {Project Jupyter},
  title = {Jupyter Notebook},
  url = {https://jupyter.org/}
}

@online{JupyterLab,
  author = {Project Jupyter},
  title = {Jupyter Lab},
  url = {https://jupyterlab.readthedocs.io/en/stable/}
}

%%% Especificaciones del servidor


% Metodología

%% Diagrama de Flujo
%% Diagrama de Gantt
%% Proceso


@online{DatasetMadrid,
  author = {Portal de Datos Abiertos del Ayuntamiento de Madrid},
  title = {Accidentes de tráfico Madrid},
  url = {https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=7c2843010d9c3610VgnVCM2000001f4a900aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default},
  urldate = {2022-05-11}
}

@online{InfoDatasetMadrid,
  author = {Portal de Datos Abiertos del Ayuntamiento de Madrid},
  title = {ESTRUCTURA DEL CONJUNTO DE DATOS},
  url = {https://datos.madrid.es/FWProjects/egob/Catalogo/Seguridad/Ficheros/Estructura_DS_Accidentes_trafico_desde_2019.pdf}
}

%%% Datos

%%%% Limpieza de datos
%%%% Transformaciones de datos
%%%% Análisis de datos

@article{WhyImbalancedDataIsAProblem,
  title={Classification of imbalanced data: A review},
  author={Sun, Yanmin and Wong, Andrew KC and Kamel, Mohamed S},
  journal={International journal of pattern recognition and artificial intelligence},
  volume={23},
  number={04},
  pages={687--719},
  year={2009},
  publisher={World Scientific}
}


@article{ImbalancedDataReview,
  title = {Learning from class-imbalanced data: Review of methods and applications},
  journal = {Expert Systems with Applications},
  volume = {73},
  pages = {220-239},
  year = {2017},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2016.12.035},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417416307175},
  author = {Guo Haixiang and Li Yijing and Jennifer Shang and Gu Mingyun and Huang Yuanyue and Gong Bing},
  keywords = {Rare events, Imbalanced data, Machine learning, Data mining},
  abstract = {Rare events, especially those that could potentially negatively impact society, often require humans’ decision-making responses. Detecting rare events can be viewed as a prediction task in data mining and machine learning communities. As these events are rarely observed in daily life, the prediction task suffers from a lack of balanced data. In this paper, we provide an in depth review of rare event detection from an imbalanced learning perspective. Five hundred and seventeen related papers that have been published in the past decade were collected for the study. The initial statistics suggested that rare events detection and imbalanced learning are concerned across a wide range of research areas from management science to engineering. We reviewed all collected papers from both a technical and a practical point of view. Modeling methods discussed include techniques such as data preprocessing, classification algorithms and model evaluation. For applications, we first provide a comprehensive taxonomy of the existing application domains of imbalanced learning, and then we detail the applications for each category. Finally, some suggestions from the reviewed papers are incorporated with our experiences and judgments to offer further research directions for the imbalanced learning and rare event detection fields.}
}



%%%% Separación de datos
%%%% Normalización de datos


@article{DataNormalizationInvestigation,
  title = {Investigating the impact of data normalization on classification performance},
  journal = {Applied Soft Computing},
  volume = {97},
  pages = {105524},
  year = {2020},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2019.105524},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494619302947},
  author = {Dalwinder Singh and Birmohan Singh},
  keywords = {Ant lion optimization, Data normalization, Feature selection, Feature weighting, -NN classifier},
  abstract = {Data normalization is one of the pre-processing approaches where the data is either scaled or transformed to make an equal contribution of each feature. The success of machine learning algorithms depends upon the quality of the data to obtain a generalized predictive model of the classification problem. The importance of data normalization for improving data quality and subsequently the performance of machine learning algorithms has been presented in many studies. But, the work lacks for the feature selection and feature weighting approaches, a current research trend in machine learning for improving performance. Therefore, this study aims to investigate the impact of fourteen data normalization methods on classification performance considering full feature set, feature selection, and feature weighting. In this paper, we also present a modified Ant Lion optimization that search feature subsets and the best feature weights along with the parameter of Nearest Neighbor Classifier. Experiments are performed on 21 publicly available real and synthetic datasets, and results are analyzed based on the accuracy, the percentage of feature reduced and runtime. It has been observed from the results that no single method outperforms others. Therefore, we have suggested a set of the best and the worst methods combining the normalization procedure and empirical analysis of results. The better performers are z-Score and Pareto Scaling for the full feature set and feature selection, and tanh and its variant for feature weighting. The worst performers are Mean Centered, Variable Stability Scaling and Median and Median Absolute Deviation methods along with un-normalized data.}
}

@online{NormalizationSensitiveModels,
  author = {Data School},
  title = {Comparing supervised learning algorithms},
  url = {https://www.dataschool.io/comparing-supervised-learning-algorithms/},
}


%%% Muestreo

%%%% Downsampling

@ARTICLE{Downsampling,
  author = {Liu, Xu-Ying and Wu, Jianxin and Zhou, Zhi-Hua},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  title={Exploratory Undersampling for Class-Imbalance Learning},
  year={2009},
  volume={39},
  number={2},
  pages={539-550},
  doi={10.1109/TSMCB.2008.2007853}
  }

%%%% Synthetic Data

@article{SMOTEII,
  doi = {10.1613/jair.953},  
  url = {https://doi.org/10.1613%2Fjair.953},  
  year = 2002,
  month = {jun},  
  publisher = {AI Access Foundation},  
  volume = {16},  
  pages = {321--357},  
  author = {N. V. Chawla and K. W. Bowyer and L. O. Hall and W. P. Kegelmeyer},  
  title = {SMOTE: Synthetic Minority Over-sampling Technique},  
  journal = {Journal of Artificial Intelligence Research}
}



@article{SNE,
  title={Stochastic neighbor embedding},
  author={Hinton, Geoffrey E and Roweis, Sam},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}


@article{TSNEPaper,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}


%%% Algoritmo Genético

@ARTICLE{GAXGBoostPaper,
  author={Jiang, Yu and Tong, Guoxiang and Yin, Henan and Xiong, Naixue},
  journal={IEEE Access},
  title={A Pedestrian Detection Method Based on Genetic Algorithm for Optimize XGBoost Training Parameters},
  year={2019},
  volume={7},
  number={},
  pages={118310-118321},
  doi={10.1109/ACCESS.2019.2936454}
}

@online{GAXGBoostCode,
  author = {Mohit Jain},
  title = {Hyperparameter tuning in XGBoost using genetic algorithm},
  url = {https://towardsdatascience.com/hyperparameter-tuning-in-xgboost-using-genetic-algorithm-17bd2e581b17},
}

@online{XGBoostHyperparamsMeaning,
  author = {Cambridge Spark},
  title = {Hyperparameter tuning in XGBoost},
  url = {https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f},
}

%%% XGBoost
@online{XGBoostFeatureWeightsMeaning,
  author = {Amjad Abu-Rmileh},
  title = {The Multiple faces of ‘Feature importance’ in XGBoost},
  url = {https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7},
}
%%% Construcción de imágenes

@article{JerarquiaImagenes,
  author = {Kopelias, Pantelis and Papadimitriou, Fanis and Papandreou, Konstantinos and Prevedouros, P. D.},
  year = {2007},
  month = {12},
  pages = {123-131},
  title = {Urban Freeway Crash Analysis: Geometric, Operational, and Weather Effects on Crash Number and Severity},
  volume = {2015},
  journal = {Transportation Research Record},
  doi = {10.3141/2015-14}
}

@article{TASPCNN,
  title={Traffic accident’s severity prediction: A deep-learning approach-based CNN network},
  author={Zheng, Ming and Li, Tong and Zhu, Rui and Chen, Jing and Ma, Zifei and Tang, Mingjing and Cui, Zhongqiang and Wang, Zhan},
  journal={IEEE Access},
  volume={7},
  pages={39897--39910},
  year={2019},
  publisher={IEEE}
}

%%% Implementación de Modelos


%%%% KNN

@online{GridSearchSklearnLibrary,
  author = {Sklearn},
  title = {GridSearchCV},
  url = {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html},
}

@article{AutoSklearn,
    title     = {Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning},
    author    = {Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
    journal   = {arXiv:2007.04074 [cs.LG]},
    year      = {2020},
}


@inproceedings{GradientVanishingRelu,
  title={Deep sparse rectifier neural networks},
  author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={315--323},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}


@Comment{jabref-meta: databaseType:biblatex;}
