%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plantilla TFG/TFM
% Escuela Politécnica Superior de la Universidad de Alicante
% Realizado por: Jose Manuel Requena Plens
% Contacto: info@jmrplens.com / Telegram:@jmrplens
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Marco Teórico}
\label{marcoteorico}


Hay muchas definiciones de inteligencia artificial (IA) pero, a mi entender, hay dos que destacan: la de Marvin Minsky y John L. Gordon. Marvin Minsky dice que la inteligencia
artificial es la ciencia que hace que las máquinas o los sistemas hagan cosas que requerirían inteligencia si las hicieran los hombres. Por otra parte, John L. Gordon dice que el objetivo de la Inteligencia Artificial es crear máquinas inteligentes y, a través de esto, comprender los principios de la inteligencia. Según estas definiciones podemos decir que los sistemas de IA se caracterizan por pensar y actuar como personas de forma razonable.


Hace tiempo que la inteligencia artificial abandonó el espectro de la ciencia ficción para colarse en nuestras vidas y, aunque todavía en una fase muy inicial, está llamada a protagonizar una revolución que, como mínimo, se puede comparar a la que generó Internet. Sus aplicaciones en múltiples sectores —como salud, finanzas, transporte o educación, entre otros— han provocado una explosión de trabajos de investigación desde diferentes puntos de vista. Además, estas tecnologías inteligentes, están penetrando en diferentes partes de la vida humana y entre ellas , todo lo referente al transporte. Como ejemplo podemos tomar los sistemas de transporte inteligente y los sistemas de transporte automatizados que utilizan tecnologías de información, transporte y comunicación implementadas en vehículos o infraestructuras. Estos sistemas tienen entre otros objetivos, aumentar la movilidad de personas o mercancías, aumentar la seguridad vial, reducir los accidentes de tráfico o la reducción del impacto en el medio ambiente.


La teoría de la IA se está desarrollando desde hace una década pero, su uso, tuvo que esperar a que se produjeran avances en el área de las tecnologías de la información ya
que requiere el desarrollo de componentes informáticos, principalmente procesadores rápidos, dispositivos de memoria de alta capacidad, creación y crecimiento de redes
informáticas como redes inalámbricas, redes de satélites y móviles, etc. Gracias a este progreso técnico actual, la Inteligencia Artificial contiene técnicas y medios suficientes para ser utilizada en diferentes áreas. Así, las redes neuronales, la planificación de IA, los algoritmos evolutivos, los sistemas expertos y de conocimiento, la lógica difusa, los sistemas multiagente, la regresión vectorial, la minería de datos o las técnicas de optimización, permiten la su utilización en los más diversos campos. Además, hoy en día las Tecnologías de la Información obligan a manejar cada vez más información en muy poco tiempo, con lo que resulta inevitable construir y utilizar técnicas capaces de clasificar, e incluso predecir, la información. Estos problemas complicados se resuelven parcialmente mediante redes neuronales que utilizan el conocimiento sobre la
organización y administración de datos en el cerebro humano.


Centrándonos más en el objeto de estudio de este trabajo, los accidentes de tráfico, podemos decir que a lo largo de los últimos años diferentes estudios se han focalizado en
analizar las causas y la severidad de los mismos. En las últimas décadas, han coexistido dos vertientes relacionadas con el análisis de la severidad de los accidentes: la
perspectiva de modelos estadísticos y la del aprendizaje automático (beneficiada por el aumento de los recursos computacionales). Los modelos estadísticos tienen la
característica de realizar ciertas premisas sobre los datos, asumiendo que éstos se encuentran de acuerdo a una distribución de probabilidad determinada. Sin embargo, si
esta premisa sobre los datos de entrada no se cumple, se pueden producir resultados erróneos. Estos métodos se han aplicado en campos relacionados con los factores que
influyen en los accidentes; como el análisis de características principales en atropellos de peatones en pasos de cebra \cite{FactoresQueInfluyen2003}, o la aplicación de regresiones logísticas multinomiales que ponderan los factores que afectan a la gravedad de accidentes en motocicletas \cite{MotorcycleCrashesMultinominalStatistic}. Otros estudios se centran en comparar distintas técnicas de modelos estadísticos entre sí para este mismo problema, como comparaciones entre \glsentrylong{svm} y probit ordenado (OP) \cite{MetodosEstadisticosComparacionSVMOP}.


Por otra parte, los modelos \textit{machine learning} no realizan ninguna presuposición sobre los datos, por lo que estos pueden lograr un rendimiento similar o superior a los estadísticos. En la literatura existen múltiples modelos aplicados al problema de evaluación de accidentes de tráfico, como la implementación de reglas de decisión basadas en árboles de decisión para evaluar la importancia de las características \cite{ArbolDecisionSeveridadDeAccidentes}, o el uso de regresiones logísticas para clasificar su severidad \cite{LogisticRegressionPrediccionAccidentes}.

Otras metodologías han sido propuestas para este problema en los últimos tiempos aplicando algoritmos genéticos. Una de las propuestas más influyentes en este campo involucra el conocimiento de los usuarios de las vías, como ingenieros de caminos o agentes de la autoridad, para entrenar modelos basados en reglas de decisión utilizando un algoritmo genético multiobjetivo \glsentryfull{nsgaii}, con el objetivo de clasificar los accidentes de tráfico \cite{NSGAIIAccidentPrediction}. Además, se han realizado trabajos para entrenar, mediante programación genética, clasificadores difusos para el descubrimiento de características influyentes y relaciones importantes entre ellas \cite{GeneticProgrammingAccidents}.

En los últimos tiempos, las técnicas de \textit{deep learning} se están mostrando muy efectivas en resolver problemas de clasificación. Es por esto que se han aplicado a contextos relacionados con los incidentes de tráfico, como por ejemplo, la predicción de accidentes en autopistas basados en redes neuronales \cite{RedNeuronalAutopistaAccidentes} o la aplicación de técnicas convolucionales \cite{OtraPrediccionConCNNs}. Como se ha comentado, estos enfoques se muestra más efectivos que los métodos estadísticos o los basados en técnicas de \textit{machine learning} más tradicionales. No obstante, existen otros muchos enfoques recientes en la detección de accidentes, como el basado en aplicar redes \glsentryfull{lstm} sobre tweets capturados tiempo real \cite{DeteccionDeAccidentesPorTweets} con el objetivo de localizar accidentes, o el uso de \glsentryfull{rnn} para la predicción de lesividad \cite{RNNAccidentSeverityPrediction}.


Uno de los principales inconvenientes comunes en estos estudios suele ser la baja calidad de los conjuntos de datos ofrecidos por las instituciones y el difícil acceso a los mismos \cite{ImportanciaDeBajaCalidadActualmenteDeDatasets}. Además, el desbalanceo de datos, asociado a la naturaleza del problema, genera una dificultad añadida a dichos estudios, ya que gran parte de los accidentes suelen ser leves, siendo el número de accidentes serios y fatales mucho menor. Existen numerosos artículos que analizan este problema y se proponen distintas soluciones, como la utilización de técnicas de remuestreo \cite{ItalianoMetricasDesbalanceo} o la definición de nuevas métricas de clasificación.


Paralelamente a la predicción de gravedad de los accidentes de tráfico, existen múltiples variantes al problema que permitirían reducir el número de fallecidos en las carreteras aplicando distintas técnicas de \textit{deep learning}. Recientes investigaciones proponen arquitecturas complejas para crear mapas de riesgos de accidentes, tomando como entrada imágenes tomadas por satélite, segmentación de carreteras, información GPS de los vehículos y datos de accidentes con la finalidad de detectar las zonas más peligrosas
\cite{MIT}.